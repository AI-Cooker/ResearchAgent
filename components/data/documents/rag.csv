,Unnamed: 0,title,snippet,links,raw_document,clean_document
0,0,Retrieval Augmented Generation (RAG): Reducing ... - Pinecone,,"https://www.pinecone.io/learn/retrieval-augmented-generation/#:~:text=Retrieval%20Augmented%20Generation%20means%20fetching,a%20response%2C%20solving%20this%20problem.","['Retrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI Applications | PineconeProductSolutionsPricingResourcesCompanyLog InSign Up FreeLearn | ArticleRetrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI ApplicationsJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Products built on top of Large Language Models (LLMs) such as OpenAI\'s ChatGPT and Anthropic\'s Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.LLMs are “stuck” at a particular time, but RAG can bring them into the present.ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”The LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.The reason LLMs like ChatGPT feel so bright is that they\'ve seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.RAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.Why is RAG the preferred approach from a cost-efficacy perspective?There are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.Fine-tuning: adapting a foundation model to your domain’s data.Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.Prompt engineering is insufficient for reducing hallucinations.Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.It’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.Let’s now take a deeper dive into how Retrieval Augmented Generation works.We’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window"".An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.Creating a vector database from your domain-specific proprietary data using an embedding model.This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.You originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.You send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM\'s context window.Since the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.Vector databases can support even more advanced search functionality.Semantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query\'s semantics and keywords.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.Pinecone enables you to integrate RAG within minutes. Check out our examples repository on GitHub for runnable examples, such as this RAG Jupyter Notebook.Share via:  Zachary ProserStaff Developer AdvocateJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.PRODUCTOverviewDocumentationTrust and SecuritySOLUTIONSSearchGenerative AICustomersRESOURCESLearning CenterCommunityPinecone BlogSupport CenterSystem StatusCOMPANYAboutPartnersCareersNewsroomContactLEGALTermsPrivacyCookies© Pinecone Systems, Inc. | San Francisco, CAPinecone is a registered trademark of Pinecone Systems, Inc.']","**Retrieval Augmented Generation (RAG) is a technique that helps Large Language Models (LLMs) produce more accurate and relevant responses by providing them with additional context from a database of proprietary data.**

RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.

**Here are the key benefits of using RAG:**

* **RAG reduces the likelihood of hallucinations.** LLMs are often inaccurate when asked about recent events or domain-specific information because they lack up-to-date context. RAG addresses this problem by providing LLMs with access to the most relevant information from a database of proprietary data.
* **RAG allows GenAI applications to cite their sources.** This improves auditability and makes it easier for end users to understand how GenAI applications arrive at their conclusions.
* **RAG is easy to implement.** RAG can be integrated with GenAI applications with just a few code changes.
* **RAG is cost-effective.** RAG does not require the same level of expertise or resources as other methods for improving the performance of GenAI applications, such as building a foundation model or fine-tuning an existing model.

**Here are the steps involved in using RAG:**

1. Create a vector database from your domain-specific proprietary data.
2. Convert your user's query into embeddings.
3. Send the embeddings to the vector database.
4. The vector database will return the most relevant results.
5. Provide the results to the LLM via its context window.

**RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.**"
1,1,What is retrieval-augmented generation?,"Aug 22, 2023 — ",https://research.ibm.com/blog/retrieval-augmented-generation-RAG,"[""What is retrieval-augmented generation? | IBM Research BlogSkip to main contentResearchFocus areasBlogPublicationsCareersAboutBackFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBackAboutOverviewLabsPeopleCollaborateBackArtificial IntelligenceBackHybrid CloudBackQuantum ComputingBackScienceBackSecurityBackSemiconductorsBackOverviewBackLabsBackPeopleBackCollaborateResearchFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBlogPublicationsCareersAboutOverviewLabsPeopleCollaborateOpen IBM search fieldClose22 Aug 2023Explainer4 minute readWhat is retrieval-augmented generation?RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.Large language models can be inconsistent. Sometimes they nail the answer to questions, other times they regurgitate random facts from their training data. If they occasionally sound like they have no idea what they’re saying, it’s because they don’t. LLMs know how words relate statistically, but not what they mean.Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.“You want to cross-reference a model’s answers with the original content so you can see what it is basing its answer on,” said Luis Lastras, director of language technologies at IBM Research.RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. IBM unveiled its new AI and data platform, watsonx, which offers RAG, back in May.An ‘open book’ approach to answering tough questionsUnderpinning all foundation models, including LLMs, is an AI architecture known as the transformer. It turns heaps of raw data into a compressed representation of its basic structure. Starting from this raw representation, a foundation model can be adapted to a variety of tasks with some additional fine-tuning on labeled, domain-specific knowledge.But fine-tuning alone rarely gives the model the full breadth of knowledge it needs to answer highly specific questions in an ever-changing context. In a 2020 paper, Meta (then known as Facebook) came up with a framework called retrieval-augmented generation to give LLMs access to information beyond their training data. RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”As the name suggests, RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.Toward personalized and verifiable responsesBefore LLMs, digital conversation agents followed a manual dialogue flow. They confirmed the customer’s intent, fetched the requested information, and delivered an answer in a one-size-fits all script. For straightforward queries, this manual decision-tree method worked just fine.But it had limitations. Anticipating and scripting answers to every question a customer might conceivably ask took time; if you missed a scenario, the chatbot had no ability to improvise. Updating the scripts as policies and circumstances evolved was either impractical or impossible.Today, LLM-powered chatbots can give customers more personalized answers without humans having to write out new scripts. And RAG allows LLMs to go one step further by greatly reducing the need to feed and retrain the model on fresh examples. Simply upload the latest documents or policies, and the model retrieves the information in open-book mode to answer the question.IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources.Teaching the model to recognize when it doesn’t knowCustomer queries aren’t always this straightforward. They can be ambiguously worded, complex, or require knowledge the model either doesn’t have or can’t easily parse. These are the conditions in which LLMs are prone to making things up.“Think of the model as an overeager junior employee that blurts out an answer before checking the facts,” said Lastras. “Experience teaches us to stop and say when we don’t know something. But LLMs need to be explicitly trained to recognize questions they can’t answer.”In a more challenging scenario taken from real life, Alice wants to know how many days of maternity leave she gets. A chatbot that does not use RAG responds cheerfully (and incorrectly): “Take as long as you want.”Maternity-leave policies are complex, in part, because they vary by the state or country of the employee’s home-office. When the LLM failed to find a precise answer, it should have responded, “I’m sorry, I don’t know,” said Lastras, or asked additional questions until it could land on a question it could definitively answer. Instead, it pulled a phrase from a training set stocked with empathetic, customer-pleasing language.With enough fine-tuning, an LLM can be trained to pause and say when it’s stuck. But it may need to see thousands of examples of questions that can and can’t be answered. Only then can the model learn to identify an unanswerable question, and probe for more detail until it hits on a question that it has the information to answer.RAG is currently the best-known tool for grounding LLMs on the latest, verifiable information, and lowering the costs of having to constantly retrain and update them. But RAG is imperfect, and many interesting challenges remain in getting RAG done right.At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.Subscribe to our Future Forward newsletter and stay up to date on the latest research newsSubscribe to our newsletterHome↳ BlogDate22 Aug 2023AuthorsKim MartineauTopicsAIExplainable AIGenerative AINatural Language ProcessingTrustworthy GenerationShareWhat is AI inferencing?ExplainerKim Martineau05 Oct 2023AIFoundation ModelsGenerative AIHybrid CloudCelebrating a decade of IBM Research innovation in AfricaNewsMike Murphy28 Sep 2023AIQuantumFind and fix IT glitches before they crash the system NewsKim Martineau28 Sep 2023AI for CodeAI for ITExplainable AIFoundation ModelsGenerative AIHow open source paved the way for computing from anywhere NewsKim Martineau20 Sep 2023AIHybrid Cloud PlatformMachine LearningScaling AIPreviousError correcting codes for near-term quantum computersNextIBM Research’s newest prototype chips use drastically less power to solve AI tasksIBM LogoFocus areasFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usTwitterLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility""]","Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information.

RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user's prompt or question. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant.

RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.

IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted."
2,2,Retrieval Augmented Generation (RAG),RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient ...,https://www.promptingguide.ai/techniques/rag,"['Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuidePrompt Engineering CoursePrompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingSelf-ConsistencyGenerate Knowledge PromptingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingReActMultimodal CoTGraph PromptingApplicationsProgram-Aided Language ModelsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionModelsFlanChatGPTLLaMAGPT-4LLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesPapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightQuestion? Give us feedback → (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)', ""General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge."", 'For more complex and knowledge-intensive tasks, it\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of ""hallucination"".', 'Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.', ""RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."", 'Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:', 'Image Source: Lewis et el. (2021) (opens in a new tab)', 'RAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.', 'This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.', 'More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.', 'You can find a simple example of how to use retrievers and LLMs for question answering with sources (opens in a new tab) from the LangChain documentation.Tree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright © 2023 DAIR.AI']","Retrieval Augmented Generation (RAG) is a method proposed by Meta AI researchers to address knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.

RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.

RAG performs strong on several benchmarks such as Natural Questions, WebQuestions, and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.

This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks."
3,3,What Is Retrieval-Augmented Generation (RAG)?,"Sep 19, 2023 — ",https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/,"['What Is Retrieval-Augmented Generation (RAG)?', 'Accessibility Policy', 'Skip to content', 'About', 'Services', 'Solutions', 'Pricing', 'Partners', 'Resources', 'Close Search', 'Close', 'We’re sorry.  We could not find a match for your search.', ""We suggest you try the following to help find what you're looking for:"", 'Check the spelling of your keyword search.', 'Use synonyms for the keyword you typed, for example, try “application” instead of “software.”', 'Start a new search.', 'Clear Search', 'Search', 'Menu', 'Menu', 'Contact Sales', 'Sign in to Oracle Cloud', 'Cloud', 'Artificial Intelligence', 'Generative AI', 'What Is Retrieval-Augmented Generation (RAG)?', 'Alan Zeichick | Tech Content Strategist | September 19, 2023', 'In This Article', 'What Is Retrieval-Augmented Generation (RAG)?', 'Retrieval-Augmented Generation Explained', 'How Does Retrieval-Augmented Generation Work?', 'Using RAG in Chat Applications', 'Benefits of Retrieval-Augmented Generation', 'Challenges of Retrieval-Augmented Generation', 'Examples of Retrieval-Augmented Generation', 'Future of Retrieval-Augmented Generation', 'Generative AI With Oracle', 'Retrieval-Augmented Generation FAQs', 'Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.', 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees.', 'What Is Retrieval-Augmented Generation (RAG)?', 'That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.', 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.', 'Retrieval-Augmented Generation Explained', 'Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.', 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. ', 'Simply put, RAG helps LLMs give better answers.', 'Key Takeaways', 'RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.', 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.', 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.', 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.', 'How Does Retrieval-Augmented Generation Work? ', 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.', ' The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.', 'RAG and Large Language Models (LLMs)', 'Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.', 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.', 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.', 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.', 'Retrieval-Augmented Generation vs. Semantic Search', ' RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.', ' Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.', ' Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG.', 'Using RAG in Chat Applications', 'When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.', ' The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.', 'Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.', 'Benefits of Retrieval-Augmented Generation', 'RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following:', 'The RAG has access to information that may be fresher than the data used to train the LLM.', 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.', 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.', 'The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted.', 'Challenges of Retrieval-Augmented Generation', 'Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are', 'Improving organizational knowledge and understanding of RAG because it’s so new', 'Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself', 'Determining how to best model the structured and unstructured data within the knowledge library and vector database', 'Developing requirements for a process to incrementally feed data into the RAG system', 'Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system', 'Examples of Retrieval-Augmented Generation', 'There are many possible examples of generative AI augmented by RAG.', 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.', 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.', 'Future of Retrieval-Augmented Generation', 'Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.', 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.', 'RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.', 'Generative AI With Oracle', 'Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.', 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.', 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot.', 'What makes Oracle best suited for generative AI?', 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.', 'Learn more about Oracle’s generative AI strategy', 'Retrieval-Augmented Generation FAQs', 'Is RAG the same as generative AI?', 'No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.', 'What type of information is used in RAG?', 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.', 'How does generative AI use RAG?', 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.', 'Can a RAG cite references for the data it retrieves?', 'Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.', 'Resources for', 'Careers', 'Developers', 'Investors', 'Partners', 'Startups', 'Students and Educators', 'Why Oracle', 'Analyst Reports', 'Cloud Economics', 'with Microsoft Azure', 'vs. AWS', 'vs. Google Cloud', 'vs. MongoDB', 'Learn', 'What is AI?', 'What is Cloud Computing?', 'What is Cloud Storage?', 'What is HPC?', 'What is IaaS?', 'What is PaaS?', 'What’s new', 'Oracle Supports Ukraine', 'Oracle Cloud Free Tier', 'Cloud Architecture Center', 'Cloud Lift', 'Oracle Support Rewards', 'Oracle Red Bull Racing', 'Contact us', 'US Sales: +1.800.633.0738', 'How can we help?', 'Subscribe to emails', 'Events', 'News', 'OCI Blog', ' Country/Region ', '© 2023 Oracle', 'Privacy/Do Not Sell My Info', 'Ad Choices', 'Careers', 'Facebook', 'Twitter', 'LinkedIn', 'YouTube']","Retrieval-augmented generation (RAG) is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.

RAG models build knowledge repositories based on the organization's own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.

Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.

Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM."
4,4,What is Retrieval Augmented Generation (RAG)?,"RAG puts together a pre-trained system that finds relevant information ( retriever ) with another system that generates text ( generator ). Then, when the user ...",https://vercel.com/guides/retrieval-augmented-generation,"['What is Retrieval Augmented Generation (RAG)?Skip to contentDocumentationGuidesHelp← Back to GuidesWhat is Retrieval Augmented Generation (RAG)?Large-language modals (LLMs) like OpenAI\'s GPT-4 and Anthropic\'s Claude are incredible at generating coherent and contextually relevant text based on given prompts. They can assist in a wide range of tasks, such as writing, translation, and even conversation.Despite this, LLMs have limitations. In this guide, we\'ll go over these constraints and explain how Retrieval Augmented Generation (RAG) can alleviate these pains. We\'ll also dive into the ways you can build better chat experiences with this technique.The problem with LLMsAs groundbreaking as LLMs may be, they have a few limitations:They\'re limited by the amount of training data they have access to. For example, GPT-4 has a training data cutoff date, which means that it doesn\'t have access to information beyond that date. This limitation affects the model\'s ability to generate up-to-date and accurate responses.They\'re generic and lack subject-matter expertise. LLMs are trained on a large dataset that covers a wide range of topics, but they don\'t possess specialized knowledge in any particular field. This leads to hallucinations or inaccurate information when asked about specific subject areas.Citations are tricky. LLMs don\'t have a reliable way of returning the exact location of the text where they retrieved the information. This exacerbates the issue of hallucination, as they may not be able to provide proper attribution or verify the accuracy of their responses. Additionally, the lack of specific citations makes it difficult for users to fact-check or delve deeper into the information provided by the models.Retrieval Augmented GenerationTo solve this problem, researchers at Meta published a paper about a technique called Retrieval Augmented Generation (RAG), which adds an information retrieval component to the text generation model that LLMs are already good at. This allows for fine-tuning and adjustments to the LLM\'s internal knowledge, making it more accurate and up-to-date.Source: Lewis et el. (2021)\xa0–\xa0Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksHere\'s how RAG works on a high level:RAG puts together a pre-trained system that finds relevant information (retriever) with another system that generates text (generator).Then, when the user inputs a question (query), the retriever use a technique called ""Maximum Inner Product Search (MIPS)"" to find the most relevant documents. The information from these documents will then be fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.Using RAG in Chat ApplicationsTo illustrate how you can apply RAG in a real-world application, here\'s a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel\'s AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:Setting up a Next.js applicationCreating a chatbot frontend componentBuilding an API endpoint using OpenAI\'s API for response generationProgressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments. By following the tutorial, you\'ll build a context-aware chatbot with improved user experience.Build better AI chat experiences with RAGBy integrating Retrieval Augmented Generation into chat applications like the Pinecone chatbot template above, developers can reduce hallucinations in their AI models and create more accurate and evidence-based conversational experiences.If you\'re interested in learning more about RAG, check out this article about integrating RAG with Langchain and a Supabase vector database.Couldn\'t find the guide you need?View Help© 2023ProductPreviewsNext.jsInfrastructurev0Edge FunctionsTurboAnalyticsEnterpriseChangelogCLI & APIResourcesDocsExpertsPricingGuidesCustomersHelpIntegrations⌘KTemplatesCompanyAboutBlogCareersContact UsNext.js ConfOpen SourcePartnersSecurityPrivacy PolicyLegal']","Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generative models to deliver accurate responses. It is used to solve the problem of hallucinations or inaccurate information when asked about specific subject areas.

RAG works by first finding the most relevant documents to the user's query. The information from these documents is then fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.

To illustrate how you can apply RAG in a real-world application, here's a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:

```
npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""
```

The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel's AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:

* Setting up a Next.js application
* Creating a chatbot frontend component
* Building an API endpoint using OpenAI's API for response generation
* Progressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments.

By following the tutorial, you'll build a context-aware chatbot with improved user experience."
5,5,Retrieval Augmented Generation (RAG),You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data ...,https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html,"['Retrieval Augmented Generation (RAG) - Amazon SageMakerRetrieval Augmented Generation (RAG) - Amazon SageMakerAWSDocumentationAmazon SageMakerDeveloper GuideRetrieval Augmented Generation (RAG)Foundation models are usually trained offline, making the model agnostic to any', '                data that is created after the model was trained. Additionally, foundation models', '                are trained on very general domain corpora, making them less effective for', '                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve', '                data from outside a foundation model and augment your prompts by adding the relevant', '                retrieved data in context. For more information about RAG model architectures, see', '                    Retrieval-Augmented Generation for', '                    Knowledge-Intensive NLP Tasks.With RAG, the external data used to augment your prompts can come from multiple', '                data sources, such as a document repositories, databases, or APIs. The first step is', '                to convert your documents and any user queries into a compatible format to perform', '                relevancy search. To make the formats compatible, a document collection, or', '                knowledge library, and user-submitted queries are converted to numerical', '                representations using embedding language models. Embedding is the process by which text is given numerical', '                representation in a vector space. RAG model architectures compare the embeddings of', '                user queries within the vector of the knowledge library. The original user prompt is', '                then appended with relevant context from similar documents within the knowledge', '                library. This augmented prompt is then sent to the foundation model. You can update', '                knowledge libraries and their relevant embeddings asynchronously.', 'For more information, see the following example notebooks: ', 'Retrieval-Augmented Generation: Question Answering based on Custom', '                            Dataset', 'Retrieval-Augmented Generation: Question Answering based on Custom', ""                            Dataset with Open-sourced LangChain Library Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsFine-tuningTask-Specific ModelsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.""]","Retrieval Augmented Generation (RAG) is a technique that can be used to improve the performance of foundation models on domain-specific tasks.

RAG works by retrieving relevant data from outside the foundation model and augmenting the user's prompt with this data. This augmented prompt is then sent to the foundation model, which can then generate more relevant results.

For more information, see the following example notebooks:

* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset](https://github.com/aws-samples/sagemaker-rag-qa)
* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library](https://github.com/aws-samples/sagemaker-rag-qa-langchain)"
6,6,What Is Retrieval-Augmented Generation? | Definition from ...,Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the ...,https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation,"['5XX Level Error', 'Sorry something went wrong. Contact customer support by email at : ', 'Please reference the following information : ', 'Bad gateway', 'The web server reported a bad gateway error.', 'Ray ID: 812c911561a70501', 'Your IP address: 2402:800:63eb:bb88:f1c8:f72d:7fcd:aabd', 'Error reference number: 502', 'Cloudflare Location: Hong Kong', '[Go Back]']","The following error occurred:

5XX Level Error: Bad gateway

The web server reported a bad gateway error.

Contact customer support by email at:

[Go Back]"
7,7,Retrieval-Augmented Generation (RAG) in AI,"Sep 29, 2023 — ",https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/,"['Retrieval-Augmented Generation (RAG) in AI', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'Home', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'What is Retrieval-Augmented Generation (RAG) in AI?', 'Facebook', 'Twitter', 'Linkedin', 'Soumyadarshan Dash —', 'Updated On September 29th, 2023 ', 'Advanced', 'Artificial Intelligence', 'Excel', 'Generative AI', 'Healthcare', 'LLMs', 'NLP', 'Introduction', 'The rapid advancements in Large Language Models (LLMs) have transformed the landscape of AI, offering unparalleled capabilities in natural language understanding and generation. LLMs have ushered in a new language understanding and generation era, with OpenAI’s GPT models at the forefront. These remarkable models honed on extensive online data, have broadened our horizons, enabling us to interact with AI-powered systems like never before. However, like any technological marvel, they come with their own set of limitations. One glaring issue is their occasional tendency to provide information that is either inaccurate or outdated. Moreover, these LLMs do not furnish the sources of their responses, making it challenging to verify the reliability of their output. This limitation becomes especially critical in contexts where accuracy and traceability are paramount. Retrieval Augmented Generation (RAG) in AI is a transformative paradigm that promises to revolutionize the capabilities of LLMs.', 'Rapid advancements in LLMs have propelled them to the forefront of AI, yet they still grapple with constraints like information capacity and occasional inaccuracies. RAG bridges these gaps by seamlessly integrating retrieval-based and generative components, endowing LLMs to tap into external knowledge sources. This article explores RAG’s profound impact, unraveling its architecture, benefits, challenges, and the diverse approaches that empower it. In doing so, we unveil the potential of RAG to redefine the landscape of Large Language Models and pave the way for more accurate, context-aware, and reliable AI-driven communication.', 'Learning Objectives', 'Learn about language models and how RAG enhances their capabilities.', 'Discover methods to integrate external data into RAG systems effectively.', 'Explore ethical issues in RAG, including bias and privacy.', 'Gain hands-on experience with RAG using LangChain for real-world applications.', 'This article was published as a part of the\xa0Data Science Blogathon.', 'Table of contentsIntroductionUnderstanding Retrieval Augmented Generation (RAG)The Power of External DataBenefits of Retrieval Augmented Generation (RAG)Diverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsUtilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)OutputConclusionFrequently Asked Questions', 'Understanding Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.', 'Do you want to know more about RAG? Read more here. ', 'What is the Need for RAG?', 'The development of RAG is a direct response to the limitations of Large Language Models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG aims to bridge this gap by offering a solution that excels in understanding user intent and delivering meaningful and context-aware replies.', 'The Fusion of Retrieval-Based and Generative Models', 'RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.', 'Deconstructing RAG’s Mechanics', 'To grasp the essence of RAG, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps. ', 'Begin by receiving and processing user input.', 'Analyze the user input to understand its meaning and intent.', 'Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.', 'Use the retrieved external knowledge to enhance comprehension.', 'Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.', 'Combine all the information gathered to produce responses that are meaningful and human-like.', 'Ensure that the transformation of user queries into responses is done effectively.', 'The Role of Language Models and User Input', 'Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects through its integration of retrieval and generation.', 'Incorporating External Knowledge Sources', 'RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.', 'Generating Contextual Responses', 'Ultimately, the hallmark of RAG is its ability to generate contextual responses. It considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. These context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.', 'Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. By harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.', 'The Power of External Data', 'In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.', 'APIs and Real-time Databases', 'APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They allow models to access the latest data as it becomes available.', 'Document Repositories', 'Document repositories serve as valuable knowledge stores, offering structured and unstructured information. They are fundamental in expanding the knowledge base that RAG models can draw upon.', 'Webpages and Scraping', 'Web scraping is a method for extracting information from web pages. It enables RAG models to access dynamic web content, making it a crucial source for real-time data retrieval.', 'Databases and Structured Information', 'Databases provide structured data that can be queried and extracted. RAG models can use databases to retrieve specific information, enhancing the accuracy of their responses.', 'Benefits of Retrieval Augmented Generation (RAG)', 'Enhanced LLM Memory', 'RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.', 'Improved Contextualization', 'RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.', 'Updatable Memory', 'A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. This keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.', 'Source Citations', 'RAG-equipped models can provide sources for their responses, enhancing transparency and credibility. Users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.', 'Reduced Hallucinations', 'Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.', 'These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing, overcoming the limitations of traditional language models and enhancing the capabilities of AI-powered applications.', 'Diverse Approaches in RAG', 'RAG offers a spectrum of approaches for the retrieval mechanism, catering to various needs and scenarios:', 'Simple: Retrieve relevant documents and seamlessly incorporate them into the generation process, ensuring comprehensive responses.', 'Map Reduce: Combine responses generated individually for each document to craft the final response, synthesizing insights from multiple sources.', 'Map Refine: Iteratively refine responses using initial and subsequent documents, enhancing response quality through continuous improvement.', 'Map Rerank: Rank responses and select the highest-ranked response as the final answer, prioritizing accuracy and relevance.', 'Filtering: Apply advanced models to filter documents, utilizing the refined set as context for generating more focused and contextually relevant responses.', 'Contextual Compression: Extract pertinent snippets from documents, generating concise and informative responses and minimizing information overload.', 'Summary-Based Index: Leverage document summaries, index document snippets, and generate responses using relevant summaries and snippets, ensuring concise yet informative answers.', 'Forward-Looking Active Retrieval Augmented Generation (FLARE): Predict forthcoming sentences by initially retrieving relevant documents and iteratively refining responses. Flare ensures a dynamic and contextually aligned generation process.', 'These diverse approaches empower RAG to adapt to various use cases and retrieval scenarios, allowing for tailored solutions that maximize AI-generated responses’ relevance, accuracy, and efficiency.', 'Ethical Considerations in RAG', 'RAG introduces ethical considerations that demand careful attention:', 'Ensuring Fair and Responsible Use: Ethical deployment of RAG involves using the technology responsibly and refraining from any misuse or harmful applications. Developers and users must adhere to ethical guidelines to maintain the integrity of AI-generated content.', 'Addressing Privacy Concerns: RAG’s reliance on external data sources may involve accessing user data or sensitive information. Establishing robust privacy safeguards to protect individuals’ data and ensure compliance with privacy regulations is imperative.', 'Mitigating Biases in External Data Sources: External data sources can inherit biases in their content or collection methods. Developers must implement mechanisms to identify and rectify biases, ensuring AI-generated responses remain unbiased and fair. This involves constant monitoring and refinement of data sources and training processes.', 'Applications of Retrieval Augmented Generation (RAG)', 'RAG finds versatile applications across various domains, enhancing AI capabilities in different contexts:', 'Chatbots and AI Assistants: RAG-powered systems excel in question-answering scenarios, providing context-aware and detailed answers from extensive knowledge bases. These systems enable more informative and engaging interactions with users.', 'Education Tools: RAG can significantly improve educational tools by offering students access to answers, explanations, and additional context based on textbooks and reference materials. This facilitates more effective learning and comprehension.', 'Legal Research and Document Review: Legal professionals can leverage RAG models to streamline document review processes and conduct efficient legal research. RAG assists in summarizing statutes, case law, and other legal documents, saving time and improving accuracy.', 'Medical Diagnosis and Healthcare: In the healthcare domain, RAG models serve as valuable tools for doctors and medical professionals. They provide access to the latest medical literature and clinical guidelines, aiding in accurate diagnosis and treatment recommendations.', 'Language Translation with Context: RAG enhances language translation tasks by considering the context in knowledge bases. This approach results in more accurate translations, accounting for specific terminology and domain knowledge, particularly valuable in technical or specialized fields.', 'These applications highlight how RAG’s integration of external knowledge sources empowers AI systems to excel in various domains, providing context-aware, accurate, and valuable insights and responses.', 'The Future of RAGs and LLMs', 'The evolution of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) is poised for exciting developments:', 'Advancements in Retrieval Mechanisms: The future of RAG will witness refinements in retrieval mechanisms. These enhancements will focus on improving the precision and efficiency of document retrieval, ensuring that LLMs access the most relevant information quickly. Advanced algorithms and AI techniques will play a pivotal role in this evolution.', 'Integration with Multimodal AI: The synergy between RAG and multimodal AI, which combines text with other data types like images and videos, holds immense promise. Future RAG models will seamlessly incorporate multimodal data to provide richer and more contextually aware responses. This will open doors to innovative applications like content generation, recommendation systems, and virtual assistants.', 'RAG in Industry-Specific Applications: As RAG matures, it will find its way into industry-specific applications. Healthcare, law, finance, and education sectors will harness RAG-powered LLMs for specialized tasks. For example, in healthcare, RAG models will aid in diagnosing medical conditions by instantly retrieving the latest clinical guidelines and research papers, ensuring doctors have access to the most current information.', 'Ongoing Research and Innovation in RAG: The future of RAG is marked by relentless research and innovation. AI researchers will continue to push the boundaries of what RAG can achieve, exploring novel architectures, training methodologies, and applications. This ongoing pursuit of excellence will result in more accurate, efficient, and versatile RAG models.', 'LLMs with Enhanced Retrieval Capabilities: LLMs will evolve to possess enhanced retrieval capabilities as a core feature. They will seamlessly integrate retrieval and generation components, making them more efficient at accessing external knowledge sources. This integration will lead to LLMs that are proficient in understanding context and excel in providing context-aware responses.', 'Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)', 'Installation of LangChain and OpenAI Libraries', 'This line of code installs the LangChain and OpenAI libraries. LangChain is critical for handling text data and embedding, while OpenAI provides access to state-of-the-art Large Language Models (LLMs). This installation step is essential for setting up the required tools for RAG.', '!pip install langchain openai', '!pip install -q -U faiss-cpu tiktoken', 'import os', 'import getpass', 'os.environ[""OPENAI_API_KEY""] = getpass.getpass(""Open AI API Key:"")', 'Web Data Loading for the RAG Knowledge Base', 'The code utilizes LangChain’s “WebBaseLoader.”', 'Three web pages are specified for data retrieval: YOLO-NAS object detection, DeciCoder’s code generation efficiency, and a Deep Learning Daily newsletter.', 'This step is essential for building the knowledge base used in RAG, enabling contextually relevant and accurate information retrieval and integration into language model responses.', 'from langchain.document_loaders import WebBaseLoader', 'yolo_nas_loader = WebBaseLoader(""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"").load()', 'decicoder_loader = WebBaseLoader(""https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder\'s%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency."").load()', 'yolo_newsletter_loader = WebBaseLoader(""https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas"").load()', 'Embedding and Vector Store Setup', 'The code sets up embeddings for the RAG process.', 'It uses “OpenAIEmbeddings” to create an embedding model.', 'A “CacheBackedEmbeddings” object is initialized, allowing embeddings to be stored and retrieved efficiently using a local file store.', 'A “FAISS” vector store is created from the preprocessed chunks of web data (yolo_nas_chunks, decicoder_chunks, and yolo_newsletter_chunks), enabling fast and accurate similarity-based retrieval.', 'Finally, a retriever is instantiated from the vector store, facilitating efficient document retrieval during the RAG process.', 'from langchain.embeddings.openai import OpenAIEmbeddings', 'from langchain.embeddings import CacheBackedEmbeddings', 'from langchain.vectorstores import FAISS', 'from langchain.storage import LocalFileStore', 'store = LocalFileStore(""./cachce/"")', '# create an embedder', 'core_embeddings_model = OpenAIEmbeddings()', 'embedder = CacheBackedEmbeddings.from_bytes_store(', '    core_embeddings_model,', '    store,', '    namespace = core_embeddings_model.model', ')', '# store embeddings in vector store', 'vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)', 'vectorstore.add_documents(decicoder_chunks)', 'vectorstore.add_documents(yolo_newsletter_chunks)', '# instantiate a retriever', 'retriever = vectorstore.as_retriever()', 'Establishing the Retrieval System', 'The code configures the retrieval system for Retrieval Augmented Generation (RAG).', 'It uses “OpenAIChat” from the LangChain library to set up a chat-based Large Language Model (LLM).', 'A callback handler named “StdOutCallbackHandler” is defined to manage interactions with the retrieval system.', 'The “RetrievalQA” chain is created, incorporating the LLM, retriever (previously initialized), and callback handler.', 'This chain is designed to perform retrieval-based question-answering tasks, and it is configured to return source documents for added context during the RAG process.', 'from langchain.llms.openai import OpenAIChat', 'from langchain.chains import RetrievalQA', 'from langchain.callbacks import StdOutCallbackHandler', 'llm = OpenAIChat()', 'handler =  StdOutCallbackHandler()', '# This is the entire retrieval system', 'qa_with_sources_chain = RetrievalQA.from_chain_type(', '    llm=llm,', '    retriever=retriever,', '    callbacks=[handler],', '    return_source_documents=True', ')', 'Initializes the RAG System', 'The code sets up a RetrievalQA chain, a critical part of the RAG system, by combining an OpenAIChat language model (LLM) with a retriever and callback handler.', 'Issue Queries to the RAG System', 'It sends various user queries to the RAG system, prompting it to retrieve contextually relevant information.', 'Retrieves Responses', 'After processing the queries, the RAG system generates and returns contextually rich and accurate responses. The responses are printed on the console.', '# This is the entire augment system!', 'response = qa_with_sources_chain({""query"":""What does Neural Architecture Search have to do with how Deci creates its models?""})', 'response', ""print(response['result'])"", ""print(response['source_documents'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder""})', ""print(response['result'])"", 'This code exemplifies how RAG and LangChain can enhance information retrieval and generation in AI applications.', 'Output', 'Conclusion', 'Retrieval Augmented Generation (RAG) represents a transformative leap in artificial intelligence. It seamlessly integrates Large Language Models (LLMs) with external knowledge sources, addressing the limitations of LLMs’ parametric memory.', 'RAG’s ability to access real-time data, coupled with improved contextualization, enhances the relevance and accuracy of AI-generated responses. Its updatable memory ensures responses are current without extensive model retraining. RAG also offers source citations, bolstering transparency and reducing data leakage. In summary, RAG empowers AI to provide more accurate, context-aware, and reliable information, promising a brighter future for AI applications across industries.', 'Key Takeaways', 'Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.', 'RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.', 'With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.', 'RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.', 'This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.', 'Frequently Asked Questions', 'Q1. What is RAG? How does it differ from traditional AI models? A. RAG, or Retrieval Augmented Generation, is an innovative AI framework combining retrieval-based and generative models’ strengths. Unlike traditional AI models, which generate responses solely based on their pre-trained knowledge, RAG integrates external knowledge sources, allowing it to provide more accurate, up-to-date, and contextually relevant responses.  Q2. How does RAG ensure the accuracy of the retrieved information? A. RAG employs a retrieval system that fetches information from external sources. It ensures accuracy through techniques like vector similarity search and real-time updates to external datasets. Additionally, RAG allows users to access source citations, enhancing transparency and credibility.  Q3. Can RAG be used in specific industries or applications? A. Yes, RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.  Q4. Does implementing RAG require extensive technical expertise? A. While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.  Q5. What are the potential ethical concerns with RAG, such as misinformation or data privacy? A. RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns.  ', 'The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\xa0', 'Related', 'AIApplicationsblogathonData Sourcesdocumentslanguage modelsmemoryModels ', 'Recommended For You', 'Become a full stack data scientist', 'Large Language Models Demystified: A Beginner’s Roadmap', 'Training Your Own LLM Without Coding', 'How to Build LLMs for Code?\xa0', 'LLMs in Conversational AI: Building Smarter Chatbots & Assistants', 'From GPT-3 to Future Generations of Language Models', 'What are Large Language Models (LLMs)?', 'About the Author', 'Soumyadarshan Dash', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'How to Explore Text Generation with GPT-2? ', 'Next Post', 'How does Generative AI in Recipe Generation and Culinary Arts Work? ', 'Top Resources', '10 Best AI Image Generator Tools to Use in 2023', 'avcontentteam - ', 'Aug 17, 2023', 'Everything you need to Know about Linear Regression!', 'KAVITA MALI - ', 'Oct 04, 2021', 'Skewness and Kurtosis: Quick Guide (Updated 2023)', 'Suvarna Gawali - ', 'May 02, 2021', 'Guide on Support Vector Machine (SVM) Algorithm', 'Anshul Saini - ', 'Oct 12, 2021', '×', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Join the Community', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2023 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'Loading...', 'To continue reading please login', 'google-icon', 'Continue with Google', 'linkedin', 'Continue with Linkedin', 'email', 'Continue with Email', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Welcome Back :)', 'email 2', 'key-', 'Forgot Password?', 'Log In', ""Don't have an account yet?Register here"", 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Start your journey here!', 'user', 'user', 'email 2', 'key-', 'Sign up', 'Already have an accountLogin here', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', ' A verification link has been sent to your email id ', ' If you have not recieved the link please goto', 'Sign Up  page again', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your registered email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter your registered email id', 'email 2', 'Next', 'This email id is not registered with us. Please enter your registered email id.', ""Don't have an account yet?Register here"", 'Loading...', '×', 'back', 'Please enter the OTP that is sent your registered email id', 'email 2', 'Next', 'Loading...', '×', 'Please create the new password here', 'key-', 'key-', 'Submit', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview ', 'This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.', 'Necessary ', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. ', 'Non-necessary ', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. ', 'SAVE & ACCEPT', '×']","**Summarize the article in 10 bullet points**
- Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.
- RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.
- With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.
- RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.
- This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.
- RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.
- While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.
- RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns."
8,8,What Is Retrieval-Augmented Generation (RAG)? - Oracle,,"https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/#:~:text=Retrieval%2Daugmented%20generation%20is%20a,already%20contained%20in%20the%20LLM.","['What Is Retrieval-Augmented Generation (RAG)?', 'Accessibility Policy', 'Skip to content', 'About', 'Services', 'Solutions', 'Pricing', 'Partners', 'Resources', 'Close Search', 'Close', 'We’re sorry.  We could not find a match for your search.', ""We suggest you try the following to help find what you're looking for:"", 'Check the spelling of your keyword search.', 'Use synonyms for the keyword you typed, for example, try “application” instead of “software.”', 'Start a new search.', 'Clear Search', 'Search', 'Menu', 'Menu', 'Contact Sales', 'Sign in to Oracle Cloud', 'Cloud', 'Artificial Intelligence', 'Generative AI', 'What Is Retrieval-Augmented Generation (RAG)?', 'Alan Zeichick | Tech Content Strategist | September 19, 2023', 'In This Article', 'What Is Retrieval-Augmented Generation (RAG)?', 'Retrieval-Augmented Generation Explained', 'How Does Retrieval-Augmented Generation Work?', 'Using RAG in Chat Applications', 'Benefits of Retrieval-Augmented Generation', 'Challenges of Retrieval-Augmented Generation', 'Examples of Retrieval-Augmented Generation', 'Future of Retrieval-Augmented Generation', 'Generative AI With Oracle', 'Retrieval-Augmented Generation FAQs', 'Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.', 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees.', 'What Is Retrieval-Augmented Generation (RAG)?', 'That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.', 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.', 'Retrieval-Augmented Generation Explained', 'Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.', 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. ', 'Simply put, RAG helps LLMs give better answers.', 'Key Takeaways', 'RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.', 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.', 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.', 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.', 'How Does Retrieval-Augmented Generation Work? ', 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.', ' The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.', 'RAG and Large Language Models (LLMs)', 'Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.', 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.', 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.', 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.', 'Retrieval-Augmented Generation vs. Semantic Search', ' RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.', ' Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.', ' Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG.', 'Using RAG in Chat Applications', 'When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.', ' The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.', 'Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.', 'Benefits of Retrieval-Augmented Generation', 'RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following:', 'The RAG has access to information that may be fresher than the data used to train the LLM.', 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.', 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.', 'The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted.', 'Challenges of Retrieval-Augmented Generation', 'Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are', 'Improving organizational knowledge and understanding of RAG because it’s so new', 'Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself', 'Determining how to best model the structured and unstructured data within the knowledge library and vector database', 'Developing requirements for a process to incrementally feed data into the RAG system', 'Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system', 'Examples of Retrieval-Augmented Generation', 'There are many possible examples of generative AI augmented by RAG.', 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.', 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.', 'Future of Retrieval-Augmented Generation', 'Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.', 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.', 'RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.', 'Generative AI With Oracle', 'Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.', 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.', 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot.', 'What makes Oracle best suited for generative AI?', 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.', 'Learn more about Oracle’s generative AI strategy', 'Retrieval-Augmented Generation FAQs', 'Is RAG the same as generative AI?', 'No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.', 'What type of information is used in RAG?', 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.', 'How does generative AI use RAG?', 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.', 'Can a RAG cite references for the data it retrieves?', 'Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.', 'Resources for', 'Careers', 'Developers', 'Investors', 'Partners', 'Startups', 'Students and Educators', 'Why Oracle', 'Analyst Reports', 'Cloud Economics', 'with Microsoft Azure', 'vs. AWS', 'vs. Google Cloud', 'vs. MongoDB', 'Learn', 'What is AI?', 'What is Cloud Computing?', 'What is Cloud Storage?', 'What is HPC?', 'What is IaaS?', 'What is PaaS?', 'What’s new', 'Oracle Supports Ukraine', 'Oracle Cloud Free Tier', 'Cloud Architecture Center', 'Cloud Lift', 'Oracle Support Rewards', 'Oracle Red Bull Racing', 'Contact us', 'US Sales: +1.800.633.0738', 'How can we help?', 'Subscribe to emails', 'Events', 'News', 'OCI Blog', ' Country/Region ', '© 2023 Oracle', 'Privacy/Do Not Sell My Info', 'Ad Choices', 'Careers', 'Facebook', 'Twitter', 'LinkedIn', 'YouTube']","Retrieval-augmented generation (RAG) is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.

RAG models build knowledge repositories based on the organization's own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.

Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.

Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM."
9,9,What is retrieval-augmented generation?,"Aug 22, 2023 — ",https://research.ibm.com/blog/retrieval-augmented-generation-RAG,"[""What is retrieval-augmented generation? | IBM Research BlogSkip to main contentResearchFocus areasBlogPublicationsCareersAboutBackFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBackAboutOverviewLabsPeopleCollaborateBackArtificial IntelligenceBackHybrid CloudBackQuantum ComputingBackScienceBackSecurityBackSemiconductorsBackOverviewBackLabsBackPeopleBackCollaborateResearchFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBlogPublicationsCareersAboutOverviewLabsPeopleCollaborateOpen IBM search fieldClose22 Aug 2023Explainer4 minute readWhat is retrieval-augmented generation?RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.Large language models can be inconsistent. Sometimes they nail the answer to questions, other times they regurgitate random facts from their training data. If they occasionally sound like they have no idea what they’re saying, it’s because they don’t. LLMs know how words relate statistically, but not what they mean.Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.“You want to cross-reference a model’s answers with the original content so you can see what it is basing its answer on,” said Luis Lastras, director of language technologies at IBM Research.RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. IBM unveiled its new AI and data platform, watsonx, which offers RAG, back in May.An ‘open book’ approach to answering tough questionsUnderpinning all foundation models, including LLMs, is an AI architecture known as the transformer. It turns heaps of raw data into a compressed representation of its basic structure. Starting from this raw representation, a foundation model can be adapted to a variety of tasks with some additional fine-tuning on labeled, domain-specific knowledge.But fine-tuning alone rarely gives the model the full breadth of knowledge it needs to answer highly specific questions in an ever-changing context. In a 2020 paper, Meta (then known as Facebook) came up with a framework called retrieval-augmented generation to give LLMs access to information beyond their training data. RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”As the name suggests, RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.Toward personalized and verifiable responsesBefore LLMs, digital conversation agents followed a manual dialogue flow. They confirmed the customer’s intent, fetched the requested information, and delivered an answer in a one-size-fits all script. For straightforward queries, this manual decision-tree method worked just fine.But it had limitations. Anticipating and scripting answers to every question a customer might conceivably ask took time; if you missed a scenario, the chatbot had no ability to improvise. Updating the scripts as policies and circumstances evolved was either impractical or impossible.Today, LLM-powered chatbots can give customers more personalized answers without humans having to write out new scripts. And RAG allows LLMs to go one step further by greatly reducing the need to feed and retrain the model on fresh examples. Simply upload the latest documents or policies, and the model retrieves the information in open-book mode to answer the question.IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources.Teaching the model to recognize when it doesn’t knowCustomer queries aren’t always this straightforward. They can be ambiguously worded, complex, or require knowledge the model either doesn’t have or can’t easily parse. These are the conditions in which LLMs are prone to making things up.“Think of the model as an overeager junior employee that blurts out an answer before checking the facts,” said Lastras. “Experience teaches us to stop and say when we don’t know something. But LLMs need to be explicitly trained to recognize questions they can’t answer.”In a more challenging scenario taken from real life, Alice wants to know how many days of maternity leave she gets. A chatbot that does not use RAG responds cheerfully (and incorrectly): “Take as long as you want.”Maternity-leave policies are complex, in part, because they vary by the state or country of the employee’s home-office. When the LLM failed to find a precise answer, it should have responded, “I’m sorry, I don’t know,” said Lastras, or asked additional questions until it could land on a question it could definitively answer. Instead, it pulled a phrase from a training set stocked with empathetic, customer-pleasing language.With enough fine-tuning, an LLM can be trained to pause and say when it’s stuck. But it may need to see thousands of examples of questions that can and can’t be answered. Only then can the model learn to identify an unanswerable question, and probe for more detail until it hits on a question that it has the information to answer.RAG is currently the best-known tool for grounding LLMs on the latest, verifiable information, and lowering the costs of having to constantly retrain and update them. But RAG is imperfect, and many interesting challenges remain in getting RAG done right.At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.Subscribe to our Future Forward newsletter and stay up to date on the latest research newsSubscribe to our newsletterHome↳ BlogDate22 Aug 2023AuthorsKim MartineauTopicsAIExplainable AIGenerative AINatural Language ProcessingTrustworthy GenerationShareWhat is AI inferencing?ExplainerKim Martineau05 Oct 2023AIFoundation ModelsGenerative AIHybrid CloudCelebrating a decade of IBM Research innovation in AfricaNewsMike Murphy28 Sep 2023AIQuantumFind and fix IT glitches before they crash the system NewsKim Martineau28 Sep 2023AI for CodeAI for ITExplainable AIFoundation ModelsGenerative AIHow open source paved the way for computing from anywhere NewsKim Martineau20 Sep 2023AIHybrid Cloud PlatformMachine LearningScaling AIPreviousError correcting codes for near-term quantum computersNextIBM Research’s newest prototype chips use drastically less power to solve AI tasksIBM LogoFocus areasFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usTwitterLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility""]","Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.

RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.

This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.

IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.

To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources."
10,10,Retrieval Augmented Generation (RAG),RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient ...,https://www.promptingguide.ai/techniques/rag,"['Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuidePrompt Engineering CoursePrompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingSelf-ConsistencyGenerate Knowledge PromptingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingReActMultimodal CoTGraph PromptingApplicationsProgram-Aided Language ModelsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionModelsFlanChatGPTLLaMAGPT-4LLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesPapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightQuestion? Give us feedback → (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)', ""General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge."", 'For more complex and knowledge-intensive tasks, it\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of ""hallucination"".', 'Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.', ""RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."", 'Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:', 'Image Source: Lewis et el. (2021) (opens in a new tab)', 'RAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.', 'This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.', 'More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.', 'You can find a simple example of how to use retrievers and LLMs for question answering with sources (opens in a new tab) from the LangChain documentation.Tree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright © 2023 DAIR.AI']","Retrieval Augmented Generation (RAG) is a method proposed by Meta AI researchers to address knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.

RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."
11,11,Retrieval Augmented Generation (RAG): Reducing ...,Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to ...,https://www.pinecone.io/learn/retrieval-augmented-generation/,"['Retrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI Applications | PineconeProductSolutionsPricingResourcesCompanyLog InSign Up FreeLearn | ArticleRetrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI ApplicationsJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Products built on top of Large Language Models (LLMs) such as OpenAI\'s ChatGPT and Anthropic\'s Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.LLMs are “stuck” at a particular time, but RAG can bring them into the present.ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”The LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.The reason LLMs like ChatGPT feel so bright is that they\'ve seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.RAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.Why is RAG the preferred approach from a cost-efficacy perspective?There are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.Fine-tuning: adapting a foundation model to your domain’s data.Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.Prompt engineering is insufficient for reducing hallucinations.Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.It’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.Let’s now take a deeper dive into how Retrieval Augmented Generation works.We’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window"".An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.Creating a vector database from your domain-specific proprietary data using an embedding model.This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.You originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.You send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM\'s context window.Since the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.Vector databases can support even more advanced search functionality.Semantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query\'s semantics and keywords.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.Pinecone enables you to integrate RAG within minutes. Check out our examples repository on GitHub for runnable examples, such as this RAG Jupyter Notebook.Share via:  Zachary ProserStaff Developer AdvocateJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.PRODUCTOverviewDocumentationTrust and SecuritySOLUTIONSSearchGenerative AICustomersRESOURCESLearning CenterCommunityPinecone BlogSupport CenterSystem StatusCOMPANYAboutPartnersCareersNewsroomContactLEGALTermsPrivacyCookies© Pinecone Systems, Inc. | San Francisco, CAPinecone is a registered trademark of Pinecone Systems, Inc.']","**Retrieval Augmented Generation (RAG) is a technique that helps Large Language Models (LLMs) produce more accurate and relevant responses by providing them with additional context from a database of proprietary data.**

RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.

**Here are the key benefits of using RAG:**

* **RAG reduces the likelihood of hallucinations.** LLMs are often inaccurate when asked about recent events or domain-specific information because they lack up-to-date context. RAG addresses this problem by providing LLMs with access to the most relevant information from a database of proprietary data.
* **RAG allows GenAI applications to cite their sources.** This improves auditability and makes it easier for end users to understand how GenAI applications arrive at their conclusions.
* **RAG is easy to implement.** RAG can be integrated with GenAI applications with just a few code changes.
* **RAG is cost-effective.** RAG does not require the same level of expertise or resources as other methods for improving the performance of GenAI applications, such as building a foundation model or fine-tuning an existing model.

**Here are the steps involved in using RAG:**

1. Create a vector database from your domain-specific proprietary data.
2. Convert your user's query into embeddings.
3. Send the embeddings to the vector database.
4. The vector database returns the most relevant results.
5. Provide the results to the LLM via its context window.

**RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.**"
12,12,Retrieval Augmented Generation (RAG),You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data ...,https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html,"['Retrieval Augmented Generation (RAG) - Amazon SageMakerRetrieval Augmented Generation (RAG) - Amazon SageMakerAWSDocumentationAmazon SageMakerDeveloper GuideRetrieval Augmented Generation (RAG)Foundation models are usually trained offline, making the model agnostic to any', '                data that is created after the model was trained. Additionally, foundation models', '                are trained on very general domain corpora, making them less effective for', '                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve', '                data from outside a foundation model and augment your prompts by adding the relevant', '                retrieved data in context. For more information about RAG model architectures, see', '                    Retrieval-Augmented Generation for', '                    Knowledge-Intensive NLP Tasks.With RAG, the external data used to augment your prompts can come from multiple', '                data sources, such as a document repositories, databases, or APIs. The first step is', '                to convert your documents and any user queries into a compatible format to perform', '                relevancy search. To make the formats compatible, a document collection, or', '                knowledge library, and user-submitted queries are converted to numerical', '                representations using embedding language models. Embedding is the process by which text is given numerical', '                representation in a vector space. RAG model architectures compare the embeddings of', '                user queries within the vector of the knowledge library. The original user prompt is', '                then appended with relevant context from similar documents within the knowledge', '                library. This augmented prompt is then sent to the foundation model. You can update', '                knowledge libraries and their relevant embeddings asynchronously.', 'For more information, see the following example notebooks: ', 'Retrieval-Augmented Generation: Question Answering based on Custom', '                            Dataset', 'Retrieval-Augmented Generation: Question Answering based on Custom', ""                            Dataset with Open-sourced LangChain Library Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsFine-tuningTask-Specific ModelsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.""]","Retrieval Augmented Generation (RAG) is a technique that can be used to improve the performance of foundation models on domain-specific tasks.

With RAG, the external data used to augment your prompts can come from multiple data sources, such as a document repositories, databases, or APIs. The first step is to convert your documents and any user queries into a compatible format to perform relevancy search. To make the formats compatible, a document collection, or knowledge library, and user-submitted queries are converted to numerical representations using embedding language models. Embedding is the process by which text is given numerical representation in a vector space. RAG model architectures compare the embeddings of user queries within the vector of the knowledge library. The original user prompt is then appended with relevant context from similar documents within the knowledge library. This augmented prompt is then sent to the foundation model. You can update knowledge libraries and their relevant embeddings asynchronously.

For more information, see the following example notebooks:

* Retrieval-Augmented Generation: Question Answering based on Custom Dataset
* Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library"
13,13,What is Retrieval Augmented Generation (RAG)?,"RAG puts together a pre-trained system that finds relevant information ( retriever ) with another system that generates text ( generator ). Then, when the user ...",https://vercel.com/guides/retrieval-augmented-generation,"['What is Retrieval Augmented Generation (RAG)?Skip to contentDocumentationGuidesHelp← Back to GuidesWhat is Retrieval Augmented Generation (RAG)?Large-language modals (LLMs) like OpenAI\'s GPT-4 and Anthropic\'s Claude are incredible at generating coherent and contextually relevant text based on given prompts. They can assist in a wide range of tasks, such as writing, translation, and even conversation.Despite this, LLMs have limitations. In this guide, we\'ll go over these constraints and explain how Retrieval Augmented Generation (RAG) can alleviate these pains. We\'ll also dive into the ways you can build better chat experiences with this technique.The problem with LLMsAs groundbreaking as LLMs may be, they have a few limitations:They\'re limited by the amount of training data they have access to. For example, GPT-4 has a training data cutoff date, which means that it doesn\'t have access to information beyond that date. This limitation affects the model\'s ability to generate up-to-date and accurate responses.They\'re generic and lack subject-matter expertise. LLMs are trained on a large dataset that covers a wide range of topics, but they don\'t possess specialized knowledge in any particular field. This leads to hallucinations or inaccurate information when asked about specific subject areas.Citations are tricky. LLMs don\'t have a reliable way of returning the exact location of the text where they retrieved the information. This exacerbates the issue of hallucination, as they may not be able to provide proper attribution or verify the accuracy of their responses. Additionally, the lack of specific citations makes it difficult for users to fact-check or delve deeper into the information provided by the models.Retrieval Augmented GenerationTo solve this problem, researchers at Meta published a paper about a technique called Retrieval Augmented Generation (RAG), which adds an information retrieval component to the text generation model that LLMs are already good at. This allows for fine-tuning and adjustments to the LLM\'s internal knowledge, making it more accurate and up-to-date.Source: Lewis et el. (2021)\xa0–\xa0Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksHere\'s how RAG works on a high level:RAG puts together a pre-trained system that finds relevant information (retriever) with another system that generates text (generator).Then, when the user inputs a question (query), the retriever use a technique called ""Maximum Inner Product Search (MIPS)"" to find the most relevant documents. The information from these documents will then be fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.Using RAG in Chat ApplicationsTo illustrate how you can apply RAG in a real-world application, here\'s a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel\'s AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:Setting up a Next.js applicationCreating a chatbot frontend componentBuilding an API endpoint using OpenAI\'s API for response generationProgressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments. By following the tutorial, you\'ll build a context-aware chatbot with improved user experience.Build better AI chat experiences with RAGBy integrating Retrieval Augmented Generation into chat applications like the Pinecone chatbot template above, developers can reduce hallucinations in their AI models and create more accurate and evidence-based conversational experiences.If you\'re interested in learning more about RAG, check out this article about integrating RAG with Langchain and a Supabase vector database.Couldn\'t find the guide you need?View Help© 2023ProductPreviewsNext.jsInfrastructurev0Edge FunctionsTurboAnalyticsEnterpriseChangelogCLI & APIResourcesDocsExpertsPricingGuidesCustomersHelpIntegrations⌘KTemplatesCompanyAboutBlogCareersContact UsNext.js ConfOpen SourcePartnersSecurityPrivacy PolicyLegal']","Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generative models to deliver accurate responses. It is used to solve the problem of hallucinations or inaccurate information when asked about specific subject areas.

RAG works by first finding the most relevant documents to the user's query. The information from these documents is then fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.

To illustrate how you can apply RAG in a real-world application, here's a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:

```
npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""
```

The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel's AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:

* Setting up a Next.js application
* Creating a chatbot frontend component
* Building an API endpoint using OpenAI's API for response generation
* Progressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments.

By following the tutorial, you'll build a context-aware chatbot with improved user experience."
14,14,What Is Retrieval-Augmented Generation? | Definition from ...,Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the ...,https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation,"['What Is Retrieval-Augmented Generation? | Definition from TechTarget', 'Enterprise AI', 'Search the TechTarget Network', 'Login', 'Register', 'Explore the Network', 'TechTarget Network', 'Business Analytics', 'CIO', 'Data Management', 'ERP', 'Enterprise AI', 'AI Business Strategies', 'AI Careers', 'AI Infrastructure', 'AI Platforms ', 'AI Technologies', 'More Topics', 'Applications of AI', 'ML Platforms', 'Other Content', 'News', 'Features', 'Tips', 'Webinars', '2023 IT Salary Survey Results', '                                More', 'Answers', 'Conference Guides', 'Definitions', 'Opinions', 'Podcasts', 'Quizzes', 'Tech Accelerators', 'Tutorials', 'Videos', 'Sponsored Communities', 'Follow:', 'Home', 'AI technologies', 'Tech Accelerator', 'What is generative AI? Everything you need to know', 'Prev', 'Next', 'Generative AI in the enterprise raises questions for CIOs', '15 of the best large language models', 'Download this guide1', 'X', 'Free Download', 'What is generative AI? Everything you need to know', 'The potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight.', 'This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.', 'Corporate Email Address:You forgot to provide an Email Address.This email address doesn’t appear to be valid.This email address is already registered. Please log in.You have exceeded the maximum character limit.Please provide a Corporate Email Address.I agree to TechTarget’s Terms of Use, Privacy Policy, and the transfer of my information to the United States for processing to provide me with relevant information as described in our Privacy Policy.Please check the box if you want to proceed.I agree to my information being processed by TechTarget and its Partners to contact me via phone, email, or other means regarding information relevant to my professional interests. I may unsubscribe at any time.Please check the box if you want to proceed.', 'By submitting my Email address I confirm that I have read and accepted the Terms of Use and Declaration of Consent.', 'Definition', 'retrieval-augmented generation ', 'Share this item with your network:', 'By', 'Alexander S. Gillis,', 'Technical Writer and Editor', 'What is retrieval-augmented generation?', 'Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.', ""LLMs are AI models that power chatbots such as OpenAI's ChatGPT and Google Bard. LLMs can understand, summarize, generate and predict new content. However, they can still be inconsistent and fail at some knowledge-intensive tasks -- especially tasks that are outside their initial training data or those that require up-to-date information and transparency about how they make their decisions. When this happens, the LLM can return false information, also known as an AI hallucination."", ""By retrieving information from external sources when the LLM's trained data isn't enough, the quality of LLM responses improves. Retrieving information from an online source, for example, enables the LLM to access current information that it wasn't initially trained on."", 'What does RAG do?', ""LLMs are commonly trained offline, making the model uncertain of any data that's created after the model was trained. RAG is used to retrieve data from outside the LLM, which then augments the user's prompts by adding relevant retrieved data in its response."", 'This article is part of', 'What is generative AI? Everything you need to know', 'Which also includes:', '15 of the best large language models', 'Will AI replace jobs? 9 job types that might be affected', 'Pros and cons of AI-generated content', 'This process helps reduce any apparent knowledge gaps and AI hallucinations. This can be important in fields that require as much up-to-date and accurate information as possible, such as healthcare.', 'How to use RAG with LLMs', 'RAG combines information retrieval with a text generator model. External knowledge can be retrieved from data sources, online sources, application programming interfaces, databases or document repositories. ', 'Using the example of a chatbot, once a user inputs a prompt, RAG summarizes that prompt using keywords or semantic data. The converted data is then sent to a search platform to retrieve the requested data, which is then sorted through based on relevancy.', 'The LLM then synthesizes the retrieved data with the augmented prompt and its internal training data to create a generated response that can be passed to the chatbot with sourced links for the user.', 'An LLM using RAG can pull from both internal and external data to return a response for users, ensuring it provides relevant information.', 'What are the benefits of RAG?', 'Benefits of a RAG model include the following:', 'Provides current information. RAG pulls information from relevant, reliable and up-to-date sources.', ""Increases user trust. Users can access the model's sources, which promotes transparency and trust in the content and lets users verify its accuracy."", 'Reduces AI hallucinations. Because LLMs are grounded to external data, the model has less of a chance to make up or return incorrect information.', ""Reduces computational and financial costs. Organizations don't have to spend time and resources to continuously train the model on new data."", 'Synthesizes information. RAG synthesizes data by combining relevant information from retrieval and generative models to produce a response.', 'Easier to train. Because RAG uses retrieved knowledge sources, the need to train the LLM on a massive amount of training data is reduced.', 'Can be used for multiple tasks. Aside from chatbots, RAG can be fine-tuned for a variety of specific use cases, such as text summarization and dialogue systems.', 'Learn more about generative AI models, such as VAEs, GANs, diffusion, transformers and NeRFs.', '\t\t\t\t\tThis was last updated in October 2023', '\t\t\tContinue Reading About retrieval-augmented generation', '7 generative AI challenges that businesses should consider', 'Generative AI ethics: 8 biggest concerns', 'Assessing different types of generative AI applications', 'Pros and cons of AI-generated content', 'Cohesity Turing aims AI tools at backup and ransomware', '\t\t\t\tRelated Terms', 'language modeling', 'Language modeling, or LM, is the use of various statistical and probabilistic techniques to determine the probability of a given ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'OpenAI', 'OpenAI is a private research laboratory that aims to develop and direct artificial intelligence (AI) in ways that benefit ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Salesforce Einstein', 'Salesforce Einstein refers to an integrated set of artificial intelligence (AI) technologies developed for the Salesforce ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Dig Deeper on AI technologies', 'Databricks improves support for generative AI models', 'By: Eric\xa0Avidon', '15 of the best large language models', 'By: Ben\xa0Lutkevich', 'LangChain', 'By: Cameron\xa0Hashemi-Pour', 'large language models (LLMs)', 'By: Sean\xa0Kerner', 'Sponsored News', '4 Things You Need to Know Now About Edge Computing', '–HPE', 'Supporting a more diverse management team', '–AWS', 'See More', 'Vendor Resources', ""What generative AI's rise means for the cybersecurity industry"", '–TechTarget ComputerWeekly.com', 'Fast Track Generative AI with Dell™ Poweredge™ XE9680', '–Dell Technologies & Intel®', 'Latest TechTarget resources', '\t\t\t\t\t\t\tBusiness Analytics', '\t\t\t\t\t\t\tCIO', '\t\t\t\t\t\t\tData Management', '\t\t\t\t\t\t\tERP', 'Business Analytics', 'MicroStrategy launches new suite of generative AI tools', ""The longtime analytics vendor's suite incorporates LLM technology to make users more efficient by enabling them to use natural ..."", 'Sisense unveils composable toolkit for app development', 'Compose SDK for Fusion is a composable set of APIs that enable developers to build customized advanced analytics applications to ...', 'SAS unveils plans to add generative AI to analytics suite', 'After holding off on integrating with LLMs until it could ensure data security and accurate outcomes, the vendor is making ...', 'CIO', '7 challenges with blockchain adoption and how to avoid them', 'Organizations tend to face the same hurdles when they try to implement blockchain. Knowing what they are could be the first big ...', 'U.S. antitrust law enforcers defend actions, lawsuits', 'The FTC and DOJ, which enforce U.S. antitrust law, are focused on reining in big tech through antitrust lawsuits and revising ...', 'Is quantum computing overhyped?', ""Quantum computing may be coming to the enterprise. Here's what to understand about the benefits it promises, the risks it poses ..."", 'Data Management', 'Amazon launches DataZone, a new data management service', ""The tech giant's new service provides data governance, collaboration and catalog capabilities that enable organizations to find ..."", 'Databricks improves support for generative AI models', 'A new service enables users to easily deploy privately built language models and uses a GPU-based architecture to optimize and ...', 'New Boomi AI tool enables natural language data integration', ""The iPaaS vendor's new capabilities are aimed at increasing efficiency by enabling customers to build pipelines and manage data ..."", 'ERP', 'Infor Enterprise Automation looks to ease RPA for ERP', 'Infor unveiled Enterprise Automation, which is designed to help customers bring RPA to Infor cloud ERP applications and Developer...', 'When integrating generative AI and ERP, focus on use cases', 'ERP vendors are rapidly introducing new AI functionality, but experts caution against the hype and advise using the ...', 'PLM and PDM software: Learn the differences', 'PLM and PDM software may seem similar, but they fulfill different needs for manufacturers. Learn the differences and which is ...', 'About Us', 'Editorial Ethics Policy', 'Meet The Editors', 'Contact Us', 'Advertisers', 'Partner with Us', 'Media Kit', 'Corporate Site', 'Contributors', 'Reprints', 'Answers', 'Definitions', 'E-Products', 'Events', 'Features', 'Guides', 'Opinions', 'Photo Stories', 'Quizzes', 'Tips', 'Tutorials', 'Videos', 'All Rights Reserved, ', 'Copyright 2018 - 2023, TechTarget', 'Privacy Policy', 'Cookie Preferences ', 'Cookie Preferences ', 'Do Not Sell or Share My Personal Information', 'Close']",
15,15,Retrieval-Augmented Generation (RAG) in AI,"Sep 29, 2023 — ",https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/,"['Retrieval-Augmented Generation (RAG) in AI', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'Home', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'What is Retrieval-Augmented Generation (RAG) in AI?', 'Facebook', 'Twitter', 'Linkedin', 'Soumyadarshan Dash —', 'Updated On September 29th, 2023 ', 'Advanced', 'Artificial Intelligence', 'Excel', 'Generative AI', 'Healthcare', 'LLMs', 'NLP', 'Introduction', 'The rapid advancements in Large Language Models (LLMs) have transformed the landscape of AI, offering unparalleled capabilities in natural language understanding and generation. LLMs have ushered in a new language understanding and generation era, with OpenAI’s GPT models at the forefront. These remarkable models honed on extensive online data, have broadened our horizons, enabling us to interact with AI-powered systems like never before. However, like any technological marvel, they come with their own set of limitations. One glaring issue is their occasional tendency to provide information that is either inaccurate or outdated. Moreover, these LLMs do not furnish the sources of their responses, making it challenging to verify the reliability of their output. This limitation becomes especially critical in contexts where accuracy and traceability are paramount. Retrieval Augmented Generation (RAG) in AI is a transformative paradigm that promises to revolutionize the capabilities of LLMs.', 'Rapid advancements in LLMs have propelled them to the forefront of AI, yet they still grapple with constraints like information capacity and occasional inaccuracies. RAG bridges these gaps by seamlessly integrating retrieval-based and generative components, endowing LLMs to tap into external knowledge sources. This article explores RAG’s profound impact, unraveling its architecture, benefits, challenges, and the diverse approaches that empower it. In doing so, we unveil the potential of RAG to redefine the landscape of Large Language Models and pave the way for more accurate, context-aware, and reliable AI-driven communication.', 'Learning Objectives', 'Learn about language models and how RAG enhances their capabilities.', 'Discover methods to integrate external data into RAG systems effectively.', 'Explore ethical issues in RAG, including bias and privacy.', 'Gain hands-on experience with RAG using LangChain for real-world applications.', 'This article was published as a part of the\xa0Data Science Blogathon.', 'Table of contentsIntroductionUnderstanding Retrieval Augmented Generation (RAG)The Power of External DataBenefits of Retrieval Augmented Generation (RAG)Diverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsUtilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)OutputConclusionFrequently Asked Questions', 'Understanding Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.', 'Do you want to know more about RAG? Read more here. ', 'What is the Need for RAG?', 'The development of RAG is a direct response to the limitations of Large Language Models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG aims to bridge this gap by offering a solution that excels in understanding user intent and delivering meaningful and context-aware replies.', 'The Fusion of Retrieval-Based and Generative Models', 'RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.', 'Deconstructing RAG’s Mechanics', 'To grasp the essence of RAG, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps. ', 'Begin by receiving and processing user input.', 'Analyze the user input to understand its meaning and intent.', 'Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.', 'Use the retrieved external knowledge to enhance comprehension.', 'Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.', 'Combine all the information gathered to produce responses that are meaningful and human-like.', 'Ensure that the transformation of user queries into responses is done effectively.', 'The Role of Language Models and User Input', 'Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects through its integration of retrieval and generation.', 'Incorporating External Knowledge Sources', 'RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.', 'Generating Contextual Responses', 'Ultimately, the hallmark of RAG is its ability to generate contextual responses. It considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. These context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.', 'Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. By harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.', 'The Power of External Data', 'In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.', 'APIs and Real-time Databases', 'APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They allow models to access the latest data as it becomes available.', 'Document Repositories', 'Document repositories serve as valuable knowledge stores, offering structured and unstructured information. They are fundamental in expanding the knowledge base that RAG models can draw upon.', 'Webpages and Scraping', 'Web scraping is a method for extracting information from web pages. It enables RAG models to access dynamic web content, making it a crucial source for real-time data retrieval.', 'Databases and Structured Information', 'Databases provide structured data that can be queried and extracted. RAG models can use databases to retrieve specific information, enhancing the accuracy of their responses.', 'Benefits of Retrieval Augmented Generation (RAG)', 'Enhanced LLM Memory', 'RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.', 'Improved Contextualization', 'RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.', 'Updatable Memory', 'A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. This keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.', 'Source Citations', 'RAG-equipped models can provide sources for their responses, enhancing transparency and credibility. Users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.', 'Reduced Hallucinations', 'Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.', 'These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing, overcoming the limitations of traditional language models and enhancing the capabilities of AI-powered applications.', 'Diverse Approaches in RAG', 'RAG offers a spectrum of approaches for the retrieval mechanism, catering to various needs and scenarios:', 'Simple: Retrieve relevant documents and seamlessly incorporate them into the generation process, ensuring comprehensive responses.', 'Map Reduce: Combine responses generated individually for each document to craft the final response, synthesizing insights from multiple sources.', 'Map Refine: Iteratively refine responses using initial and subsequent documents, enhancing response quality through continuous improvement.', 'Map Rerank: Rank responses and select the highest-ranked response as the final answer, prioritizing accuracy and relevance.', 'Filtering: Apply advanced models to filter documents, utilizing the refined set as context for generating more focused and contextually relevant responses.', 'Contextual Compression: Extract pertinent snippets from documents, generating concise and informative responses and minimizing information overload.', 'Summary-Based Index: Leverage document summaries, index document snippets, and generate responses using relevant summaries and snippets, ensuring concise yet informative answers.', 'Forward-Looking Active Retrieval Augmented Generation (FLARE): Predict forthcoming sentences by initially retrieving relevant documents and iteratively refining responses. Flare ensures a dynamic and contextually aligned generation process.', 'These diverse approaches empower RAG to adapt to various use cases and retrieval scenarios, allowing for tailored solutions that maximize AI-generated responses’ relevance, accuracy, and efficiency.', 'Ethical Considerations in RAG', 'RAG introduces ethical considerations that demand careful attention:', 'Ensuring Fair and Responsible Use: Ethical deployment of RAG involves using the technology responsibly and refraining from any misuse or harmful applications. Developers and users must adhere to ethical guidelines to maintain the integrity of AI-generated content.', 'Addressing Privacy Concerns: RAG’s reliance on external data sources may involve accessing user data or sensitive information. Establishing robust privacy safeguards to protect individuals’ data and ensure compliance with privacy regulations is imperative.', 'Mitigating Biases in External Data Sources: External data sources can inherit biases in their content or collection methods. Developers must implement mechanisms to identify and rectify biases, ensuring AI-generated responses remain unbiased and fair. This involves constant monitoring and refinement of data sources and training processes.', 'Applications of Retrieval Augmented Generation (RAG)', 'RAG finds versatile applications across various domains, enhancing AI capabilities in different contexts:', 'Chatbots and AI Assistants: RAG-powered systems excel in question-answering scenarios, providing context-aware and detailed answers from extensive knowledge bases. These systems enable more informative and engaging interactions with users.', 'Education Tools: RAG can significantly improve educational tools by offering students access to answers, explanations, and additional context based on textbooks and reference materials. This facilitates more effective learning and comprehension.', 'Legal Research and Document Review: Legal professionals can leverage RAG models to streamline document review processes and conduct efficient legal research. RAG assists in summarizing statutes, case law, and other legal documents, saving time and improving accuracy.', 'Medical Diagnosis and Healthcare: In the healthcare domain, RAG models serve as valuable tools for doctors and medical professionals. They provide access to the latest medical literature and clinical guidelines, aiding in accurate diagnosis and treatment recommendations.', 'Language Translation with Context: RAG enhances language translation tasks by considering the context in knowledge bases. This approach results in more accurate translations, accounting for specific terminology and domain knowledge, particularly valuable in technical or specialized fields.', 'These applications highlight how RAG’s integration of external knowledge sources empowers AI systems to excel in various domains, providing context-aware, accurate, and valuable insights and responses.', 'The Future of RAGs and LLMs', 'The evolution of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) is poised for exciting developments:', 'Advancements in Retrieval Mechanisms: The future of RAG will witness refinements in retrieval mechanisms. These enhancements will focus on improving the precision and efficiency of document retrieval, ensuring that LLMs access the most relevant information quickly. Advanced algorithms and AI techniques will play a pivotal role in this evolution.', 'Integration with Multimodal AI: The synergy between RAG and multimodal AI, which combines text with other data types like images and videos, holds immense promise. Future RAG models will seamlessly incorporate multimodal data to provide richer and more contextually aware responses. This will open doors to innovative applications like content generation, recommendation systems, and virtual assistants.', 'RAG in Industry-Specific Applications: As RAG matures, it will find its way into industry-specific applications. Healthcare, law, finance, and education sectors will harness RAG-powered LLMs for specialized tasks. For example, in healthcare, RAG models will aid in diagnosing medical conditions by instantly retrieving the latest clinical guidelines and research papers, ensuring doctors have access to the most current information.', 'Ongoing Research and Innovation in RAG: The future of RAG is marked by relentless research and innovation. AI researchers will continue to push the boundaries of what RAG can achieve, exploring novel architectures, training methodologies, and applications. This ongoing pursuit of excellence will result in more accurate, efficient, and versatile RAG models.', 'LLMs with Enhanced Retrieval Capabilities: LLMs will evolve to possess enhanced retrieval capabilities as a core feature. They will seamlessly integrate retrieval and generation components, making them more efficient at accessing external knowledge sources. This integration will lead to LLMs that are proficient in understanding context and excel in providing context-aware responses.', 'Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)', 'Installation of LangChain and OpenAI Libraries', 'This line of code installs the LangChain and OpenAI libraries. LangChain is critical for handling text data and embedding, while OpenAI provides access to state-of-the-art Large Language Models (LLMs). This installation step is essential for setting up the required tools for RAG.', '!pip install langchain openai', '!pip install -q -U faiss-cpu tiktoken', 'import os', 'import getpass', 'os.environ[""OPENAI_API_KEY""] = getpass.getpass(""Open AI API Key:"")', 'Web Data Loading for the RAG Knowledge Base', 'The code utilizes LangChain’s “WebBaseLoader.”', 'Three web pages are specified for data retrieval: YOLO-NAS object detection, DeciCoder’s code generation efficiency, and a Deep Learning Daily newsletter.', 'This step is essential for building the knowledge base used in RAG, enabling contextually relevant and accurate information retrieval and integration into language model responses.', 'from langchain.document_loaders import WebBaseLoader', 'yolo_nas_loader = WebBaseLoader(""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"").load()', 'decicoder_loader = WebBaseLoader(""https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder\'s%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency."").load()', 'yolo_newsletter_loader = WebBaseLoader(""https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas"").load()', 'Embedding and Vector Store Setup', 'The code sets up embeddings for the RAG process.', 'It uses “OpenAIEmbeddings” to create an embedding model.', 'A “CacheBackedEmbeddings” object is initialized, allowing embeddings to be stored and retrieved efficiently using a local file store.', 'A “FAISS” vector store is created from the preprocessed chunks of web data (yolo_nas_chunks, decicoder_chunks, and yolo_newsletter_chunks), enabling fast and accurate similarity-based retrieval.', 'Finally, a retriever is instantiated from the vector store, facilitating efficient document retrieval during the RAG process.', 'from langchain.embeddings.openai import OpenAIEmbeddings', 'from langchain.embeddings import CacheBackedEmbeddings', 'from langchain.vectorstores import FAISS', 'from langchain.storage import LocalFileStore', 'store = LocalFileStore(""./cachce/"")', '# create an embedder', 'core_embeddings_model = OpenAIEmbeddings()', 'embedder = CacheBackedEmbeddings.from_bytes_store(', '    core_embeddings_model,', '    store,', '    namespace = core_embeddings_model.model', ')', '# store embeddings in vector store', 'vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)', 'vectorstore.add_documents(decicoder_chunks)', 'vectorstore.add_documents(yolo_newsletter_chunks)', '# instantiate a retriever', 'retriever = vectorstore.as_retriever()', 'Establishing the Retrieval System', 'The code configures the retrieval system for Retrieval Augmented Generation (RAG).', 'It uses “OpenAIChat” from the LangChain library to set up a chat-based Large Language Model (LLM).', 'A callback handler named “StdOutCallbackHandler” is defined to manage interactions with the retrieval system.', 'The “RetrievalQA” chain is created, incorporating the LLM, retriever (previously initialized), and callback handler.', 'This chain is designed to perform retrieval-based question-answering tasks, and it is configured to return source documents for added context during the RAG process.', 'from langchain.llms.openai import OpenAIChat', 'from langchain.chains import RetrievalQA', 'from langchain.callbacks import StdOutCallbackHandler', 'llm = OpenAIChat()', 'handler =  StdOutCallbackHandler()', '# This is the entire retrieval system', 'qa_with_sources_chain = RetrievalQA.from_chain_type(', '    llm=llm,', '    retriever=retriever,', '    callbacks=[handler],', '    return_source_documents=True', ')', 'Initializes the RAG System', 'The code sets up a RetrievalQA chain, a critical part of the RAG system, by combining an OpenAIChat language model (LLM) with a retriever and callback handler.', 'Issue Queries to the RAG System', 'It sends various user queries to the RAG system, prompting it to retrieve contextually relevant information.', 'Retrieves Responses', 'After processing the queries, the RAG system generates and returns contextually rich and accurate responses. The responses are printed on the console.', '# This is the entire augment system!', 'response = qa_with_sources_chain({""query"":""What does Neural Architecture Search have to do with how Deci creates its models?""})', 'response', ""print(response['result'])"", ""print(response['source_documents'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder""})', ""print(response['result'])"", 'This code exemplifies how RAG and LangChain can enhance information retrieval and generation in AI applications.', 'Output', 'Conclusion', 'Retrieval Augmented Generation (RAG) represents a transformative leap in artificial intelligence. It seamlessly integrates Large Language Models (LLMs) with external knowledge sources, addressing the limitations of LLMs’ parametric memory.', 'RAG’s ability to access real-time data, coupled with improved contextualization, enhances the relevance and accuracy of AI-generated responses. Its updatable memory ensures responses are current without extensive model retraining. RAG also offers source citations, bolstering transparency and reducing data leakage. In summary, RAG empowers AI to provide more accurate, context-aware, and reliable information, promising a brighter future for AI applications across industries.', 'Key Takeaways', 'Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.', 'RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.', 'With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.', 'RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.', 'This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.', 'Frequently Asked Questions', 'Q1. What is RAG? How does it differ from traditional AI models? A. RAG, or Retrieval Augmented Generation, is an innovative AI framework combining retrieval-based and generative models’ strengths. Unlike traditional AI models, which generate responses solely based on their pre-trained knowledge, RAG integrates external knowledge sources, allowing it to provide more accurate, up-to-date, and contextually relevant responses.  Q2. How does RAG ensure the accuracy of the retrieved information? A. RAG employs a retrieval system that fetches information from external sources. It ensures accuracy through techniques like vector similarity search and real-time updates to external datasets. Additionally, RAG allows users to access source citations, enhancing transparency and credibility.  Q3. Can RAG be used in specific industries or applications? A. Yes, RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.  Q4. Does implementing RAG require extensive technical expertise? A. While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.  Q5. What are the potential ethical concerns with RAG, such as misinformation or data privacy? A. RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns.  ', 'The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\xa0', 'Related', 'AIApplicationsblogathonData Sourcesdocumentslanguage modelsmemoryModels ', 'Recommended For You', 'Become a full stack data scientist', 'Large Language Models Demystified: A Beginner’s Roadmap', 'Training Your Own LLM Without Coding', 'How to Build LLMs for Code?\xa0', 'LLMs in Conversational AI: Building Smarter Chatbots & Assistants', 'From GPT-3 to Future Generations of Language Models', 'What are Large Language Models (LLMs)?', 'About the Author', 'Soumyadarshan Dash', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'How to Explore Text Generation with GPT-2? ', 'Next Post', 'How does Generative AI in Recipe Generation and Culinary Arts Work? ', 'Top Resources', '10 Best AI Image Generator Tools to Use in 2023', 'avcontentteam - ', 'Aug 17, 2023', 'Everything you need to Know about Linear Regression!', 'KAVITA MALI - ', 'Oct 04, 2021', 'Skewness and Kurtosis: Quick Guide (Updated 2023)', 'Suvarna Gawali - ', 'May 02, 2021', 'Guide on Support Vector Machine (SVM) Algorithm', 'Anshul Saini - ', 'Oct 12, 2021', '×', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Join the Community', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2023 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'Loading...', 'To continue reading please login', 'google-icon', 'Continue with Google', 'linkedin', 'Continue with Linkedin', 'email', 'Continue with Email', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Welcome Back :)', 'email 2', 'key-', 'Forgot Password?', 'Log In', ""Don't have an account yet?Register here"", 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Start your journey here!', 'user', 'user', 'email 2', 'key-', 'Sign up', 'Already have an accountLogin here', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', ' A verification link has been sent to your email id ', ' If you have not recieved the link please goto', 'Sign Up  page again', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your registered email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter your registered email id', 'email 2', 'Next', 'This email id is not registered with us. Please enter your registered email id.', ""Don't have an account yet?Register here"", 'Loading...', '×', 'back', 'Please enter the OTP that is sent your registered email id', 'email 2', 'Next', 'Loading...', '×', 'Please create the new password here', 'key-', 'key-', 'Submit', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview ', 'This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.', 'Necessary ', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. ', 'Non-necessary ', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. ', 'SAVE & ACCEPT', '×']","**Summarize the article in 10 bullet points**
- Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.
- RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.
- With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.
- RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.
- This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.
- RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.
- While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.
- RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns."
16,16,Retrieval Augmented Generation (RAG): Reducing ... - Pinecone,,"https://www.pinecone.io/learn/retrieval-augmented-generation/#:~:text=Retrieval%20Augmented%20Generation%20means%20fetching,a%20response%2C%20solving%20this%20problem.","['Retrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI Applications | PineconeProductSolutionsPricingResourcesCompanyLog InSign Up FreeLearn | ArticleRetrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI ApplicationsJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Products built on top of Large Language Models (LLMs) such as OpenAI\'s ChatGPT and Anthropic\'s Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.LLMs are “stuck” at a particular time, but RAG can bring them into the present.ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”The LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.The reason LLMs like ChatGPT feel so bright is that they\'ve seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.RAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.Why is RAG the preferred approach from a cost-efficacy perspective?There are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.Fine-tuning: adapting a foundation model to your domain’s data.Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.Prompt engineering is insufficient for reducing hallucinations.Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.It’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.Let’s now take a deeper dive into how Retrieval Augmented Generation works.We’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window"".An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.Creating a vector database from your domain-specific proprietary data using an embedding model.This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.You originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.You send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM\'s context window.Since the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.Vector databases can support even more advanced search functionality.Semantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query\'s semantics and keywords.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.Pinecone enables you to integrate RAG within minutes. Check out our examples repository on GitHub for runnable examples, such as this RAG Jupyter Notebook.Share via:  Zachary ProserStaff Developer AdvocateJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.PRODUCTOverviewDocumentationTrust and SecuritySOLUTIONSSearchGenerative AICustomersRESOURCESLearning CenterCommunityPinecone BlogSupport CenterSystem StatusCOMPANYAboutPartnersCareersNewsroomContactLEGALTermsPrivacyCookies© Pinecone Systems, Inc. | San Francisco, CAPinecone is a registered trademark of Pinecone Systems, Inc.']","**Retrieval Augmented Generation (RAG) is a technique that helps Large Language Models (LLMs) produce more accurate and relevant responses by providing them with additional context from a database of proprietary data.**

RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.

**Here are the key benefits of using RAG:**

* **RAG reduces the likelihood of hallucinations.** LLMs are often referred to as ""black boxes"" because it can be difficult to understand which sources they were considering when they arrived at their conclusions. RAG provides additional context and factual information to LLMs, which helps them produce more informed answers.
* **RAG allows GenAI applications to cite their sources.** This improves auditability and makes it easier for end users to understand how GenAI applications arrived at their conclusions.
* **RAG is the most cost-effective way to improve the performance of GenAI applications.** It is significantly cheaper than building or fine-tuning a foundation model, and it does not require specialized expertise or rare talent.

**How does RAG work?**

RAG works by passing additional relevant content from a database of proprietary data to an LLM at generation time. This additional content is called the ""context window."" The context window provides the LLM with the information it needs to produce a more accurate and relevant response.

**To create a context window, you first need to convert your proprietary data into vectors.** A vector is a numerical representation of a piece of data. You can then use a vector database to search for similar vectors. The results of this search are the items that will be included in the context window.

**When the user issues a query or question, you translate their natural language search terms into embeddings.** You then send these embeddings to the vector database. The database performs a ""nearest neighbor"" search, finding the vectors that most closely resemble the user's intent.

**The vector database returns the relevant results, which are then provided to the LLM via its context window.** The LLM uses this information to produce a more accurate and relevant response.

**RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.** It is a powerful technique that can help you deliver superior results to your end users."
17,17,What is retrieval-augmented generation?,"Aug 22, 2023 — ",https://research.ibm.com/blog/retrieval-augmented-generation-RAG,"[""What is retrieval-augmented generation? | IBM Research BlogSkip to main contentResearchFocus areasBlogPublicationsCareersAboutBackFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBackAboutOverviewLabsPeopleCollaborateBackArtificial IntelligenceBackHybrid CloudBackQuantum ComputingBackScienceBackSecurityBackSemiconductorsBackOverviewBackLabsBackPeopleBackCollaborateResearchFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBlogPublicationsCareersAboutOverviewLabsPeopleCollaborateOpen IBM search fieldClose22 Aug 2023Explainer4 minute readWhat is retrieval-augmented generation?RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.Large language models can be inconsistent. Sometimes they nail the answer to questions, other times they regurgitate random facts from their training data. If they occasionally sound like they have no idea what they’re saying, it’s because they don’t. LLMs know how words relate statistically, but not what they mean.Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.“You want to cross-reference a model’s answers with the original content so you can see what it is basing its answer on,” said Luis Lastras, director of language technologies at IBM Research.RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. IBM unveiled its new AI and data platform, watsonx, which offers RAG, back in May.An ‘open book’ approach to answering tough questionsUnderpinning all foundation models, including LLMs, is an AI architecture known as the transformer. It turns heaps of raw data into a compressed representation of its basic structure. Starting from this raw representation, a foundation model can be adapted to a variety of tasks with some additional fine-tuning on labeled, domain-specific knowledge.But fine-tuning alone rarely gives the model the full breadth of knowledge it needs to answer highly specific questions in an ever-changing context. In a 2020 paper, Meta (then known as Facebook) came up with a framework called retrieval-augmented generation to give LLMs access to information beyond their training data. RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”As the name suggests, RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.Toward personalized and verifiable responsesBefore LLMs, digital conversation agents followed a manual dialogue flow. They confirmed the customer’s intent, fetched the requested information, and delivered an answer in a one-size-fits all script. For straightforward queries, this manual decision-tree method worked just fine.But it had limitations. Anticipating and scripting answers to every question a customer might conceivably ask took time; if you missed a scenario, the chatbot had no ability to improvise. Updating the scripts as policies and circumstances evolved was either impractical or impossible.Today, LLM-powered chatbots can give customers more personalized answers without humans having to write out new scripts. And RAG allows LLMs to go one step further by greatly reducing the need to feed and retrain the model on fresh examples. Simply upload the latest documents or policies, and the model retrieves the information in open-book mode to answer the question.IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources.Teaching the model to recognize when it doesn’t knowCustomer queries aren’t always this straightforward. They can be ambiguously worded, complex, or require knowledge the model either doesn’t have or can’t easily parse. These are the conditions in which LLMs are prone to making things up.“Think of the model as an overeager junior employee that blurts out an answer before checking the facts,” said Lastras. “Experience teaches us to stop and say when we don’t know something. But LLMs need to be explicitly trained to recognize questions they can’t answer.”In a more challenging scenario taken from real life, Alice wants to know how many days of maternity leave she gets. A chatbot that does not use RAG responds cheerfully (and incorrectly): “Take as long as you want.”Maternity-leave policies are complex, in part, because they vary by the state or country of the employee’s home-office. When the LLM failed to find a precise answer, it should have responded, “I’m sorry, I don’t know,” said Lastras, or asked additional questions until it could land on a question it could definitively answer. Instead, it pulled a phrase from a training set stocked with empathetic, customer-pleasing language.With enough fine-tuning, an LLM can be trained to pause and say when it’s stuck. But it may need to see thousands of examples of questions that can and can’t be answered. Only then can the model learn to identify an unanswerable question, and probe for more detail until it hits on a question that it has the information to answer.RAG is currently the best-known tool for grounding LLMs on the latest, verifiable information, and lowering the costs of having to constantly retrain and update them. But RAG is imperfect, and many interesting challenges remain in getting RAG done right.At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.Subscribe to our Future Forward newsletter and stay up to date on the latest research newsSubscribe to our newsletterHome↳ BlogDate22 Aug 2023AuthorsKim MartineauTopicsAIExplainable AIGenerative AINatural Language ProcessingTrustworthy GenerationShareWhat is AI inferencing?ExplainerKim Martineau05 Oct 2023AIFoundation ModelsGenerative AIHybrid CloudCelebrating a decade of IBM Research innovation in AfricaNewsMike Murphy28 Sep 2023AIQuantumFind and fix IT glitches before they crash the system NewsKim Martineau28 Sep 2023AI for CodeAI for ITExplainable AIFoundation ModelsGenerative AIHow open source paved the way for computing from anywhere NewsKim Martineau20 Sep 2023AIHybrid Cloud PlatformMachine LearningScaling AIPreviousError correcting codes for near-term quantum computersNextIBM Research’s newest prototype chips use drastically less power to solve AI tasksIBM LogoFocus areasFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usTwitterLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility""]","Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.

RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.

This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.

RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way. It reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting."
18,18,Retrieval Augmented Generation (RAG),RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient ...,https://www.promptingguide.ai/techniques/rag,"['Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuidePrompt Engineering CoursePrompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingSelf-ConsistencyGenerate Knowledge PromptingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingReActMultimodal CoTGraph PromptingApplicationsProgram-Aided Language ModelsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionModelsFlanChatGPTLLaMAGPT-4LLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesPapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightQuestion? Give us feedback → (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)', ""General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge."", 'For more complex and knowledge-intensive tasks, it\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of ""hallucination"".', 'Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.', ""RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."", 'Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:', 'Image Source: Lewis et el. (2021) (opens in a new tab)', 'RAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.', 'This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.', 'More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.', 'You can find a simple example of how to use retrievers and LLMs for question answering with sources (opens in a new tab) from the LangChain documentation.Tree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright © 2023 DAIR.AI']","Retrieval Augmented Generation (RAG) is a method proposed by Meta AI researchers to address knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.

RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.

RAG performs strong on several benchmarks such as Natural Questions, WebQuestions, and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.

This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks."
19,19,What Is Retrieval-Augmented Generation (RAG)?,"Sep 19, 2023 — ",https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/,"['What Is Retrieval-Augmented Generation (RAG)?', 'Accessibility Policy', 'Skip to content', 'About', 'Services', 'Solutions', 'Pricing', 'Partners', 'Resources', 'Close Search', 'Close', 'We’re sorry.  We could not find a match for your search.', ""We suggest you try the following to help find what you're looking for:"", 'Check the spelling of your keyword search.', 'Use synonyms for the keyword you typed, for example, try “application” instead of “software.”', 'Start a new search.', 'Clear Search', 'Search', 'Menu', 'Menu', 'Contact Sales', 'Sign in to Oracle Cloud', 'Cloud', 'Artificial Intelligence', 'Generative AI', 'What Is Retrieval-Augmented Generation (RAG)?', 'Alan Zeichick | Tech Content Strategist | September 19, 2023', 'In This Article', 'What Is Retrieval-Augmented Generation (RAG)?', 'Retrieval-Augmented Generation Explained', 'How Does Retrieval-Augmented Generation Work?', 'Using RAG in Chat Applications', 'Benefits of Retrieval-Augmented Generation', 'Challenges of Retrieval-Augmented Generation', 'Examples of Retrieval-Augmented Generation', 'Future of Retrieval-Augmented Generation', 'Generative AI With Oracle', 'Retrieval-Augmented Generation FAQs', 'Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.', 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees.', 'What Is Retrieval-Augmented Generation (RAG)?', 'That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.', 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.', 'Retrieval-Augmented Generation Explained', 'Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.', 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. ', 'Simply put, RAG helps LLMs give better answers.', 'Key Takeaways', 'RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.', 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.', 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.', 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.', 'How Does Retrieval-Augmented Generation Work? ', 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.', ' The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.', 'RAG and Large Language Models (LLMs)', 'Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.', 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.', 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.', 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.', 'Retrieval-Augmented Generation vs. Semantic Search', ' RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.', ' Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.', ' Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG.', 'Using RAG in Chat Applications', 'When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.', ' The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.', 'Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.', 'Benefits of Retrieval-Augmented Generation', 'RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following:', 'The RAG has access to information that may be fresher than the data used to train the LLM.', 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.', 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.', 'The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted.', 'Challenges of Retrieval-Augmented Generation', 'Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are', 'Improving organizational knowledge and understanding of RAG because it’s so new', 'Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself', 'Determining how to best model the structured and unstructured data within the knowledge library and vector database', 'Developing requirements for a process to incrementally feed data into the RAG system', 'Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system', 'Examples of Retrieval-Augmented Generation', 'There are many possible examples of generative AI augmented by RAG.', 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.', 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.', 'Future of Retrieval-Augmented Generation', 'Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.', 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.', 'RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.', 'Generative AI With Oracle', 'Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.', 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.', 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot.', 'What makes Oracle best suited for generative AI?', 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.', 'Learn more about Oracle’s generative AI strategy', 'Retrieval-Augmented Generation FAQs', 'Is RAG the same as generative AI?', 'No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.', 'What type of information is used in RAG?', 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.', 'How does generative AI use RAG?', 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.', 'Can a RAG cite references for the data it retrieves?', 'Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.', 'Resources for', 'Careers', 'Developers', 'Investors', 'Partners', 'Startups', 'Students and Educators', 'Why Oracle', 'Analyst Reports', 'Cloud Economics', 'with Microsoft Azure', 'vs. AWS', 'vs. Google Cloud', 'vs. MongoDB', 'Learn', 'What is AI?', 'What is Cloud Computing?', 'What is Cloud Storage?', 'What is HPC?', 'What is IaaS?', 'What is PaaS?', 'What’s new', 'Oracle Supports Ukraine', 'Oracle Cloud Free Tier', 'Cloud Architecture Center', 'Cloud Lift', 'Oracle Support Rewards', 'Oracle Red Bull Racing', 'Contact us', 'US Sales: +1.800.633.0738', 'How can we help?', 'Subscribe to emails', 'Events', 'News', 'OCI Blog', ' Country/Region ', '© 2023 Oracle', 'Privacy/Do Not Sell My Info', 'Ad Choices', 'Careers', 'Facebook', 'Twitter', 'LinkedIn', 'YouTube']","Retrieval-augmented generation (RAG) is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.

RAG models build knowledge repositories based on the organization's own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.

Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.

Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM."
20,20,What is Retrieval Augmented Generation (RAG)?,"RAG puts together a pre-trained system that finds relevant information ( retriever ) with another system that generates text ( generator ). Then, when the user ...",https://vercel.com/guides/retrieval-augmented-generation,"['What is Retrieval Augmented Generation (RAG)?Skip to contentDocumentationGuidesHelp← Back to GuidesWhat is Retrieval Augmented Generation (RAG)?Large-language modals (LLMs) like OpenAI\'s GPT-4 and Anthropic\'s Claude are incredible at generating coherent and contextually relevant text based on given prompts. They can assist in a wide range of tasks, such as writing, translation, and even conversation.Despite this, LLMs have limitations. In this guide, we\'ll go over these constraints and explain how Retrieval Augmented Generation (RAG) can alleviate these pains. We\'ll also dive into the ways you can build better chat experiences with this technique.The problem with LLMsAs groundbreaking as LLMs may be, they have a few limitations:They\'re limited by the amount of training data they have access to. For example, GPT-4 has a training data cutoff date, which means that it doesn\'t have access to information beyond that date. This limitation affects the model\'s ability to generate up-to-date and accurate responses.They\'re generic and lack subject-matter expertise. LLMs are trained on a large dataset that covers a wide range of topics, but they don\'t possess specialized knowledge in any particular field. This leads to hallucinations or inaccurate information when asked about specific subject areas.Citations are tricky. LLMs don\'t have a reliable way of returning the exact location of the text where they retrieved the information. This exacerbates the issue of hallucination, as they may not be able to provide proper attribution or verify the accuracy of their responses. Additionally, the lack of specific citations makes it difficult for users to fact-check or delve deeper into the information provided by the models.Retrieval Augmented GenerationTo solve this problem, researchers at Meta published a paper about a technique called Retrieval Augmented Generation (RAG), which adds an information retrieval component to the text generation model that LLMs are already good at. This allows for fine-tuning and adjustments to the LLM\'s internal knowledge, making it more accurate and up-to-date.Source: Lewis et el. (2021)\xa0–\xa0Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksHere\'s how RAG works on a high level:RAG puts together a pre-trained system that finds relevant information (retriever) with another system that generates text (generator).Then, when the user inputs a question (query), the retriever use a technique called ""Maximum Inner Product Search (MIPS)"" to find the most relevant documents. The information from these documents will then be fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.Using RAG in Chat ApplicationsTo illustrate how you can apply RAG in a real-world application, here\'s a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel\'s AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:Setting up a Next.js applicationCreating a chatbot frontend componentBuilding an API endpoint using OpenAI\'s API for response generationProgressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments. By following the tutorial, you\'ll build a context-aware chatbot with improved user experience.Build better AI chat experiences with RAGBy integrating Retrieval Augmented Generation into chat applications like the Pinecone chatbot template above, developers can reduce hallucinations in their AI models and create more accurate and evidence-based conversational experiences.If you\'re interested in learning more about RAG, check out this article about integrating RAG with Langchain and a Supabase vector database.Couldn\'t find the guide you need?View Help© 2023ProductPreviewsNext.jsInfrastructurev0Edge FunctionsTurboAnalyticsEnterpriseChangelogCLI & APIResourcesDocsExpertsPricingGuidesCustomersHelpIntegrations⌘KTemplatesCompanyAboutBlogCareersContact UsNext.js ConfOpen SourcePartnersSecurityPrivacy PolicyLegal']","Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generative models to deliver accurate responses.

RAG works by first finding the most relevant documents to a user's query. The information from these documents is then fed into a generative model to create the final response. This allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.

RAG can be used to build better AI chat experiences by reducing hallucinations in AI models and creating more accurate and evidence-based conversational experiences."
21,21,Retrieval Augmented Generation (RAG),You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data ...,https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html,"['Retrieval Augmented Generation (RAG) - Amazon SageMakerRetrieval Augmented Generation (RAG) - Amazon SageMakerAWSDocumentationAmazon SageMakerDeveloper GuideRetrieval Augmented Generation (RAG)Foundation models are usually trained offline, making the model agnostic to any', '                data that is created after the model was trained. Additionally, foundation models', '                are trained on very general domain corpora, making them less effective for', '                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve', '                data from outside a foundation model and augment your prompts by adding the relevant', '                retrieved data in context. For more information about RAG model architectures, see', '                    Retrieval-Augmented Generation for', '                    Knowledge-Intensive NLP Tasks.With RAG, the external data used to augment your prompts can come from multiple', '                data sources, such as a document repositories, databases, or APIs. The first step is', '                to convert your documents and any user queries into a compatible format to perform', '                relevancy search. To make the formats compatible, a document collection, or', '                knowledge library, and user-submitted queries are converted to numerical', '                representations using embedding language models. Embedding is the process by which text is given numerical', '                representation in a vector space. RAG model architectures compare the embeddings of', '                user queries within the vector of the knowledge library. The original user prompt is', '                then appended with relevant context from similar documents within the knowledge', '                library. This augmented prompt is then sent to the foundation model. You can update', '                knowledge libraries and their relevant embeddings asynchronously.', 'For more information, see the following example notebooks: ', 'Retrieval-Augmented Generation: Question Answering based on Custom', '                            Dataset', 'Retrieval-Augmented Generation: Question Answering based on Custom', ""                            Dataset with Open-sourced LangChain Library Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsFine-tuningTask-Specific ModelsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.""]","Retrieval Augmented Generation (RAG) is a technique that can be used to improve the performance of foundation models on domain-specific tasks.

RAG works by retrieving relevant data from outside the foundation model and augmenting the user's prompt with this data. This augmented prompt is then sent to the foundation model, which can then generate more relevant results.

RAG can be used with a variety of data sources, such as document repositories, databases, or APIs. The first step is to convert the data into a compatible format. This can be done using embedding language models, which convert text into numerical representations.

Once the data is in a compatible format, it can be used to augment the user's prompt. The RAG model architectures compare the embeddings of the user queries within the vector of the knowledge library. The original user prompt is then appended with relevant context from similar documents within the knowledge library. This augmented prompt is then sent to the foundation model.

You can update knowledge libraries and their relevant embeddings asynchronously.

For more information, see the following example notebooks:

* Retrieval-Augmented Generation: Question Answering based on Custom Dataset
* Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library"
22,22,What Is Retrieval-Augmented Generation? | Definition from ...,Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the ...,https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation,"['What Is Retrieval-Augmented Generation? | Definition from TechTarget', 'Enterprise AI', 'Search the TechTarget Network', 'Login', 'Register', 'Explore the Network', 'TechTarget Network', 'Business Analytics', 'CIO', 'Data Management', 'ERP', 'Enterprise AI', 'AI Business Strategies', 'AI Careers', 'AI Infrastructure', 'AI Platforms ', 'AI Technologies', 'More Topics', 'Applications of AI', 'ML Platforms', 'Other Content', 'News', 'Features', 'Tips', 'Webinars', '2023 IT Salary Survey Results', '                                More', 'Answers', 'Conference Guides', 'Definitions', 'Opinions', 'Podcasts', 'Quizzes', 'Tech Accelerators', 'Tutorials', 'Videos', 'Sponsored Communities', 'Follow:', 'Home', 'AI technologies', 'Tech Accelerator', 'What is generative AI? Everything you need to know', 'Prev', 'Next', 'Generative AI in the enterprise raises questions for CIOs', '15 of the best large language models', 'Download this guide1', 'X', 'Free Download', 'What is generative AI? Everything you need to know', 'The potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight.', 'This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.', 'Corporate Email Address:You forgot to provide an Email Address.This email address doesn’t appear to be valid.This email address is already registered. Please log in.You have exceeded the maximum character limit.Please provide a Corporate Email Address.I agree to TechTarget’s Terms of Use, Privacy Policy, and the transfer of my information to the United States for processing to provide me with relevant information as described in our Privacy Policy.Please check the box if you want to proceed.I agree to my information being processed by TechTarget and its Partners to contact me via phone, email, or other means regarding information relevant to my professional interests. I may unsubscribe at any time.Please check the box if you want to proceed.', 'By submitting my Email address I confirm that I have read and accepted the Terms of Use and Declaration of Consent.', 'Definition', 'retrieval-augmented generation ', 'Share this item with your network:', 'By', 'Alexander S. Gillis,', 'Technical Writer and Editor', 'What is retrieval-augmented generation?', 'Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.', ""LLMs are AI models that power chatbots such as OpenAI's ChatGPT and Google Bard. LLMs can understand, summarize, generate and predict new content. However, they can still be inconsistent and fail at some knowledge-intensive tasks -- especially tasks that are outside their initial training data or those that require up-to-date information and transparency about how they make their decisions. When this happens, the LLM can return false information, also known as an AI hallucination."", ""By retrieving information from external sources when the LLM's trained data isn't enough, the quality of LLM responses improves. Retrieving information from an online source, for example, enables the LLM to access current information that it wasn't initially trained on."", 'What does RAG do?', ""LLMs are commonly trained offline, making the model uncertain of any data that's created after the model was trained. RAG is used to retrieve data from outside the LLM, which then augments the user's prompts by adding relevant retrieved data in its response."", 'This article is part of', 'What is generative AI? Everything you need to know', 'Which also includes:', '15 of the best large language models', 'Will AI replace jobs? 9 job types that might be affected', 'Pros and cons of AI-generated content', 'This process helps reduce any apparent knowledge gaps and AI hallucinations. This can be important in fields that require as much up-to-date and accurate information as possible, such as healthcare.', 'How to use RAG with LLMs', 'RAG combines information retrieval with a text generator model. External knowledge can be retrieved from data sources, online sources, application programming interfaces, databases or document repositories. ', 'Using the example of a chatbot, once a user inputs a prompt, RAG summarizes that prompt using keywords or semantic data. The converted data is then sent to a search platform to retrieve the requested data, which is then sorted through based on relevancy.', 'The LLM then synthesizes the retrieved data with the augmented prompt and its internal training data to create a generated response that can be passed to the chatbot with sourced links for the user.', 'An LLM using RAG can pull from both internal and external data to return a response for users, ensuring it provides relevant information.', 'What are the benefits of RAG?', 'Benefits of a RAG model include the following:', 'Provides current information. RAG pulls information from relevant, reliable and up-to-date sources.', ""Increases user trust. Users can access the model's sources, which promotes transparency and trust in the content and lets users verify its accuracy."", 'Reduces AI hallucinations. Because LLMs are grounded to external data, the model has less of a chance to make up or return incorrect information.', ""Reduces computational and financial costs. Organizations don't have to spend time and resources to continuously train the model on new data."", 'Synthesizes information. RAG synthesizes data by combining relevant information from retrieval and generative models to produce a response.', 'Easier to train. Because RAG uses retrieved knowledge sources, the need to train the LLM on a massive amount of training data is reduced.', 'Can be used for multiple tasks. Aside from chatbots, RAG can be fine-tuned for a variety of specific use cases, such as text summarization and dialogue systems.', 'Learn more about generative AI models, such as VAEs, GANs, diffusion, transformers and NeRFs.', '\t\t\t\t\tThis was last updated in October 2023', '\t\t\tContinue Reading About retrieval-augmented generation', '7 generative AI challenges that businesses should consider', 'Generative AI ethics: 8 biggest concerns', 'Assessing different types of generative AI applications', 'Pros and cons of AI-generated content', 'Cohesity Turing aims AI tools at backup and ransomware', '\t\t\t\tRelated Terms', 'language modeling', 'Language modeling, or LM, is the use of various statistical and probabilistic techniques to determine the probability of a given ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'OpenAI', 'OpenAI is a private research laboratory that aims to develop and direct artificial intelligence (AI) in ways that benefit ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Salesforce Einstein', 'Salesforce Einstein refers to an integrated set of artificial intelligence (AI) technologies developed for the Salesforce ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Dig Deeper on AI technologies', 'Databricks improves support for generative AI models', 'By: Eric\xa0Avidon', '15 of the best large language models', 'By: Ben\xa0Lutkevich', 'LangChain', 'By: Cameron\xa0Hashemi-Pour', 'large language models (LLMs)', 'By: Sean\xa0Kerner', 'Sponsored News', '4 Things You Need to Know Now About Edge Computing', '–HPE', 'Supporting a more diverse management team', '–AWS', 'See More', 'Vendor Resources', ""What generative AI's rise means for the cybersecurity industry"", '–TechTarget ComputerWeekly.com', 'Fast Track Generative AI with Dell™ Poweredge™ XE9680', '–Dell Technologies & Intel®', 'Latest TechTarget resources', '\t\t\t\t\t\t\tBusiness Analytics', '\t\t\t\t\t\t\tCIO', '\t\t\t\t\t\t\tData Management', '\t\t\t\t\t\t\tERP', 'Business Analytics', 'MicroStrategy launches new suite of generative AI tools', ""The longtime analytics vendor's suite incorporates LLM technology to make users more efficient by enabling them to use natural ..."", 'Sisense unveils composable toolkit for app development', 'Compose SDK for Fusion is a composable set of APIs that enable developers to build customized advanced analytics applications to ...', 'SAS unveils plans to add generative AI to analytics suite', 'After holding off on integrating with LLMs until it could ensure data security and accurate outcomes, the vendor is making ...', 'CIO', '7 challenges with blockchain adoption and how to avoid them', 'Organizations tend to face the same hurdles when they try to implement blockchain. Knowing what they are could be the first big ...', 'U.S. antitrust law enforcers defend actions, lawsuits', 'The FTC and DOJ, which enforce U.S. antitrust law, are focused on reining in big tech through antitrust lawsuits and revising ...', 'Is quantum computing overhyped?', ""Quantum computing may be coming to the enterprise. Here's what to understand about the benefits it promises, the risks it poses ..."", 'Data Management', 'Amazon launches DataZone, a new data management service', ""The tech giant's new service provides data governance, collaboration and catalog capabilities that enable organizations to find ..."", 'Databricks improves support for generative AI models', 'A new service enables users to easily deploy privately built language models and uses a GPU-based architecture to optimize and ...', 'New Boomi AI tool enables natural language data integration', ""The iPaaS vendor's new capabilities are aimed at increasing efficiency by enabling customers to build pipelines and manage data ..."", 'ERP', 'Infor Enterprise Automation looks to ease RPA for ERP', 'Infor unveiled Enterprise Automation, which is designed to help customers bring RPA to Infor cloud ERP applications and Developer...', 'When integrating generative AI and ERP, focus on use cases', 'ERP vendors are rapidly introducing new AI functionality, but experts caution against the hype and advise using the ...', 'PLM and PDM software: Learn the differences', 'PLM and PDM software may seem similar, but they fulfill different needs for manufacturers. Learn the differences and which is ...', 'About Us', 'Editorial Ethics Policy', 'Meet The Editors', 'Contact Us', 'Advertisers', 'Partner with Us', 'Media Kit', 'Corporate Site', 'Contributors', 'Reprints', 'Answers', 'Definitions', 'E-Products', 'Events', 'Features', 'Guides', 'Opinions', 'Photo Stories', 'Quizzes', 'Tips', 'Tutorials', 'Videos', 'All Rights Reserved, ', 'Copyright 2018 - 2023, TechTarget', 'Privacy Policy', 'Cookie Preferences ', 'Cookie Preferences ', 'Do Not Sell or Share My Personal Information', 'Close']",
23,23,Retrieval-Augmented Generation (RAG) in AI,"Sep 29, 2023 — ",https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/,"['Retrieval-Augmented Generation (RAG) in AI', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'Home', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'What is Retrieval-Augmented Generation (RAG) in AI?', 'Facebook', 'Twitter', 'Linkedin', 'Soumyadarshan Dash —', 'Updated On September 29th, 2023 ', 'Advanced', 'Artificial Intelligence', 'Excel', 'Generative AI', 'Healthcare', 'LLMs', 'NLP', 'Introduction', 'The rapid advancements in Large Language Models (LLMs) have transformed the landscape of AI, offering unparalleled capabilities in natural language understanding and generation. LLMs have ushered in a new language understanding and generation era, with OpenAI’s GPT models at the forefront. These remarkable models honed on extensive online data, have broadened our horizons, enabling us to interact with AI-powered systems like never before. However, like any technological marvel, they come with their own set of limitations. One glaring issue is their occasional tendency to provide information that is either inaccurate or outdated. Moreover, these LLMs do not furnish the sources of their responses, making it challenging to verify the reliability of their output. This limitation becomes especially critical in contexts where accuracy and traceability are paramount. Retrieval Augmented Generation (RAG) in AI is a transformative paradigm that promises to revolutionize the capabilities of LLMs.', 'Rapid advancements in LLMs have propelled them to the forefront of AI, yet they still grapple with constraints like information capacity and occasional inaccuracies. RAG bridges these gaps by seamlessly integrating retrieval-based and generative components, endowing LLMs to tap into external knowledge sources. This article explores RAG’s profound impact, unraveling its architecture, benefits, challenges, and the diverse approaches that empower it. In doing so, we unveil the potential of RAG to redefine the landscape of Large Language Models and pave the way for more accurate, context-aware, and reliable AI-driven communication.', 'Learning Objectives', 'Learn about language models and how RAG enhances their capabilities.', 'Discover methods to integrate external data into RAG systems effectively.', 'Explore ethical issues in RAG, including bias and privacy.', 'Gain hands-on experience with RAG using LangChain for real-world applications.', 'This article was published as a part of the\xa0Data Science Blogathon.', 'Table of contentsIntroductionUnderstanding Retrieval Augmented Generation (RAG)The Power of External DataBenefits of Retrieval Augmented Generation (RAG)Diverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsUtilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)OutputConclusionFrequently Asked Questions', 'Understanding Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.', 'Do you want to know more about RAG? Read more here. ', 'What is the Need for RAG?', 'The development of RAG is a direct response to the limitations of Large Language Models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG aims to bridge this gap by offering a solution that excels in understanding user intent and delivering meaningful and context-aware replies.', 'The Fusion of Retrieval-Based and Generative Models', 'RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.', 'Deconstructing RAG’s Mechanics', 'To grasp the essence of RAG, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps. ', 'Begin by receiving and processing user input.', 'Analyze the user input to understand its meaning and intent.', 'Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.', 'Use the retrieved external knowledge to enhance comprehension.', 'Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.', 'Combine all the information gathered to produce responses that are meaningful and human-like.', 'Ensure that the transformation of user queries into responses is done effectively.', 'The Role of Language Models and User Input', 'Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects through its integration of retrieval and generation.', 'Incorporating External Knowledge Sources', 'RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.', 'Generating Contextual Responses', 'Ultimately, the hallmark of RAG is its ability to generate contextual responses. It considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. These context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.', 'Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. By harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.', 'The Power of External Data', 'In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.', 'APIs and Real-time Databases', 'APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They allow models to access the latest data as it becomes available.', 'Document Repositories', 'Document repositories serve as valuable knowledge stores, offering structured and unstructured information. They are fundamental in expanding the knowledge base that RAG models can draw upon.', 'Webpages and Scraping', 'Web scraping is a method for extracting information from web pages. It enables RAG models to access dynamic web content, making it a crucial source for real-time data retrieval.', 'Databases and Structured Information', 'Databases provide structured data that can be queried and extracted. RAG models can use databases to retrieve specific information, enhancing the accuracy of their responses.', 'Benefits of Retrieval Augmented Generation (RAG)', 'Enhanced LLM Memory', 'RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.', 'Improved Contextualization', 'RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.', 'Updatable Memory', 'A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. This keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.', 'Source Citations', 'RAG-equipped models can provide sources for their responses, enhancing transparency and credibility. Users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.', 'Reduced Hallucinations', 'Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.', 'These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing, overcoming the limitations of traditional language models and enhancing the capabilities of AI-powered applications.', 'Diverse Approaches in RAG', 'RAG offers a spectrum of approaches for the retrieval mechanism, catering to various needs and scenarios:', 'Simple: Retrieve relevant documents and seamlessly incorporate them into the generation process, ensuring comprehensive responses.', 'Map Reduce: Combine responses generated individually for each document to craft the final response, synthesizing insights from multiple sources.', 'Map Refine: Iteratively refine responses using initial and subsequent documents, enhancing response quality through continuous improvement.', 'Map Rerank: Rank responses and select the highest-ranked response as the final answer, prioritizing accuracy and relevance.', 'Filtering: Apply advanced models to filter documents, utilizing the refined set as context for generating more focused and contextually relevant responses.', 'Contextual Compression: Extract pertinent snippets from documents, generating concise and informative responses and minimizing information overload.', 'Summary-Based Index: Leverage document summaries, index document snippets, and generate responses using relevant summaries and snippets, ensuring concise yet informative answers.', 'Forward-Looking Active Retrieval Augmented Generation (FLARE): Predict forthcoming sentences by initially retrieving relevant documents and iteratively refining responses. Flare ensures a dynamic and contextually aligned generation process.', 'These diverse approaches empower RAG to adapt to various use cases and retrieval scenarios, allowing for tailored solutions that maximize AI-generated responses’ relevance, accuracy, and efficiency.', 'Ethical Considerations in RAG', 'RAG introduces ethical considerations that demand careful attention:', 'Ensuring Fair and Responsible Use: Ethical deployment of RAG involves using the technology responsibly and refraining from any misuse or harmful applications. Developers and users must adhere to ethical guidelines to maintain the integrity of AI-generated content.', 'Addressing Privacy Concerns: RAG’s reliance on external data sources may involve accessing user data or sensitive information. Establishing robust privacy safeguards to protect individuals’ data and ensure compliance with privacy regulations is imperative.', 'Mitigating Biases in External Data Sources: External data sources can inherit biases in their content or collection methods. Developers must implement mechanisms to identify and rectify biases, ensuring AI-generated responses remain unbiased and fair. This involves constant monitoring and refinement of data sources and training processes.', 'Applications of Retrieval Augmented Generation (RAG)', 'RAG finds versatile applications across various domains, enhancing AI capabilities in different contexts:', 'Chatbots and AI Assistants: RAG-powered systems excel in question-answering scenarios, providing context-aware and detailed answers from extensive knowledge bases. These systems enable more informative and engaging interactions with users.', 'Education Tools: RAG can significantly improve educational tools by offering students access to answers, explanations, and additional context based on textbooks and reference materials. This facilitates more effective learning and comprehension.', 'Legal Research and Document Review: Legal professionals can leverage RAG models to streamline document review processes and conduct efficient legal research. RAG assists in summarizing statutes, case law, and other legal documents, saving time and improving accuracy.', 'Medical Diagnosis and Healthcare: In the healthcare domain, RAG models serve as valuable tools for doctors and medical professionals. They provide access to the latest medical literature and clinical guidelines, aiding in accurate diagnosis and treatment recommendations.', 'Language Translation with Context: RAG enhances language translation tasks by considering the context in knowledge bases. This approach results in more accurate translations, accounting for specific terminology and domain knowledge, particularly valuable in technical or specialized fields.', 'These applications highlight how RAG’s integration of external knowledge sources empowers AI systems to excel in various domains, providing context-aware, accurate, and valuable insights and responses.', 'The Future of RAGs and LLMs', 'The evolution of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) is poised for exciting developments:', 'Advancements in Retrieval Mechanisms: The future of RAG will witness refinements in retrieval mechanisms. These enhancements will focus on improving the precision and efficiency of document retrieval, ensuring that LLMs access the most relevant information quickly. Advanced algorithms and AI techniques will play a pivotal role in this evolution.', 'Integration with Multimodal AI: The synergy between RAG and multimodal AI, which combines text with other data types like images and videos, holds immense promise. Future RAG models will seamlessly incorporate multimodal data to provide richer and more contextually aware responses. This will open doors to innovative applications like content generation, recommendation systems, and virtual assistants.', 'RAG in Industry-Specific Applications: As RAG matures, it will find its way into industry-specific applications. Healthcare, law, finance, and education sectors will harness RAG-powered LLMs for specialized tasks. For example, in healthcare, RAG models will aid in diagnosing medical conditions by instantly retrieving the latest clinical guidelines and research papers, ensuring doctors have access to the most current information.', 'Ongoing Research and Innovation in RAG: The future of RAG is marked by relentless research and innovation. AI researchers will continue to push the boundaries of what RAG can achieve, exploring novel architectures, training methodologies, and applications. This ongoing pursuit of excellence will result in more accurate, efficient, and versatile RAG models.', 'LLMs with Enhanced Retrieval Capabilities: LLMs will evolve to possess enhanced retrieval capabilities as a core feature. They will seamlessly integrate retrieval and generation components, making them more efficient at accessing external knowledge sources. This integration will lead to LLMs that are proficient in understanding context and excel in providing context-aware responses.', 'Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)', 'Installation of LangChain and OpenAI Libraries', 'This line of code installs the LangChain and OpenAI libraries. LangChain is critical for handling text data and embedding, while OpenAI provides access to state-of-the-art Large Language Models (LLMs). This installation step is essential for setting up the required tools for RAG.', '!pip install langchain openai', '!pip install -q -U faiss-cpu tiktoken', 'import os', 'import getpass', 'os.environ[""OPENAI_API_KEY""] = getpass.getpass(""Open AI API Key:"")', 'Web Data Loading for the RAG Knowledge Base', 'The code utilizes LangChain’s “WebBaseLoader.”', 'Three web pages are specified for data retrieval: YOLO-NAS object detection, DeciCoder’s code generation efficiency, and a Deep Learning Daily newsletter.', 'This step is essential for building the knowledge base used in RAG, enabling contextually relevant and accurate information retrieval and integration into language model responses.', 'from langchain.document_loaders import WebBaseLoader', 'yolo_nas_loader = WebBaseLoader(""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"").load()', 'decicoder_loader = WebBaseLoader(""https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder\'s%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency."").load()', 'yolo_newsletter_loader = WebBaseLoader(""https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas"").load()', 'Embedding and Vector Store Setup', 'The code sets up embeddings for the RAG process.', 'It uses “OpenAIEmbeddings” to create an embedding model.', 'A “CacheBackedEmbeddings” object is initialized, allowing embeddings to be stored and retrieved efficiently using a local file store.', 'A “FAISS” vector store is created from the preprocessed chunks of web data (yolo_nas_chunks, decicoder_chunks, and yolo_newsletter_chunks), enabling fast and accurate similarity-based retrieval.', 'Finally, a retriever is instantiated from the vector store, facilitating efficient document retrieval during the RAG process.', 'from langchain.embeddings.openai import OpenAIEmbeddings', 'from langchain.embeddings import CacheBackedEmbeddings', 'from langchain.vectorstores import FAISS', 'from langchain.storage import LocalFileStore', 'store = LocalFileStore(""./cachce/"")', '# create an embedder', 'core_embeddings_model = OpenAIEmbeddings()', 'embedder = CacheBackedEmbeddings.from_bytes_store(', '    core_embeddings_model,', '    store,', '    namespace = core_embeddings_model.model', ')', '# store embeddings in vector store', 'vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)', 'vectorstore.add_documents(decicoder_chunks)', 'vectorstore.add_documents(yolo_newsletter_chunks)', '# instantiate a retriever', 'retriever = vectorstore.as_retriever()', 'Establishing the Retrieval System', 'The code configures the retrieval system for Retrieval Augmented Generation (RAG).', 'It uses “OpenAIChat” from the LangChain library to set up a chat-based Large Language Model (LLM).', 'A callback handler named “StdOutCallbackHandler” is defined to manage interactions with the retrieval system.', 'The “RetrievalQA” chain is created, incorporating the LLM, retriever (previously initialized), and callback handler.', 'This chain is designed to perform retrieval-based question-answering tasks, and it is configured to return source documents for added context during the RAG process.', 'from langchain.llms.openai import OpenAIChat', 'from langchain.chains import RetrievalQA', 'from langchain.callbacks import StdOutCallbackHandler', 'llm = OpenAIChat()', 'handler =  StdOutCallbackHandler()', '# This is the entire retrieval system', 'qa_with_sources_chain = RetrievalQA.from_chain_type(', '    llm=llm,', '    retriever=retriever,', '    callbacks=[handler],', '    return_source_documents=True', ')', 'Initializes the RAG System', 'The code sets up a RetrievalQA chain, a critical part of the RAG system, by combining an OpenAIChat language model (LLM) with a retriever and callback handler.', 'Issue Queries to the RAG System', 'It sends various user queries to the RAG system, prompting it to retrieve contextually relevant information.', 'Retrieves Responses', 'After processing the queries, the RAG system generates and returns contextually rich and accurate responses. The responses are printed on the console.', '# This is the entire augment system!', 'response = qa_with_sources_chain({""query"":""What does Neural Architecture Search have to do with how Deci creates its models?""})', 'response', ""print(response['result'])"", ""print(response['source_documents'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder""})', ""print(response['result'])"", 'This code exemplifies how RAG and LangChain can enhance information retrieval and generation in AI applications.', 'Output', 'Conclusion', 'Retrieval Augmented Generation (RAG) represents a transformative leap in artificial intelligence. It seamlessly integrates Large Language Models (LLMs) with external knowledge sources, addressing the limitations of LLMs’ parametric memory.', 'RAG’s ability to access real-time data, coupled with improved contextualization, enhances the relevance and accuracy of AI-generated responses. Its updatable memory ensures responses are current without extensive model retraining. RAG also offers source citations, bolstering transparency and reducing data leakage. In summary, RAG empowers AI to provide more accurate, context-aware, and reliable information, promising a brighter future for AI applications across industries.', 'Key Takeaways', 'Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.', 'RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.', 'With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.', 'RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.', 'This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.', 'Frequently Asked Questions', 'Q1. What is RAG? How does it differ from traditional AI models? A. RAG, or Retrieval Augmented Generation, is an innovative AI framework combining retrieval-based and generative models’ strengths. Unlike traditional AI models, which generate responses solely based on their pre-trained knowledge, RAG integrates external knowledge sources, allowing it to provide more accurate, up-to-date, and contextually relevant responses.  Q2. How does RAG ensure the accuracy of the retrieved information? A. RAG employs a retrieval system that fetches information from external sources. It ensures accuracy through techniques like vector similarity search and real-time updates to external datasets. Additionally, RAG allows users to access source citations, enhancing transparency and credibility.  Q3. Can RAG be used in specific industries or applications? A. Yes, RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.  Q4. Does implementing RAG require extensive technical expertise? A. While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.  Q5. What are the potential ethical concerns with RAG, such as misinformation or data privacy? A. RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns.  ', 'The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\xa0', 'Related', 'AIApplicationsblogathonData Sourcesdocumentslanguage modelsmemoryModels ', 'Recommended For You', 'Become a full stack data scientist', 'Large Language Models Demystified: A Beginner’s Roadmap', 'Training Your Own LLM Without Coding', 'How to Build LLMs for Code?\xa0', 'LLMs in Conversational AI: Building Smarter Chatbots & Assistants', 'From GPT-3 to Future Generations of Language Models', 'What are Large Language Models (LLMs)?', 'About the Author', 'Soumyadarshan Dash', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'How to Explore Text Generation with GPT-2? ', 'Next Post', 'How does Generative AI in Recipe Generation and Culinary Arts Work? ', 'Top Resources', '10 Best AI Image Generator Tools to Use in 2023', 'avcontentteam - ', 'Aug 17, 2023', 'Everything you need to Know about Linear Regression!', 'KAVITA MALI - ', 'Oct 04, 2021', 'Skewness and Kurtosis: Quick Guide (Updated 2023)', 'Suvarna Gawali - ', 'May 02, 2021', 'Guide on Support Vector Machine (SVM) Algorithm', 'Anshul Saini - ', 'Oct 12, 2021', '×', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Join the Community', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2023 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'Loading...', 'To continue reading please login', 'google-icon', 'Continue with Google', 'linkedin', 'Continue with Linkedin', 'email', 'Continue with Email', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Welcome Back :)', 'email 2', 'key-', 'Forgot Password?', 'Log In', ""Don't have an account yet?Register here"", 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Start your journey here!', 'user', 'user', 'email 2', 'key-', 'Sign up', 'Already have an accountLogin here', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', ' A verification link has been sent to your email id ', ' If you have not recieved the link please goto', 'Sign Up  page again', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your registered email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter your registered email id', 'email 2', 'Next', 'This email id is not registered with us. Please enter your registered email id.', ""Don't have an account yet?Register here"", 'Loading...', '×', 'back', 'Please enter the OTP that is sent your registered email id', 'email 2', 'Next', 'Loading...', '×', 'Please create the new password here', 'key-', 'key-', 'Submit', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview ', 'This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.', 'Necessary ', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. ', 'Non-necessary ', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. ', 'SAVE & ACCEPT', '×']","**Summarize the article in 10 bullet points**
- Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.
- RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.
- With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.
- RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.
- This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.
- RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.
- While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.
- RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns."
24,24,Retrieval Augmented Generation (RAG): Reducing ... - Pinecone,,"https://www.pinecone.io/learn/retrieval-augmented-generation/#:~:text=Retrieval%20Augmented%20Generation%20means%20fetching,a%20response%2C%20solving%20this%20problem.","['Retrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI Applications | PineconeProductSolutionsPricingResourcesCompanyLog InSign Up FreeLearn | ArticleRetrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI ApplicationsJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Products built on top of Large Language Models (LLMs) such as OpenAI\'s ChatGPT and Anthropic\'s Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.LLMs are “stuck” at a particular time, but RAG can bring them into the present.ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”The LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.The reason LLMs like ChatGPT feel so bright is that they\'ve seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.RAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.Why is RAG the preferred approach from a cost-efficacy perspective?There are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.Fine-tuning: adapting a foundation model to your domain’s data.Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.Prompt engineering is insufficient for reducing hallucinations.Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.It’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.Let’s now take a deeper dive into how Retrieval Augmented Generation works.We’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window"".An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.Creating a vector database from your domain-specific proprietary data using an embedding model.This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.You originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.You send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM\'s context window.Since the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.Vector databases can support even more advanced search functionality.Semantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query\'s semantics and keywords.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.Pinecone enables you to integrate RAG within minutes. Check out our examples repository on GitHub for runnable examples, such as this RAG Jupyter Notebook.Share via:  Zachary ProserStaff Developer AdvocateJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.PRODUCTOverviewDocumentationTrust and SecuritySOLUTIONSSearchGenerative AICustomersRESOURCESLearning CenterCommunityPinecone BlogSupport CenterSystem StatusCOMPANYAboutPartnersCareersNewsroomContactLEGALTermsPrivacyCookies© Pinecone Systems, Inc. | San Francisco, CAPinecone is a registered trademark of Pinecone Systems, Inc.']","**Retrieval Augmented Generation (RAG) is a technique that helps Large Language Models (LLMs) produce more accurate and relevant responses by providing them with additional context from a database of proprietary data.**

RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.

**Here are the key benefits of using RAG:**

* **RAG reduces the likelihood of hallucinations.** LLMs are often inaccurate when asked about things that have happened recently or about domain-specific information. RAG helps to address this problem by providing LLMs with up-to-date information and context from a database of proprietary data.
* **RAG allows GenAI applications to cite their sources.** This can improve auditability and make it easier for end users to understand how GenAI applications arrived at their conclusions.
* **RAG is easy to implement.** RAG can be implemented with a few code changes.
* **RAG is cost-effective.** RAG does not require the same level of expertise or resources as other methods for improving the performance of GenAI applications.

**Here are the steps involved in using RAG:**

1. Create a vector database from your domain-specific proprietary data.
2. Convert your user's query into embeddings.
3. Send the embeddings to the vector database.
4. The vector database will return the most relevant results.
5. Provide the results to the LLM via its context window.

**RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.**"
25,25,What is retrieval-augmented generation?,"Aug 22, 2023 — ",https://research.ibm.com/blog/retrieval-augmented-generation-RAG,"[""What is retrieval-augmented generation? | IBM Research BlogSkip to main contentResearchFocus areasBlogPublicationsCareersAboutBackFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBackAboutOverviewLabsPeopleCollaborateBackArtificial IntelligenceBackHybrid CloudBackQuantum ComputingBackScienceBackSecurityBackSemiconductorsBackOverviewBackLabsBackPeopleBackCollaborateResearchFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBlogPublicationsCareersAboutOverviewLabsPeopleCollaborateOpen IBM search fieldClose22 Aug 2023Explainer4 minute readWhat is retrieval-augmented generation?RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.Large language models can be inconsistent. Sometimes they nail the answer to questions, other times they regurgitate random facts from their training data. If they occasionally sound like they have no idea what they’re saying, it’s because they don’t. LLMs know how words relate statistically, but not what they mean.Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.“You want to cross-reference a model’s answers with the original content so you can see what it is basing its answer on,” said Luis Lastras, director of language technologies at IBM Research.RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. IBM unveiled its new AI and data platform, watsonx, which offers RAG, back in May.An ‘open book’ approach to answering tough questionsUnderpinning all foundation models, including LLMs, is an AI architecture known as the transformer. It turns heaps of raw data into a compressed representation of its basic structure. Starting from this raw representation, a foundation model can be adapted to a variety of tasks with some additional fine-tuning on labeled, domain-specific knowledge.But fine-tuning alone rarely gives the model the full breadth of knowledge it needs to answer highly specific questions in an ever-changing context. In a 2020 paper, Meta (then known as Facebook) came up with a framework called retrieval-augmented generation to give LLMs access to information beyond their training data. RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”As the name suggests, RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.Toward personalized and verifiable responsesBefore LLMs, digital conversation agents followed a manual dialogue flow. They confirmed the customer’s intent, fetched the requested information, and delivered an answer in a one-size-fits all script. For straightforward queries, this manual decision-tree method worked just fine.But it had limitations. Anticipating and scripting answers to every question a customer might conceivably ask took time; if you missed a scenario, the chatbot had no ability to improvise. Updating the scripts as policies and circumstances evolved was either impractical or impossible.Today, LLM-powered chatbots can give customers more personalized answers without humans having to write out new scripts. And RAG allows LLMs to go one step further by greatly reducing the need to feed and retrain the model on fresh examples. Simply upload the latest documents or policies, and the model retrieves the information in open-book mode to answer the question.IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources.Teaching the model to recognize when it doesn’t knowCustomer queries aren’t always this straightforward. They can be ambiguously worded, complex, or require knowledge the model either doesn’t have or can’t easily parse. These are the conditions in which LLMs are prone to making things up.“Think of the model as an overeager junior employee that blurts out an answer before checking the facts,” said Lastras. “Experience teaches us to stop and say when we don’t know something. But LLMs need to be explicitly trained to recognize questions they can’t answer.”In a more challenging scenario taken from real life, Alice wants to know how many days of maternity leave she gets. A chatbot that does not use RAG responds cheerfully (and incorrectly): “Take as long as you want.”Maternity-leave policies are complex, in part, because they vary by the state or country of the employee’s home-office. When the LLM failed to find a precise answer, it should have responded, “I’m sorry, I don’t know,” said Lastras, or asked additional questions until it could land on a question it could definitively answer. Instead, it pulled a phrase from a training set stocked with empathetic, customer-pleasing language.With enough fine-tuning, an LLM can be trained to pause and say when it’s stuck. But it may need to see thousands of examples of questions that can and can’t be answered. Only then can the model learn to identify an unanswerable question, and probe for more detail until it hits on a question that it has the information to answer.RAG is currently the best-known tool for grounding LLMs on the latest, verifiable information, and lowering the costs of having to constantly retrain and update them. But RAG is imperfect, and many interesting challenges remain in getting RAG done right.At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.Subscribe to our Future Forward newsletter and stay up to date on the latest research newsSubscribe to our newsletterHome↳ BlogDate22 Aug 2023AuthorsKim MartineauTopicsAIExplainable AIGenerative AINatural Language ProcessingTrustworthy GenerationShareWhat is AI inferencing?ExplainerKim Martineau05 Oct 2023AIFoundation ModelsGenerative AIHybrid CloudCelebrating a decade of IBM Research innovation in AfricaNewsMike Murphy28 Sep 2023AIQuantumFind and fix IT glitches before they crash the system NewsKim Martineau28 Sep 2023AI for CodeAI for ITExplainable AIFoundation ModelsGenerative AIHow open source paved the way for computing from anywhere NewsKim Martineau20 Sep 2023AIHybrid Cloud PlatformMachine LearningScaling AIPreviousError correcting codes for near-term quantum computersNextIBM Research’s newest prototype chips use drastically less power to solve AI tasksIBM LogoFocus areasFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usTwitterLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility""]","Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.

RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.

This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.

RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way. It reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting."
26,26,Retrieval Augmented Generation (RAG),RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient ...,https://www.promptingguide.ai/techniques/rag,"['Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuidePrompt Engineering CoursePrompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingSelf-ConsistencyGenerate Knowledge PromptingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingReActMultimodal CoTGraph PromptingApplicationsProgram-Aided Language ModelsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionModelsFlanChatGPTLLaMAGPT-4LLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesPapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightQuestion? Give us feedback → (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)', ""General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge."", 'For more complex and knowledge-intensive tasks, it\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of ""hallucination"".', 'Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.', ""RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."", 'Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:', 'Image Source: Lewis et el. (2021) (opens in a new tab)', 'RAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.', 'This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.', 'More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.', 'You can find a simple example of how to use retrievers and LLMs for question answering with sources (opens in a new tab) from the LangChain documentation.Tree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright © 2023 DAIR.AI']","Retrieval Augmented Generation (RAG) is a method proposed by Meta AI researchers to address knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.

RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.

RAG performs strong on several benchmarks such as Natural Questions, WebQuestions, and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.

This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks."
27,27,What Is Retrieval-Augmented Generation (RAG)?,"Sep 19, 2023 — ",https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/,"['What Is Retrieval-Augmented Generation (RAG)?', 'Accessibility Policy', 'Skip to content', 'About', 'Services', 'Solutions', 'Pricing', 'Partners', 'Resources', 'Close Search', 'Close', 'We’re sorry.  We could not find a match for your search.', ""We suggest you try the following to help find what you're looking for:"", 'Check the spelling of your keyword search.', 'Use synonyms for the keyword you typed, for example, try “application” instead of “software.”', 'Start a new search.', 'Clear Search', 'Search', 'Menu', 'Menu', 'Contact Sales', 'Sign in to Oracle Cloud', 'Cloud', 'Artificial Intelligence', 'Generative AI', 'What Is Retrieval-Augmented Generation (RAG)?', 'Alan Zeichick | Tech Content Strategist | September 19, 2023', 'In This Article', 'What Is Retrieval-Augmented Generation (RAG)?', 'Retrieval-Augmented Generation Explained', 'How Does Retrieval-Augmented Generation Work?', 'Using RAG in Chat Applications', 'Benefits of Retrieval-Augmented Generation', 'Challenges of Retrieval-Augmented Generation', 'Examples of Retrieval-Augmented Generation', 'Future of Retrieval-Augmented Generation', 'Generative AI With Oracle', 'Retrieval-Augmented Generation FAQs', 'Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.', 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees.', 'What Is Retrieval-Augmented Generation (RAG)?', 'That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.', 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.', 'Retrieval-Augmented Generation Explained', 'Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.', 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. ', 'Simply put, RAG helps LLMs give better answers.', 'Key Takeaways', 'RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.', 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.', 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.', 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.', 'How Does Retrieval-Augmented Generation Work? ', 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.', ' The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.', 'RAG and Large Language Models (LLMs)', 'Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.', 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.', 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.', 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.', 'Retrieval-Augmented Generation vs. Semantic Search', ' RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.', ' Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.', ' Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG.', 'Using RAG in Chat Applications', 'When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.', ' The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.', 'Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.', 'Benefits of Retrieval-Augmented Generation', 'RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following:', 'The RAG has access to information that may be fresher than the data used to train the LLM.', 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.', 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.', 'The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted.', 'Challenges of Retrieval-Augmented Generation', 'Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are', 'Improving organizational knowledge and understanding of RAG because it’s so new', 'Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself', 'Determining how to best model the structured and unstructured data within the knowledge library and vector database', 'Developing requirements for a process to incrementally feed data into the RAG system', 'Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system', 'Examples of Retrieval-Augmented Generation', 'There are many possible examples of generative AI augmented by RAG.', 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.', 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.', 'Future of Retrieval-Augmented Generation', 'Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.', 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.', 'RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.', 'Generative AI With Oracle', 'Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.', 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.', 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot.', 'What makes Oracle best suited for generative AI?', 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.', 'Learn more about Oracle’s generative AI strategy', 'Retrieval-Augmented Generation FAQs', 'Is RAG the same as generative AI?', 'No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.', 'What type of information is used in RAG?', 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.', 'How does generative AI use RAG?', 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.', 'Can a RAG cite references for the data it retrieves?', 'Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.', 'Resources for', 'Careers', 'Developers', 'Investors', 'Partners', 'Startups', 'Students and Educators', 'Why Oracle', 'Analyst Reports', 'Cloud Economics', 'with Microsoft Azure', 'vs. AWS', 'vs. Google Cloud', 'vs. MongoDB', 'Learn', 'What is AI?', 'What is Cloud Computing?', 'What is Cloud Storage?', 'What is HPC?', 'What is IaaS?', 'What is PaaS?', 'What’s new', 'Oracle Supports Ukraine', 'Oracle Cloud Free Tier', 'Cloud Architecture Center', 'Cloud Lift', 'Oracle Support Rewards', 'Oracle Red Bull Racing', 'Contact us', 'US Sales: +1.800.633.0738', 'How can we help?', 'Subscribe to emails', 'Events', 'News', 'OCI Blog', ' Country/Region ', '© 2023 Oracle', 'Privacy/Do Not Sell My Info', 'Ad Choices', 'Careers', 'Facebook', 'Twitter', 'LinkedIn', 'YouTube']","Retrieval-augmented generation (RAG) is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.

RAG models build knowledge repositories based on the organization's own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.

Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.

Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM."
28,28,What is Retrieval Augmented Generation (RAG)?,"RAG puts together a pre-trained system that finds relevant information ( retriever ) with another system that generates text ( generator ). Then, when the user ...",https://vercel.com/guides/retrieval-augmented-generation,"['What is Retrieval Augmented Generation (RAG)?Skip to contentDocumentationGuidesHelp← Back to GuidesWhat is Retrieval Augmented Generation (RAG)?Large-language modals (LLMs) like OpenAI\'s GPT-4 and Anthropic\'s Claude are incredible at generating coherent and contextually relevant text based on given prompts. They can assist in a wide range of tasks, such as writing, translation, and even conversation.Despite this, LLMs have limitations. In this guide, we\'ll go over these constraints and explain how Retrieval Augmented Generation (RAG) can alleviate these pains. We\'ll also dive into the ways you can build better chat experiences with this technique.The problem with LLMsAs groundbreaking as LLMs may be, they have a few limitations:They\'re limited by the amount of training data they have access to. For example, GPT-4 has a training data cutoff date, which means that it doesn\'t have access to information beyond that date. This limitation affects the model\'s ability to generate up-to-date and accurate responses.They\'re generic and lack subject-matter expertise. LLMs are trained on a large dataset that covers a wide range of topics, but they don\'t possess specialized knowledge in any particular field. This leads to hallucinations or inaccurate information when asked about specific subject areas.Citations are tricky. LLMs don\'t have a reliable way of returning the exact location of the text where they retrieved the information. This exacerbates the issue of hallucination, as they may not be able to provide proper attribution or verify the accuracy of their responses. Additionally, the lack of specific citations makes it difficult for users to fact-check or delve deeper into the information provided by the models.Retrieval Augmented GenerationTo solve this problem, researchers at Meta published a paper about a technique called Retrieval Augmented Generation (RAG), which adds an information retrieval component to the text generation model that LLMs are already good at. This allows for fine-tuning and adjustments to the LLM\'s internal knowledge, making it more accurate and up-to-date.Source: Lewis et el. (2021)\xa0–\xa0Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksHere\'s how RAG works on a high level:RAG puts together a pre-trained system that finds relevant information (retriever) with another system that generates text (generator).Then, when the user inputs a question (query), the retriever use a technique called ""Maximum Inner Product Search (MIPS)"" to find the most relevant documents. The information from these documents will then be fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.Using RAG in Chat ApplicationsTo illustrate how you can apply RAG in a real-world application, here\'s a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel\'s AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:Setting up a Next.js applicationCreating a chatbot frontend componentBuilding an API endpoint using OpenAI\'s API for response generationProgressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments. By following the tutorial, you\'ll build a context-aware chatbot with improved user experience.Build better AI chat experiences with RAGBy integrating Retrieval Augmented Generation into chat applications like the Pinecone chatbot template above, developers can reduce hallucinations in their AI models and create more accurate and evidence-based conversational experiences.If you\'re interested in learning more about RAG, check out this article about integrating RAG with Langchain and a Supabase vector database.Couldn\'t find the guide you need?View Help© 2023ProductPreviewsNext.jsInfrastructurev0Edge FunctionsTurboAnalyticsEnterpriseChangelogCLI & APIResourcesDocsExpertsPricingGuidesCustomersHelpIntegrations⌘KTemplatesCompanyAboutBlogCareersContact UsNext.js ConfOpen SourcePartnersSecurityPrivacy PolicyLegal']","Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generative models to deliver accurate responses. It is used to solve the problem of hallucinations or inaccurate information when asked about specific subject areas.

RAG works by first finding the most relevant documents to the user's query. The information from these documents is then fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.

To illustrate how you can apply RAG in a real-world application, here's a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:

```
npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""
```

The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel's AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:

* Setting up a Next.js application
* Creating a chatbot frontend component
* Building an API endpoint using OpenAI's API for response generation
* Progressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments.

By following the tutorial, you'll build a context-aware chatbot with improved user experience."
29,29,Retrieval Augmented Generation (RAG),You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data ...,https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html,"['Retrieval Augmented Generation (RAG) - Amazon SageMakerRetrieval Augmented Generation (RAG) - Amazon SageMakerAWSDocumentationAmazon SageMakerDeveloper GuideRetrieval Augmented Generation (RAG)Foundation models are usually trained offline, making the model agnostic to any', '                data that is created after the model was trained. Additionally, foundation models', '                are trained on very general domain corpora, making them less effective for', '                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve', '                data from outside a foundation model and augment your prompts by adding the relevant', '                retrieved data in context. For more information about RAG model architectures, see', '                    Retrieval-Augmented Generation for', '                    Knowledge-Intensive NLP Tasks.With RAG, the external data used to augment your prompts can come from multiple', '                data sources, such as a document repositories, databases, or APIs. The first step is', '                to convert your documents and any user queries into a compatible format to perform', '                relevancy search. To make the formats compatible, a document collection, or', '                knowledge library, and user-submitted queries are converted to numerical', '                representations using embedding language models. Embedding is the process by which text is given numerical', '                representation in a vector space. RAG model architectures compare the embeddings of', '                user queries within the vector of the knowledge library. The original user prompt is', '                then appended with relevant context from similar documents within the knowledge', '                library. This augmented prompt is then sent to the foundation model. You can update', '                knowledge libraries and their relevant embeddings asynchronously.', 'For more information, see the following example notebooks: ', 'Retrieval-Augmented Generation: Question Answering based on Custom', '                            Dataset', 'Retrieval-Augmented Generation: Question Answering based on Custom', ""                            Dataset with Open-sourced LangChain Library Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsFine-tuningTask-Specific ModelsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.""]","Retrieval Augmented Generation (RAG) is a technique that can be used to improve the performance of foundation models on domain-specific tasks.

RAG works by retrieving relevant data from outside the foundation model and augmenting the user's prompt with this data. This augmented prompt is then sent to the foundation model, which can then generate more relevant results.

For more information, see the following example notebooks:

* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset](https://github.com/aws-samples/sagemaker-rag-qa)
* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library](https://github.com/aws-samples/sagemaker-rag-qa-langchain)"
30,30,What Is Retrieval-Augmented Generation? | Definition from ...,Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the ...,https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation,"['What Is Retrieval-Augmented Generation? | Definition from TechTarget', 'Enterprise AI', 'Search the TechTarget Network', 'Login', 'Register', 'Explore the Network', 'TechTarget Network', 'Business Analytics', 'CIO', 'Data Management', 'ERP', 'Enterprise AI', 'AI Business Strategies', 'AI Careers', 'AI Infrastructure', 'AI Platforms ', 'AI Technologies', 'More Topics', 'Applications of AI', 'ML Platforms', 'Other Content', 'News', 'Features', 'Tips', 'Webinars', '2023 IT Salary Survey Results', '                                More', 'Answers', 'Conference Guides', 'Definitions', 'Opinions', 'Podcasts', 'Quizzes', 'Tech Accelerators', 'Tutorials', 'Videos', 'Sponsored Communities', 'Follow:', 'Home', 'AI technologies', 'Tech Accelerator', 'What is generative AI? Everything you need to know', 'Prev', 'Next', 'Generative AI in the enterprise raises questions for CIOs', '15 of the best large language models', 'Download this guide1', 'X', 'Free Download', 'What is generative AI? Everything you need to know', 'The potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight.', 'This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.', 'Corporate Email Address:You forgot to provide an Email Address.This email address doesn’t appear to be valid.This email address is already registered. Please log in.You have exceeded the maximum character limit.Please provide a Corporate Email Address.I agree to TechTarget’s Terms of Use, Privacy Policy, and the transfer of my information to the United States for processing to provide me with relevant information as described in our Privacy Policy.Please check the box if you want to proceed.I agree to my information being processed by TechTarget and its Partners to contact me via phone, email, or other means regarding information relevant to my professional interests. I may unsubscribe at any time.Please check the box if you want to proceed.', 'By submitting my Email address I confirm that I have read and accepted the Terms of Use and Declaration of Consent.', 'Definition', 'retrieval-augmented generation ', 'Share this item with your network:', 'By', 'Alexander S. Gillis,', 'Technical Writer and Editor', 'What is retrieval-augmented generation?', 'Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.', ""LLMs are AI models that power chatbots such as OpenAI's ChatGPT and Google Bard. LLMs can understand, summarize, generate and predict new content. However, they can still be inconsistent and fail at some knowledge-intensive tasks -- especially tasks that are outside their initial training data or those that require up-to-date information and transparency about how they make their decisions. When this happens, the LLM can return false information, also known as an AI hallucination."", ""By retrieving information from external sources when the LLM's trained data isn't enough, the quality of LLM responses improves. Retrieving information from an online source, for example, enables the LLM to access current information that it wasn't initially trained on."", 'What does RAG do?', ""LLMs are commonly trained offline, making the model uncertain of any data that's created after the model was trained. RAG is used to retrieve data from outside the LLM, which then augments the user's prompts by adding relevant retrieved data in its response."", 'This article is part of', 'What is generative AI? Everything you need to know', 'Which also includes:', '15 of the best large language models', 'Will AI replace jobs? 9 job types that might be affected', 'Pros and cons of AI-generated content', 'This process helps reduce any apparent knowledge gaps and AI hallucinations. This can be important in fields that require as much up-to-date and accurate information as possible, such as healthcare.', 'How to use RAG with LLMs', 'RAG combines information retrieval with a text generator model. External knowledge can be retrieved from data sources, online sources, application programming interfaces, databases or document repositories. ', 'Using the example of a chatbot, once a user inputs a prompt, RAG summarizes that prompt using keywords or semantic data. The converted data is then sent to a search platform to retrieve the requested data, which is then sorted through based on relevancy.', 'The LLM then synthesizes the retrieved data with the augmented prompt and its internal training data to create a generated response that can be passed to the chatbot with sourced links for the user.', 'An LLM using RAG can pull from both internal and external data to return a response for users, ensuring it provides relevant information.', 'What are the benefits of RAG?', 'Benefits of a RAG model include the following:', 'Provides current information. RAG pulls information from relevant, reliable and up-to-date sources.', ""Increases user trust. Users can access the model's sources, which promotes transparency and trust in the content and lets users verify its accuracy."", 'Reduces AI hallucinations. Because LLMs are grounded to external data, the model has less of a chance to make up or return incorrect information.', ""Reduces computational and financial costs. Organizations don't have to spend time and resources to continuously train the model on new data."", 'Synthesizes information. RAG synthesizes data by combining relevant information from retrieval and generative models to produce a response.', 'Easier to train. Because RAG uses retrieved knowledge sources, the need to train the LLM on a massive amount of training data is reduced.', 'Can be used for multiple tasks. Aside from chatbots, RAG can be fine-tuned for a variety of specific use cases, such as text summarization and dialogue systems.', 'Learn more about generative AI models, such as VAEs, GANs, diffusion, transformers and NeRFs.', '\t\t\t\t\tThis was last updated in October 2023', '\t\t\tContinue Reading About retrieval-augmented generation', '7 generative AI challenges that businesses should consider', 'Generative AI ethics: 8 biggest concerns', 'Assessing different types of generative AI applications', 'Pros and cons of AI-generated content', 'Cohesity Turing aims AI tools at backup and ransomware', '\t\t\t\tRelated Terms', 'language modeling', 'Language modeling, or LM, is the use of various statistical and probabilistic techniques to determine the probability of a given ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'OpenAI', 'OpenAI is a private research laboratory that aims to develop and direct artificial intelligence (AI) in ways that benefit ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Salesforce Einstein', 'Salesforce Einstein refers to an integrated set of artificial intelligence (AI) technologies developed for the Salesforce ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Dig Deeper on AI technologies', 'Databricks improves support for generative AI models', 'By: Eric\xa0Avidon', '15 of the best large language models', 'By: Ben\xa0Lutkevich', 'LangChain', 'By: Cameron\xa0Hashemi-Pour', 'large language models (LLMs)', 'By: Sean\xa0Kerner', 'Sponsored News', '4 Things You Need to Know Now About Edge Computing', '–HPE', 'Supporting a more diverse management team', '–AWS', 'See More', 'Vendor Resources', ""What generative AI's rise means for the cybersecurity industry"", '–TechTarget ComputerWeekly.com', 'Fast Track Generative AI with Dell™ Poweredge™ XE9680', '–Dell Technologies & Intel®', 'Latest TechTarget resources', '\t\t\t\t\t\t\tBusiness Analytics', '\t\t\t\t\t\t\tCIO', '\t\t\t\t\t\t\tData Management', '\t\t\t\t\t\t\tERP', 'Business Analytics', 'MicroStrategy launches new suite of generative AI tools', ""The longtime analytics vendor's suite incorporates LLM technology to make users more efficient by enabling them to use natural ..."", 'Sisense unveils composable toolkit for app development', 'Compose SDK for Fusion is a composable set of APIs that enable developers to build customized advanced analytics applications to ...', 'SAS unveils plans to add generative AI to analytics suite', 'After holding off on integrating with LLMs until it could ensure data security and accurate outcomes, the vendor is making ...', 'CIO', '7 challenges with blockchain adoption and how to avoid them', 'Organizations tend to face the same hurdles when they try to implement blockchain. Knowing what they are could be the first big ...', 'U.S. antitrust law enforcers defend actions, lawsuits', 'The FTC and DOJ, which enforce U.S. antitrust law, are focused on reining in big tech through antitrust lawsuits and revising ...', 'Is quantum computing overhyped?', ""Quantum computing may be coming to the enterprise. Here's what to understand about the benefits it promises, the risks it poses ..."", 'Data Management', 'Amazon launches DataZone, a new data management service', ""The tech giant's new service provides data governance, collaboration and catalog capabilities that enable organizations to find ..."", 'Databricks improves support for generative AI models', 'A new service enables users to easily deploy privately built language models and uses a GPU-based architecture to optimize and ...', 'New Boomi AI tool enables natural language data integration', ""The iPaaS vendor's new capabilities are aimed at increasing efficiency by enabling customers to build pipelines and manage data ..."", 'ERP', 'Infor Enterprise Automation looks to ease RPA for ERP', 'Infor unveiled Enterprise Automation, which is designed to help customers bring RPA to Infor cloud ERP applications and Developer...', 'When integrating generative AI and ERP, focus on use cases', 'ERP vendors are rapidly introducing new AI functionality, but experts caution against the hype and advise using the ...', 'PLM and PDM software: Learn the differences', 'PLM and PDM software may seem similar, but they fulfill different needs for manufacturers. Learn the differences and which is ...', 'About Us', 'Editorial Ethics Policy', 'Meet The Editors', 'Contact Us', 'Advertisers', 'Partner with Us', 'Media Kit', 'Corporate Site', 'Contributors', 'Reprints', 'Answers', 'Definitions', 'E-Products', 'Events', 'Features', 'Guides', 'Opinions', 'Photo Stories', 'Quizzes', 'Tips', 'Tutorials', 'Videos', 'All Rights Reserved, ', 'Copyright 2018 - 2023, TechTarget', 'Privacy Policy', 'Cookie Preferences ', 'Cookie Preferences ', 'Do Not Sell or Share My Personal Information', 'Close']",
31,31,Retrieval-Augmented Generation (RAG) in AI,"Sep 29, 2023 — ",https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/,"['Retrieval-Augmented Generation (RAG) in AI', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'Home', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'What is Retrieval-Augmented Generation (RAG) in AI?', 'Facebook', 'Twitter', 'Linkedin', 'Soumyadarshan Dash —', 'Updated On September 29th, 2023 ', 'Advanced', 'Artificial Intelligence', 'Excel', 'Generative AI', 'Healthcare', 'LLMs', 'NLP', 'Introduction', 'The rapid advancements in Large Language Models (LLMs) have transformed the landscape of AI, offering unparalleled capabilities in natural language understanding and generation. LLMs have ushered in a new language understanding and generation era, with OpenAI’s GPT models at the forefront. These remarkable models honed on extensive online data, have broadened our horizons, enabling us to interact with AI-powered systems like never before. However, like any technological marvel, they come with their own set of limitations. One glaring issue is their occasional tendency to provide information that is either inaccurate or outdated. Moreover, these LLMs do not furnish the sources of their responses, making it challenging to verify the reliability of their output. This limitation becomes especially critical in contexts where accuracy and traceability are paramount. Retrieval Augmented Generation (RAG) in AI is a transformative paradigm that promises to revolutionize the capabilities of LLMs.', 'Rapid advancements in LLMs have propelled them to the forefront of AI, yet they still grapple with constraints like information capacity and occasional inaccuracies. RAG bridges these gaps by seamlessly integrating retrieval-based and generative components, endowing LLMs to tap into external knowledge sources. This article explores RAG’s profound impact, unraveling its architecture, benefits, challenges, and the diverse approaches that empower it. In doing so, we unveil the potential of RAG to redefine the landscape of Large Language Models and pave the way for more accurate, context-aware, and reliable AI-driven communication.', 'Learning Objectives', 'Learn about language models and how RAG enhances their capabilities.', 'Discover methods to integrate external data into RAG systems effectively.', 'Explore ethical issues in RAG, including bias and privacy.', 'Gain hands-on experience with RAG using LangChain for real-world applications.', 'This article was published as a part of the\xa0Data Science Blogathon.', 'Table of contentsIntroductionUnderstanding Retrieval Augmented Generation (RAG)The Power of External DataBenefits of Retrieval Augmented Generation (RAG)Diverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsUtilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)OutputConclusionFrequently Asked Questions', 'Understanding Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.', 'Do you want to know more about RAG? Read more here. ', 'What is the Need for RAG?', 'The development of RAG is a direct response to the limitations of Large Language Models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG aims to bridge this gap by offering a solution that excels in understanding user intent and delivering meaningful and context-aware replies.', 'The Fusion of Retrieval-Based and Generative Models', 'RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.', 'Deconstructing RAG’s Mechanics', 'To grasp the essence of RAG, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps. ', 'Begin by receiving and processing user input.', 'Analyze the user input to understand its meaning and intent.', 'Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.', 'Use the retrieved external knowledge to enhance comprehension.', 'Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.', 'Combine all the information gathered to produce responses that are meaningful and human-like.', 'Ensure that the transformation of user queries into responses is done effectively.', 'The Role of Language Models and User Input', 'Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects through its integration of retrieval and generation.', 'Incorporating External Knowledge Sources', 'RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.', 'Generating Contextual Responses', 'Ultimately, the hallmark of RAG is its ability to generate contextual responses. It considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. These context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.', 'Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. By harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.', 'The Power of External Data', 'In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.', 'APIs and Real-time Databases', 'APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They allow models to access the latest data as it becomes available.', 'Document Repositories', 'Document repositories serve as valuable knowledge stores, offering structured and unstructured information. They are fundamental in expanding the knowledge base that RAG models can draw upon.', 'Webpages and Scraping', 'Web scraping is a method for extracting information from web pages. It enables RAG models to access dynamic web content, making it a crucial source for real-time data retrieval.', 'Databases and Structured Information', 'Databases provide structured data that can be queried and extracted. RAG models can use databases to retrieve specific information, enhancing the accuracy of their responses.', 'Benefits of Retrieval Augmented Generation (RAG)', 'Enhanced LLM Memory', 'RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.', 'Improved Contextualization', 'RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.', 'Updatable Memory', 'A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. This keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.', 'Source Citations', 'RAG-equipped models can provide sources for their responses, enhancing transparency and credibility. Users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.', 'Reduced Hallucinations', 'Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.', 'These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing, overcoming the limitations of traditional language models and enhancing the capabilities of AI-powered applications.', 'Diverse Approaches in RAG', 'RAG offers a spectrum of approaches for the retrieval mechanism, catering to various needs and scenarios:', 'Simple: Retrieve relevant documents and seamlessly incorporate them into the generation process, ensuring comprehensive responses.', 'Map Reduce: Combine responses generated individually for each document to craft the final response, synthesizing insights from multiple sources.', 'Map Refine: Iteratively refine responses using initial and subsequent documents, enhancing response quality through continuous improvement.', 'Map Rerank: Rank responses and select the highest-ranked response as the final answer, prioritizing accuracy and relevance.', 'Filtering: Apply advanced models to filter documents, utilizing the refined set as context for generating more focused and contextually relevant responses.', 'Contextual Compression: Extract pertinent snippets from documents, generating concise and informative responses and minimizing information overload.', 'Summary-Based Index: Leverage document summaries, index document snippets, and generate responses using relevant summaries and snippets, ensuring concise yet informative answers.', 'Forward-Looking Active Retrieval Augmented Generation (FLARE): Predict forthcoming sentences by initially retrieving relevant documents and iteratively refining responses. Flare ensures a dynamic and contextually aligned generation process.', 'These diverse approaches empower RAG to adapt to various use cases and retrieval scenarios, allowing for tailored solutions that maximize AI-generated responses’ relevance, accuracy, and efficiency.', 'Ethical Considerations in RAG', 'RAG introduces ethical considerations that demand careful attention:', 'Ensuring Fair and Responsible Use: Ethical deployment of RAG involves using the technology responsibly and refraining from any misuse or harmful applications. Developers and users must adhere to ethical guidelines to maintain the integrity of AI-generated content.', 'Addressing Privacy Concerns: RAG’s reliance on external data sources may involve accessing user data or sensitive information. Establishing robust privacy safeguards to protect individuals’ data and ensure compliance with privacy regulations is imperative.', 'Mitigating Biases in External Data Sources: External data sources can inherit biases in their content or collection methods. Developers must implement mechanisms to identify and rectify biases, ensuring AI-generated responses remain unbiased and fair. This involves constant monitoring and refinement of data sources and training processes.', 'Applications of Retrieval Augmented Generation (RAG)', 'RAG finds versatile applications across various domains, enhancing AI capabilities in different contexts:', 'Chatbots and AI Assistants: RAG-powered systems excel in question-answering scenarios, providing context-aware and detailed answers from extensive knowledge bases. These systems enable more informative and engaging interactions with users.', 'Education Tools: RAG can significantly improve educational tools by offering students access to answers, explanations, and additional context based on textbooks and reference materials. This facilitates more effective learning and comprehension.', 'Legal Research and Document Review: Legal professionals can leverage RAG models to streamline document review processes and conduct efficient legal research. RAG assists in summarizing statutes, case law, and other legal documents, saving time and improving accuracy.', 'Medical Diagnosis and Healthcare: In the healthcare domain, RAG models serve as valuable tools for doctors and medical professionals. They provide access to the latest medical literature and clinical guidelines, aiding in accurate diagnosis and treatment recommendations.', 'Language Translation with Context: RAG enhances language translation tasks by considering the context in knowledge bases. This approach results in more accurate translations, accounting for specific terminology and domain knowledge, particularly valuable in technical or specialized fields.', 'These applications highlight how RAG’s integration of external knowledge sources empowers AI systems to excel in various domains, providing context-aware, accurate, and valuable insights and responses.', 'The Future of RAGs and LLMs', 'The evolution of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) is poised for exciting developments:', 'Advancements in Retrieval Mechanisms: The future of RAG will witness refinements in retrieval mechanisms. These enhancements will focus on improving the precision and efficiency of document retrieval, ensuring that LLMs access the most relevant information quickly. Advanced algorithms and AI techniques will play a pivotal role in this evolution.', 'Integration with Multimodal AI: The synergy between RAG and multimodal AI, which combines text with other data types like images and videos, holds immense promise. Future RAG models will seamlessly incorporate multimodal data to provide richer and more contextually aware responses. This will open doors to innovative applications like content generation, recommendation systems, and virtual assistants.', 'RAG in Industry-Specific Applications: As RAG matures, it will find its way into industry-specific applications. Healthcare, law, finance, and education sectors will harness RAG-powered LLMs for specialized tasks. For example, in healthcare, RAG models will aid in diagnosing medical conditions by instantly retrieving the latest clinical guidelines and research papers, ensuring doctors have access to the most current information.', 'Ongoing Research and Innovation in RAG: The future of RAG is marked by relentless research and innovation. AI researchers will continue to push the boundaries of what RAG can achieve, exploring novel architectures, training methodologies, and applications. This ongoing pursuit of excellence will result in more accurate, efficient, and versatile RAG models.', 'LLMs with Enhanced Retrieval Capabilities: LLMs will evolve to possess enhanced retrieval capabilities as a core feature. They will seamlessly integrate retrieval and generation components, making them more efficient at accessing external knowledge sources. This integration will lead to LLMs that are proficient in understanding context and excel in providing context-aware responses.', 'Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)', 'Installation of LangChain and OpenAI Libraries', 'This line of code installs the LangChain and OpenAI libraries. LangChain is critical for handling text data and embedding, while OpenAI provides access to state-of-the-art Large Language Models (LLMs). This installation step is essential for setting up the required tools for RAG.', '!pip install langchain openai', '!pip install -q -U faiss-cpu tiktoken', 'import os', 'import getpass', 'os.environ[""OPENAI_API_KEY""] = getpass.getpass(""Open AI API Key:"")', 'Web Data Loading for the RAG Knowledge Base', 'The code utilizes LangChain’s “WebBaseLoader.”', 'Three web pages are specified for data retrieval: YOLO-NAS object detection, DeciCoder’s code generation efficiency, and a Deep Learning Daily newsletter.', 'This step is essential for building the knowledge base used in RAG, enabling contextually relevant and accurate information retrieval and integration into language model responses.', 'from langchain.document_loaders import WebBaseLoader', 'yolo_nas_loader = WebBaseLoader(""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"").load()', 'decicoder_loader = WebBaseLoader(""https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder\'s%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency."").load()', 'yolo_newsletter_loader = WebBaseLoader(""https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas"").load()', 'Embedding and Vector Store Setup', 'The code sets up embeddings for the RAG process.', 'It uses “OpenAIEmbeddings” to create an embedding model.', 'A “CacheBackedEmbeddings” object is initialized, allowing embeddings to be stored and retrieved efficiently using a local file store.', 'A “FAISS” vector store is created from the preprocessed chunks of web data (yolo_nas_chunks, decicoder_chunks, and yolo_newsletter_chunks), enabling fast and accurate similarity-based retrieval.', 'Finally, a retriever is instantiated from the vector store, facilitating efficient document retrieval during the RAG process.', 'from langchain.embeddings.openai import OpenAIEmbeddings', 'from langchain.embeddings import CacheBackedEmbeddings', 'from langchain.vectorstores import FAISS', 'from langchain.storage import LocalFileStore', 'store = LocalFileStore(""./cachce/"")', '# create an embedder', 'core_embeddings_model = OpenAIEmbeddings()', 'embedder = CacheBackedEmbeddings.from_bytes_store(', '    core_embeddings_model,', '    store,', '    namespace = core_embeddings_model.model', ')', '# store embeddings in vector store', 'vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)', 'vectorstore.add_documents(decicoder_chunks)', 'vectorstore.add_documents(yolo_newsletter_chunks)', '# instantiate a retriever', 'retriever = vectorstore.as_retriever()', 'Establishing the Retrieval System', 'The code configures the retrieval system for Retrieval Augmented Generation (RAG).', 'It uses “OpenAIChat” from the LangChain library to set up a chat-based Large Language Model (LLM).', 'A callback handler named “StdOutCallbackHandler” is defined to manage interactions with the retrieval system.', 'The “RetrievalQA” chain is created, incorporating the LLM, retriever (previously initialized), and callback handler.', 'This chain is designed to perform retrieval-based question-answering tasks, and it is configured to return source documents for added context during the RAG process.', 'from langchain.llms.openai import OpenAIChat', 'from langchain.chains import RetrievalQA', 'from langchain.callbacks import StdOutCallbackHandler', 'llm = OpenAIChat()', 'handler =  StdOutCallbackHandler()', '# This is the entire retrieval system', 'qa_with_sources_chain = RetrievalQA.from_chain_type(', '    llm=llm,', '    retriever=retriever,', '    callbacks=[handler],', '    return_source_documents=True', ')', 'Initializes the RAG System', 'The code sets up a RetrievalQA chain, a critical part of the RAG system, by combining an OpenAIChat language model (LLM) with a retriever and callback handler.', 'Issue Queries to the RAG System', 'It sends various user queries to the RAG system, prompting it to retrieve contextually relevant information.', 'Retrieves Responses', 'After processing the queries, the RAG system generates and returns contextually rich and accurate responses. The responses are printed on the console.', '# This is the entire augment system!', 'response = qa_with_sources_chain({""query"":""What does Neural Architecture Search have to do with how Deci creates its models?""})', 'response', ""print(response['result'])"", ""print(response['source_documents'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder""})', ""print(response['result'])"", 'This code exemplifies how RAG and LangChain can enhance information retrieval and generation in AI applications.', 'Output', 'Conclusion', 'Retrieval Augmented Generation (RAG) represents a transformative leap in artificial intelligence. It seamlessly integrates Large Language Models (LLMs) with external knowledge sources, addressing the limitations of LLMs’ parametric memory.', 'RAG’s ability to access real-time data, coupled with improved contextualization, enhances the relevance and accuracy of AI-generated responses. Its updatable memory ensures responses are current without extensive model retraining. RAG also offers source citations, bolstering transparency and reducing data leakage. In summary, RAG empowers AI to provide more accurate, context-aware, and reliable information, promising a brighter future for AI applications across industries.', 'Key Takeaways', 'Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.', 'RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.', 'With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.', 'RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.', 'This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.', 'Frequently Asked Questions', 'Q1. What is RAG? How does it differ from traditional AI models? A. RAG, or Retrieval Augmented Generation, is an innovative AI framework combining retrieval-based and generative models’ strengths. Unlike traditional AI models, which generate responses solely based on their pre-trained knowledge, RAG integrates external knowledge sources, allowing it to provide more accurate, up-to-date, and contextually relevant responses.  Q2. How does RAG ensure the accuracy of the retrieved information? A. RAG employs a retrieval system that fetches information from external sources. It ensures accuracy through techniques like vector similarity search and real-time updates to external datasets. Additionally, RAG allows users to access source citations, enhancing transparency and credibility.  Q3. Can RAG be used in specific industries or applications? A. Yes, RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.  Q4. Does implementing RAG require extensive technical expertise? A. While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.  Q5. What are the potential ethical concerns with RAG, such as misinformation or data privacy? A. RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns.  ', 'The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\xa0', 'Related', 'AIApplicationsblogathonData Sourcesdocumentslanguage modelsmemoryModels ', 'Recommended For You', 'Become a full stack data scientist', 'Large Language Models Demystified: A Beginner’s Roadmap', 'Training Your Own LLM Without Coding', 'How to Build LLMs for Code?\xa0', 'LLMs in Conversational AI: Building Smarter Chatbots & Assistants', 'From GPT-3 to Future Generations of Language Models', 'What are Large Language Models (LLMs)?', 'About the Author', 'Soumyadarshan Dash', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'How to Explore Text Generation with GPT-2? ', 'Next Post', 'How does Generative AI in Recipe Generation and Culinary Arts Work? ', 'Top Resources', '10 Best AI Image Generator Tools to Use in 2023', 'avcontentteam - ', 'Aug 17, 2023', 'Everything you need to Know about Linear Regression!', 'KAVITA MALI - ', 'Oct 04, 2021', 'Skewness and Kurtosis: Quick Guide (Updated 2023)', 'Suvarna Gawali - ', 'May 02, 2021', 'Guide on Support Vector Machine (SVM) Algorithm', 'Anshul Saini - ', 'Oct 12, 2021', '×', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Join the Community', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2023 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'Loading...', 'To continue reading please login', 'google-icon', 'Continue with Google', 'linkedin', 'Continue with Linkedin', 'email', 'Continue with Email', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Welcome Back :)', 'email 2', 'key-', 'Forgot Password?', 'Log In', ""Don't have an account yet?Register here"", 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Start your journey here!', 'user', 'user', 'email 2', 'key-', 'Sign up', 'Already have an accountLogin here', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', ' A verification link has been sent to your email id ', ' If you have not recieved the link please goto', 'Sign Up  page again', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your registered email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter your registered email id', 'email 2', 'Next', 'This email id is not registered with us. Please enter your registered email id.', ""Don't have an account yet?Register here"", 'Loading...', '×', 'back', 'Please enter the OTP that is sent your registered email id', 'email 2', 'Next', 'Loading...', '×', 'Please create the new password here', 'key-', 'key-', 'Submit', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview ', 'This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.', 'Necessary ', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. ', 'Non-necessary ', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. ', 'SAVE & ACCEPT', '×']","**Summarize the article in 10 bullet points**
- Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.
- RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.
- With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.
- RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.
- This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.
- RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.
- While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.
- RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns."
32,32,Retrieval Augmented Generation (RAG): Reducing ... - Pinecone,,"https://www.pinecone.io/learn/retrieval-augmented-generation/#:~:text=Retrieval%20Augmented%20Generation%20means%20fetching,a%20response%2C%20solving%20this%20problem.","['Retrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI Applications | PineconeProductSolutionsPricingResourcesCompanyLog InSign Up FreeLearn | ArticleRetrieval Augmented Generation (RAG): Reducing Hallucinations in GenAI ApplicationsJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Products built on top of Large Language Models (LLMs) such as OpenAI\'s ChatGPT and Anthropic\'s Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.LLMs are “stuck” at a particular time, but RAG can bring them into the present.ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”The LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.The reason LLMs like ChatGPT feel so bright is that they\'ve seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.RAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.Why is RAG the preferred approach from a cost-efficacy perspective?There are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.Fine-tuning: adapting a foundation model to your domain’s data.Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.Prompt engineering is insufficient for reducing hallucinations.Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.It’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.Let’s now take a deeper dive into how Retrieval Augmented Generation works.We’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window"".An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.Creating a vector database from your domain-specific proprietary data using an embedding model.This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.You originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.You send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM\'s context window.Since the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.Vector databases can support even more advanced search functionality.Semantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query\'s semantics and keywords.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.Pinecone enables you to integrate RAG within minutes. Check out our examples repository on GitHub for runnable examples, such as this RAG Jupyter Notebook.Share via:  Zachary ProserStaff Developer AdvocateJump to section LLMs are “stuck” at a particular time, but RAG can bring them into the present.RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.Why is RAG the preferred approach from a cost-efficacy perspective?Let’s now take a deeper dive into how Retrieval Augmented Generation works.Vector databases allow you to query data using natural language, which is ideal for chat interfaces.Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.PRODUCTOverviewDocumentationTrust and SecuritySOLUTIONSSearchGenerative AICustomersRESOURCESLearning CenterCommunityPinecone BlogSupport CenterSystem StatusCOMPANYAboutPartnersCareersNewsroomContactLEGALTermsPrivacyCookies© Pinecone Systems, Inc. | San Francisco, CAPinecone is a registered trademark of Pinecone Systems, Inc.']","**Retrieval Augmented Generation (RAG) is a technique that helps Large Language Models (LLMs) produce more accurate and relevant responses by providing them with additional context from a database of proprietary data.**

RAG is the most cost-effective, easy to implement, and lowest-risk path to improving the performance of GenAI applications.

**Here are the key benefits of using RAG:**

* **RAG reduces the likelihood of hallucinations.** LLMs are often referred to as ""black boxes"" because it can be difficult to understand which sources they were considering when they arrived at their conclusions. RAG provides additional context that helps to make LLMs' responses more transparent and easier to audit.
* **RAG allows GenAI applications to cite their sources.** This is important for applications that need to provide evidence to support their recommendations, such as legal applications or medical applications.
* **RAG is the most cost-effective way to improve the performance of GenAI applications.** It is much cheaper to use RAG than to build or fine-tune a LLM from scratch.

**How does RAG work?**

RAG works by passing additional relevant content from a database of proprietary data to an LLM at generation time. This additional content is called the ""context window."" The context window provides the LLM with the information it needs to produce a more accurate and relevant response.

**How to implement RAG?**

To implement RAG, you need to first create a vector database of your proprietary data. You can then use a semantic search engine to retrieve relevant content from the vector database and pass it to the LLM as the context window.

**Pinecone makes it easy to integrate RAG into your GenAI applications.** Our vector database can search billions of items for similar matches in under a second. We also provide a number of pre-trained models that you can use to get started quickly.

**To learn more about RAG, visit our website or sign up for a free trial.**"
33,33,What is retrieval-augmented generation?,"Aug 22, 2023 — ",https://research.ibm.com/blog/retrieval-augmented-generation-RAG,"[""What is retrieval-augmented generation? | IBM Research BlogSkip to main contentResearchFocus areasBlogPublicationsCareersAboutBackFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBackAboutOverviewLabsPeopleCollaborateBackArtificial IntelligenceBackHybrid CloudBackQuantum ComputingBackScienceBackSecurityBackSemiconductorsBackOverviewBackLabsBackPeopleBackCollaborateResearchFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsBlogPublicationsCareersAboutOverviewLabsPeopleCollaborateOpen IBM search fieldClose22 Aug 2023Explainer4 minute readWhat is retrieval-augmented generation?RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.Large language models can be inconsistent. Sometimes they nail the answer to questions, other times they regurgitate random facts from their training data. If they occasionally sound like they have no idea what they’re saying, it’s because they don’t. LLMs know how words relate statistically, but not what they mean.Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.“You want to cross-reference a model’s answers with the original content so you can see what it is basing its answer on,” said Luis Lastras, director of language technologies at IBM Research.RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. IBM unveiled its new AI and data platform, watsonx, which offers RAG, back in May.An ‘open book’ approach to answering tough questionsUnderpinning all foundation models, including LLMs, is an AI architecture known as the transformer. It turns heaps of raw data into a compressed representation of its basic structure. Starting from this raw representation, a foundation model can be adapted to a variety of tasks with some additional fine-tuning on labeled, domain-specific knowledge.But fine-tuning alone rarely gives the model the full breadth of knowledge it needs to answer highly specific questions in an ever-changing context. In a 2020 paper, Meta (then known as Facebook) came up with a framework called retrieval-augmented generation to give LLMs access to information beyond their training data. RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way.“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”As the name suggests, RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.Toward personalized and verifiable responsesBefore LLMs, digital conversation agents followed a manual dialogue flow. They confirmed the customer’s intent, fetched the requested information, and delivered an answer in a one-size-fits all script. For straightforward queries, this manual decision-tree method worked just fine.But it had limitations. Anticipating and scripting answers to every question a customer might conceivably ask took time; if you missed a scenario, the chatbot had no ability to improvise. Updating the scripts as policies and circumstances evolved was either impractical or impossible.Today, LLM-powered chatbots can give customers more personalized answers without humans having to write out new scripts. And RAG allows LLMs to go one step further by greatly reducing the need to feed and retrain the model on fresh examples. Simply upload the latest documents or policies, and the model retrieves the information in open-book mode to answer the question.IBM is currently using RAG to ground its internal customer-care chatbots on content that can be verified and trusted. This real-world scenario shows how it works: An employee, Alice, has learned that her son’s school will have early dismissal on Wednesdays for the rest of the year. She wants to know if she can take vacation in half-day increments and if she has enough vacation to finish the year.To craft its response, the LLM first pulls data from Alice’s HR files to find out how much vacation she gets as a longtime employee, and how many days she has left for the year. It also searches the company’s policies to verify that her vacation can be taken in half-days. These facts are injected into Alice’s initial query and passed to the LLM, which generates a concise, personalized answer. A chatbot delivers the response, with links to its sources.Teaching the model to recognize when it doesn’t knowCustomer queries aren’t always this straightforward. They can be ambiguously worded, complex, or require knowledge the model either doesn’t have or can’t easily parse. These are the conditions in which LLMs are prone to making things up.“Think of the model as an overeager junior employee that blurts out an answer before checking the facts,” said Lastras. “Experience teaches us to stop and say when we don’t know something. But LLMs need to be explicitly trained to recognize questions they can’t answer.”In a more challenging scenario taken from real life, Alice wants to know how many days of maternity leave she gets. A chatbot that does not use RAG responds cheerfully (and incorrectly): “Take as long as you want.”Maternity-leave policies are complex, in part, because they vary by the state or country of the employee’s home-office. When the LLM failed to find a precise answer, it should have responded, “I’m sorry, I don’t know,” said Lastras, or asked additional questions until it could land on a question it could definitively answer. Instead, it pulled a phrase from a training set stocked with empathetic, customer-pleasing language.With enough fine-tuning, an LLM can be trained to pause and say when it’s stuck. But it may need to see thousands of examples of questions that can and can’t be answered. Only then can the model learn to identify an unanswerable question, and probe for more detail until it hits on a question that it has the information to answer.RAG is currently the best-known tool for grounding LLMs on the latest, verifiable information, and lowering the costs of having to constantly retrain and update them. But RAG is imperfect, and many interesting challenges remain in getting RAG done right.At IBM Research, we are focused on innovating at both ends of the process: retrieval, how to find and fetch the most relevant information possible to feed the LLM; and generation, how to best structure that information to get the richest responses from the LLM.Subscribe to our Future Forward newsletter and stay up to date on the latest research newsSubscribe to our newsletterHome↳ BlogDate22 Aug 2023AuthorsKim MartineauTopicsAIExplainable AIGenerative AINatural Language ProcessingTrustworthy GenerationShareWhat is AI inferencing?ExplainerKim Martineau05 Oct 2023AIFoundation ModelsGenerative AIHybrid CloudCelebrating a decade of IBM Research innovation in AfricaNewsMike Murphy28 Sep 2023AIQuantumFind and fix IT glitches before they crash the system NewsKim Martineau28 Sep 2023AI for CodeAI for ITExplainable AIFoundation ModelsGenerative AIHow open source paved the way for computing from anywhere NewsKim Martineau20 Sep 2023AIHybrid Cloud PlatformMachine LearningScaling AIPreviousError correcting codes for near-term quantum computersNextIBM Research’s newest prototype chips use drastically less power to solve AI tasksIBM LogoFocus areasFocus areasArtificial IntelligenceHybrid CloudQuantum ComputingScienceSecuritySemiconductorsQuick linksQuick linksAboutPublicationsBlogEventsWork with usWork with usCareersCollaborateContact ResearchDirectoriesDirectoriesTopicsPeopleProjectsFollow usFollow usTwitterLinkedInYouTubeContact IBMPrivacyTerms of useAccessibility""]","Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.

RAG has two phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain, consumer setting, those facts can come from indexed documents on the internet; in a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.

This assortment of external knowledge is appended to the user’s prompt and passed to the language model. In the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant. The answer can then be passed to a chatbot with links to its sources.

RAG allows LLMs to build on a specialized body of knowledge to answer questions in more accurate way. It reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting."
34,34,Retrieval Augmented Generation (RAG),RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient ...,https://www.promptingguide.ai/techniques/rag,"['Retrieval Augmented Generation (RAG) | Prompt Engineering Guide Prompt Engineering GuidePrompt Engineering CoursePrompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsTechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingSelf-ConsistencyGenerate Knowledge PromptingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingReActMultimodal CoTGraph PromptingApplicationsProgram-Aided Language ModelsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionModelsFlanChatGPTLLaMAGPT-4LLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesPapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightQuestion? Give us feedback → (opens in a new tab)Edit this pageTechniquesRetrieval Augmented GenerationRetrieval Augmented Generation (RAG)', ""General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge."", 'For more complex and knowledge-intensive tasks, it\'s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of ""hallucination"".', 'Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.', ""RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."", 'Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:', 'Image Source: Lewis et el. (2021) (opens in a new tab)', 'RAG performs strong on several benchmarks such as Natural Questions (opens in a new tab), WebQuestions (opens in a new tab), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.', 'This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.', 'More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.', 'You can find a simple example of how to use retrievers and LLMs for question answering with sources (opens in a new tab) from the LangChain documentation.Tree of ThoughtsAutomatic Reasoning and Tool-useEnglishLightCopyright © 2023 DAIR.AI']","Retrieval Augmented Generation (RAG) is a method proposed by Meta AI researchers to address knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.

RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation."
35,35,What Is Retrieval-Augmented Generation (RAG)?,"Sep 19, 2023 — ",https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/,"['What Is Retrieval-Augmented Generation (RAG)?', 'Accessibility Policy', 'Skip to content', 'About', 'Services', 'Solutions', 'Pricing', 'Partners', 'Resources', 'Close Search', 'Close', 'We’re sorry.  We could not find a match for your search.', ""We suggest you try the following to help find what you're looking for:"", 'Check the spelling of your keyword search.', 'Use synonyms for the keyword you typed, for example, try “application” instead of “software.”', 'Start a new search.', 'Clear Search', 'Search', 'Menu', 'Menu', 'Contact Sales', 'Sign in to Oracle Cloud', 'Cloud', 'Artificial Intelligence', 'Generative AI', 'What Is Retrieval-Augmented Generation (RAG)?', 'Alan Zeichick | Tech Content Strategist | September 19, 2023', 'In This Article', 'What Is Retrieval-Augmented Generation (RAG)?', 'Retrieval-Augmented Generation Explained', 'How Does Retrieval-Augmented Generation Work?', 'Using RAG in Chat Applications', 'Benefits of Retrieval-Augmented Generation', 'Challenges of Retrieval-Augmented Generation', 'Examples of Retrieval-Augmented Generation', 'Future of Retrieval-Augmented Generation', 'Generative AI With Oracle', 'Retrieval-Augmented Generation FAQs', 'Generative artificial intelligence (AI) excels at creating text responses based on large language models (LLMs) where the AI is trained on a massive number of data points. The good news is that the generated text is often easy to read and provides detailed responses that are broadly applicable to the questions asked of the software, often called prompts.', 'The bad news is that the information used to generate the response is limited to the information used to train the AI, often a generalized LLM. The LLM’s data may be weeks, months, or years out of date and in a corporate AI chatbot may not include specific information about the organization’s products or services. That can lead to incorrect responses that erode confidence in the technology among customers and employees.', 'What Is Retrieval-Augmented Generation (RAG)?', 'That’s where retrieval-augmented generation (RAG) comes in. RAG provides a way to optimize the output of an LLM with targeted information without modifying the underlying model itself; that targeted information can be more up-to-date than the LLM as well as specific to a particular organization and industry. That means the generative AI system can provide more contextually appropriate answers to prompts as well as base those answers on extremely current data.', 'RAG first came to the attention of generative AI developers after the publication of “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” a 2020 paper published by Patrick Lewis and a team at Facebook AI Research. The RAG concept has been embraced by many academic and industry researchers, who see it as a way to significantly improve the value of generative AI systems.', 'Retrieval-Augmented Generation Explained', 'Consider a sports league that wants fans and the media to be able to use chat to access its data and answer questions about players, teams, the sport’s history and rules, and current stats and standings. A generalized LLM could answer questions about the history and rules or perhaps describe a particular team’s stadium. It wouldn’t be able to discuss last night’s game or provide current information about a particular athlete’s injury because the LLM wouldn’t have that information—and given that an LLM takes significant computing horsepower to retrain, it isn’t feasible to keep the model current.', 'In addition to the large, fairly static LLM, the sports league owns or can access many other information sources, including databases, data warehouses, documents containing player bios, and news feeds that discuss each game in depth. RAG lets the generative AI ingest this information. Now, the chat can provide information that’s more timely, more contextually appropriate, and more accurate. ', 'Simply put, RAG helps LLMs give better answers.', 'Key Takeaways', 'RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.', 'RAG models build knowledge repositories based on the organization’s own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.', 'Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.', 'Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM.', 'How Does Retrieval-Augmented Generation Work? ', 'Consider all the information that an organization has—the structured databases, the unstructured PDFs and other documents, the blogs, the news feeds, the chat transcripts from past customer service sessions. In RAG, this vast quantity of dynamic data is translated into a common format and stored in a knowledge library that’s accessible to the generative AI system.', ' The data in that knowledge library is then processed into numerical representations using a special type of algorithm called an embedded language model and stored in a vector database, which can be quickly searched and used to retrieve the correct contextual information.', 'RAG and Large Language Models (LLMs)', 'Now, say an end user sends the generative AI system a specific prompt, for example, “Where will tonight’s game be played, who are the starting players, and what are reporters saying about the matchup?” The query is transformed into a vector and used to query the vector database, which retrieves information relevant to that question’s context. That contextual information plus the original prompt are then fed into the LLM, which generates a text response based on both its somewhat out-of-date generalized knowledge and the extremely timely contextual information.', 'Interestingly, while the process of training the generalized LLM is time-consuming and costly, updates to the RAG model are just the opposite. New data can be loaded into the embedded language model and translated into vectors on a continuous, incremental basis. In fact, the answers from the entire generative AI system can be fed back into the RAG model, improving its performance and accuracy, because, in effect, it knows how it has already answered a similar question.', 'An additional benefit of RAG is that by using the vector database, the generative AI can provide the specific source of data cited in its answer—something LLMs can’t do. Therefore, if there’s an inaccuracy in the generative AI’s output, the document that contains that erroneous information can be quickly identified and corrected, and then the corrected information can be fed into the vector database.', 'In short, RAG provides timeliness, context, and accuracy grounded in evidence to generative AI, going beyond what the LLM itself can provide.', 'Retrieval-Augmented Generation vs. Semantic Search', ' RAG isn’t the only technique used to improve the accuracy of LLM-based generative AI. Another technique is semantic search, which helps the AI system narrow down the meaning of a query by seeking deep understanding of the specific words and phrases in the prompt.', ' Traditional search is focused on keywords. For example, a basic query asking about the tree species native to France might search the AI system’s database using “trees” and “France” as keywords and find data that contains both keywords—but the system might not truly comprehend the meaning of trees in France and therefore may retrieve too much information, too little, or even the wrong information. That keyword-based search might also miss information because the keyword search is too literal: The trees native to Normandy might be missed, even though they’re in France, because that keyword was missing.', ' Semantic search goes beyond keyword search by determining the meaning of questions and source documents and using that meaning to retrieve more accurate results. Semantic search is an integral part of RAG.', 'Using RAG in Chat Applications', 'When a person wants an instant answer to a question, it’s hard to beat the immediacy and usability of a chatbot. Most bots are trained on a finite number of intents—that is, the customer’s desired tasks or outcomes—and they respond to those intents. RAG capabilities can make current bots better by allowing the AI system to provide natural language answers to questions that aren’t in the intent list.', ' The “ask a question, get an answer” paradigm makes chatbots a perfect use case for generative AI, for many reasons. Questions often require specific context to generate an accurate answer, and given that chatbot users’ expectations about relevance and accuracy are often high, it’s clear how RAG techniques apply. In fact, for many organizations, chatbots may indeed be the starting point for RAG and generative AI use.', 'Questions often require specific context to deliver an accurate answer. Customer queries about a newly introduced product, for example, aren’t useful if the data pertains to the previous model and may in fact be misleading. And a hiker who wants to know if a park is open this Sunday expects timely, accurate information about that specific park on that specific date.', 'Benefits of Retrieval-Augmented Generation', 'RAG techniques can be used to improve the quality of a generative AI system’s responses to prompts, beyond what an LLM alone can deliver. Benefits include the following:', 'The RAG has access to information that may be fresher than the data used to train the LLM.', 'Data in the RAG’s knowledge repository can be continually updated without incurring significant costs.', 'The RAG’s knowledge repository can contain data that’s more contextual than the data in a generalized LLM.', 'The source of the information in the RAG’s vector database can be identified. And because the data sources are known, incorrect information in the RAG can be corrected or deleted.', 'Challenges of Retrieval-Augmented Generation', 'Because RAG is a relatively new technology, first proposed in 2020, AI developers are still learning how to best implement its information retrieval mechanisms in generative AI. Some key challenges are', 'Improving organizational knowledge and understanding of RAG because it’s so new', 'Increasing costs; while generative AI with RAG will be more expensive to implement than an LLM on its own, this route is less costly than frequently retraining the LLM itself', 'Determining how to best model the structured and unstructured data within the knowledge library and vector database', 'Developing requirements for a process to incrementally feed data into the RAG system', 'Putting processes in place to handle reports of inaccuracies and to correct or delete those information sources in the RAG system', 'Examples of Retrieval-Augmented Generation', 'There are many possible examples of generative AI augmented by RAG.', 'Cohere, a leader in the field of generative AI and RAG, has written about a chatbot that can provide contextual information about a vacation rental in the Canary Islands, including fact-based answers about beach accessibility, lifeguards on nearby beaches, and the availability of volleyball courts within walking distance.', 'Oracle has described other use cases for RAG, such as analyzing financial reports, assisting with gas and oil discovery, reviewing transcripts from call center customer exchanges, and searching medical databases for relevant research papers.', 'Future of Retrieval-Augmented Generation', 'Today, in the early phases of RAG, the technology is being used to provide timely, accurate, and contextual responses to queries. These use cases are appropriate to chatbots, email, text messaging, and other conversational applications.', 'In the future, possible directions for RAG technology would be to help generative AI take an appropriate action based on contextual information and user prompts. For example, a RAG-augmented AI system might identify the highest-rated beach vacation rental on the Canary Islands and then initiate booking a two-bedroom cabin within walking distance of the beach during a volleyball tournament.', 'RAG might also be able to assist with more sophisticated lines of questioning. Today, generative AI might be able to tell an employee about the company’s tuition reimbursement policy; RAG could add more contextual data to tell the employee which nearby schools have courses that fit into that policy and perhaps recommend programs that are suited to the employee’s jobs and previous training—maybe even help apply for those programs and initiate a reimbursement request.', 'Generative AI With Oracle', 'Oracle offers a variety of advanced cloud-based AI services, including the OCI Generative AI service running on Oracle Cloud Infrastructure (OCI). Oracle’s offerings include robust models based on your organization’s unique data and industry knowledge. Customer data is not shared with LLM providers or seen by other customers, and custom models trained on customer data can only be used by that customer.', 'In addition, Oracle is integrating generative AI across its wide range of cloud applications, and generative AI capabilities are available to developers who use OCI and across its database portfolio. What’s more, Oracle’s AI services offer predictable performance and pricing using single-tenant AI clusters dedicated to your use.', 'The power and capabilities of LLMs and generative AI are widely known and understood—they’ve been the subject of breathless news headlines for the past year. Retrieval-augmented generation builds on the benefits of LLMs by making them more timely, more accurate, and more contextual. For business applications of generative AI, RAG is an important technology to watch, study, and pilot.', 'What makes Oracle best suited for generative AI?', 'Oracle offers a modern data platform and low-cost, high-performance AI infrastructure. Additional factors, such as powerful, high-performing models, unrivaled data security, and embedded AI services demonstrate why Oracle’s AI offering is truly built for enterprises.', 'Learn more about Oracle’s generative AI strategy', 'Retrieval-Augmented Generation FAQs', 'Is RAG the same as generative AI?', 'No. Retrieval-augmented generation is a technique that can provide more accurate results to queries than a generative large language model on its own because RAG uses knowledge external to data already contained in the LLM.', 'What type of information is used in RAG?', 'RAG can incorporate data from many sources, such as relational databases, unstructured document repositories, internet data streams, media newsfeeds, audio transcripts, and transaction logs.', 'How does generative AI use RAG?', 'Data from enterprise data sources is embedded into a knowledge repository and then converted to vectors, which are stored in a vector database. When an end user makes a query, the vector database retrieves relevant contextual information. This contextual information, along with the query, is sent to the large language model, which uses the context to create a more timely, accurate, and contextual response.', 'Can a RAG cite references for the data it retrieves?', 'Yes. The vector databases and knowledge repositories used by RAG contain specific information about the sources of information. This means that sources can be cited, and if there’s an error in one of those sources it can be quickly corrected or deleted so that subsequent queries won’t return that incorrect information.', 'Resources for', 'Careers', 'Developers', 'Investors', 'Partners', 'Startups', 'Students and Educators', 'Why Oracle', 'Analyst Reports', 'Cloud Economics', 'with Microsoft Azure', 'vs. AWS', 'vs. Google Cloud', 'vs. MongoDB', 'Learn', 'What is AI?', 'What is Cloud Computing?', 'What is Cloud Storage?', 'What is HPC?', 'What is IaaS?', 'What is PaaS?', 'What’s new', 'Oracle Supports Ukraine', 'Oracle Cloud Free Tier', 'Cloud Architecture Center', 'Cloud Lift', 'Oracle Support Rewards', 'Oracle Red Bull Racing', 'Contact us', 'US Sales: +1.800.633.0738', 'How can we help?', 'Subscribe to emails', 'Events', 'News', 'OCI Blog', ' Country/Region ', '© 2023 Oracle', 'Privacy/Do Not Sell My Info', 'Ad Choices', 'Careers', 'Facebook', 'Twitter', 'LinkedIn', 'YouTube']","Retrieval-augmented generation (RAG) is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining.

RAG models build knowledge repositories based on the organization's own data, and the repositories can be continually updated to help the generative AI provide timely, contextual answers.

Chatbots and other conversational systems that use natural language processing can benefit greatly from RAG and generative AI.

Implementing RAG requires technologies such as vector databases, which allow for the rapid coding of new data, and searches against that data to feed into the LLM."
36,36,What is Retrieval Augmented Generation (RAG)?,"RAG puts together a pre-trained system that finds relevant information ( retriever ) with another system that generates text ( generator ). Then, when the user ...",https://vercel.com/guides/retrieval-augmented-generation,"['What is Retrieval Augmented Generation (RAG)?Skip to contentDocumentationGuidesHelp← Back to GuidesWhat is Retrieval Augmented Generation (RAG)?Large-language modals (LLMs) like OpenAI\'s GPT-4 and Anthropic\'s Claude are incredible at generating coherent and contextually relevant text based on given prompts. They can assist in a wide range of tasks, such as writing, translation, and even conversation.Despite this, LLMs have limitations. In this guide, we\'ll go over these constraints and explain how Retrieval Augmented Generation (RAG) can alleviate these pains. We\'ll also dive into the ways you can build better chat experiences with this technique.The problem with LLMsAs groundbreaking as LLMs may be, they have a few limitations:They\'re limited by the amount of training data they have access to. For example, GPT-4 has a training data cutoff date, which means that it doesn\'t have access to information beyond that date. This limitation affects the model\'s ability to generate up-to-date and accurate responses.They\'re generic and lack subject-matter expertise. LLMs are trained on a large dataset that covers a wide range of topics, but they don\'t possess specialized knowledge in any particular field. This leads to hallucinations or inaccurate information when asked about specific subject areas.Citations are tricky. LLMs don\'t have a reliable way of returning the exact location of the text where they retrieved the information. This exacerbates the issue of hallucination, as they may not be able to provide proper attribution or verify the accuracy of their responses. Additionally, the lack of specific citations makes it difficult for users to fact-check or delve deeper into the information provided by the models.Retrieval Augmented GenerationTo solve this problem, researchers at Meta published a paper about a technique called Retrieval Augmented Generation (RAG), which adds an information retrieval component to the text generation model that LLMs are already good at. This allows for fine-tuning and adjustments to the LLM\'s internal knowledge, making it more accurate and up-to-date.Source: Lewis et el. (2021)\xa0–\xa0Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksHere\'s how RAG works on a high level:RAG puts together a pre-trained system that finds relevant information (retriever) with another system that generates text (generator).Then, when the user inputs a question (query), the retriever use a technique called ""Maximum Inner Product Search (MIPS)"" to find the most relevant documents. The information from these documents will then be fed into the generator to create the final response. This also allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.Using RAG in Chat ApplicationsTo illustrate how you can apply RAG in a real-world application, here\'s a chatbot template that uses RAG with a Pinecone vector store and the Vercel AI SDK to create an accurate, evidence-based chat experience. You can deploy the template on Vercel with one click, or run the following command to develop it locally:npx create-next-app pinecone-vercel-starter --example ""https://github.com/pinecone-io/pinecone-vercel-starter""The chatbot combines retrieval-based and generative models to deliver accurate responses. The application integrates Vercel\'s AI SDK for efficient chatbot setup and streaming in edge environments. The guide section of the template covers the following steps:Setting up a Next.js applicationCreating a chatbot frontend componentBuilding an API endpoint using OpenAI\'s API for response generationProgressively enhancing the chatbot with context-based capabilities, including context seeding, context retrieval, and displaying context segments. By following the tutorial, you\'ll build a context-aware chatbot with improved user experience.Build better AI chat experiences with RAGBy integrating Retrieval Augmented Generation into chat applications like the Pinecone chatbot template above, developers can reduce hallucinations in their AI models and create more accurate and evidence-based conversational experiences.If you\'re interested in learning more about RAG, check out this article about integrating RAG with Langchain and a Supabase vector database.Couldn\'t find the guide you need?View Help© 2023ProductPreviewsNext.jsInfrastructurev0Edge FunctionsTurboAnalyticsEnterpriseChangelogCLI & APIResourcesDocsExpertsPricingGuidesCustomersHelpIntegrations⌘KTemplatesCompanyAboutBlogCareersContact UsNext.js ConfOpen SourcePartnersSecurityPrivacy PolicyLegal']","Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generative models to deliver accurate responses.

RAG works by first finding the most relevant documents to a user's query. The information from these documents is then fed into a generative model to create the final response. This allows for citations, which allows the end user to verify the sources and delve deeper into the information provided.

RAG can be used to build better AI chat experiences by reducing hallucinations in AI models and creating more accurate and evidence-based conversational experiences."
37,37,Retrieval Augmented Generation (RAG),You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data ...,https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html,"['Retrieval Augmented Generation (RAG) - Amazon SageMakerRetrieval Augmented Generation (RAG) - Amazon SageMakerAWSDocumentationAmazon SageMakerDeveloper GuideRetrieval Augmented Generation (RAG)Foundation models are usually trained offline, making the model agnostic to any', '                data that is created after the model was trained. Additionally, foundation models', '                are trained on very general domain corpora, making them less effective for', '                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve', '                data from outside a foundation model and augment your prompts by adding the relevant', '                retrieved data in context. For more information about RAG model architectures, see', '                    Retrieval-Augmented Generation for', '                    Knowledge-Intensive NLP Tasks.With RAG, the external data used to augment your prompts can come from multiple', '                data sources, such as a document repositories, databases, or APIs. The first step is', '                to convert your documents and any user queries into a compatible format to perform', '                relevancy search. To make the formats compatible, a document collection, or', '                knowledge library, and user-submitted queries are converted to numerical', '                representations using embedding language models. Embedding is the process by which text is given numerical', '                representation in a vector space. RAG model architectures compare the embeddings of', '                user queries within the vector of the knowledge library. The original user prompt is', '                then appended with relevant context from similar documents within the knowledge', '                library. This augmented prompt is then sent to the foundation model. You can update', '                knowledge libraries and their relevant embeddings asynchronously.', 'For more information, see the following example notebooks: ', 'Retrieval-Augmented Generation: Question Answering based on Custom', '                            Dataset', 'Retrieval-Augmented Generation: Question Answering based on Custom', ""                            Dataset with Open-sourced LangChain Library Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.Document ConventionsFine-tuningTask-Specific ModelsDid this page help you? - YesThanks for letting us know we're doing a good job!If you've got a moment, please tell us what we did right so we can do more of it.Did this page help you? - NoThanks for letting us know this page needs work. We're sorry we let you down.If you've got a moment, please tell us how we can make the documentation better.""]","Retrieval Augmented Generation (RAG) is a technique that can be used to improve the performance of foundation models on domain-specific tasks.

RAG works by retrieving relevant data from outside the foundation model and augmenting the user's prompt with this data. This augmented prompt is then sent to the foundation model, which can then generate more relevant results.

For more information, see the following example notebooks:

* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset](https://github.com/aws-samples/amazon-sagemaker-retrieval-augmented-generation/blob/master/notebooks/Retrieval-Augmented%20Generation%20-%20Question%20Answering%20based%20on%20Custom%20Dataset.ipynb)
* [Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library](https://github.com/aws-samples/amazon-sagemaker-retrieval-augmented-generation/blob/master/notebooks/Retrieval-Augmented%20Generation%20-%20Question%20Answering%20based%20on%20Custom%20Dataset%20with%20Open-sourced%20LangChain%20Library.ipynb)"
38,38,What Is Retrieval-Augmented Generation? | Definition from ...,Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the ...,https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation,"['What Is Retrieval-Augmented Generation? | Definition from TechTarget', 'Enterprise AI', 'Search the TechTarget Network', 'Login', 'Register', 'Explore the Network', 'TechTarget Network', 'Business Analytics', 'CIO', 'Data Management', 'ERP', 'Enterprise AI', 'AI Business Strategies', 'AI Careers', 'AI Infrastructure', 'AI Platforms ', 'AI Technologies', 'More Topics', 'Applications of AI', 'ML Platforms', 'Other Content', 'News', 'Features', 'Tips', 'Webinars', '2023 IT Salary Survey Results', '                                More', 'Answers', 'Conference Guides', 'Definitions', 'Opinions', 'Podcasts', 'Quizzes', 'Tech Accelerators', 'Tutorials', 'Videos', 'Sponsored Communities', 'Follow:', 'Home', 'AI technologies', 'Tech Accelerator', 'What is generative AI? Everything you need to know', 'Prev', 'Next', 'Generative AI in the enterprise raises questions for CIOs', '15 of the best large language models', 'Download this guide1', 'X', 'Free Download', 'What is generative AI? Everything you need to know', 'The potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight.', 'This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.', 'Corporate Email Address:You forgot to provide an Email Address.This email address doesn’t appear to be valid.This email address is already registered. Please log in.You have exceeded the maximum character limit.Please provide a Corporate Email Address.I agree to TechTarget’s Terms of Use, Privacy Policy, and the transfer of my information to the United States for processing to provide me with relevant information as described in our Privacy Policy.Please check the box if you want to proceed.I agree to my information being processed by TechTarget and its Partners to contact me via phone, email, or other means regarding information relevant to my professional interests. I may unsubscribe at any time.Please check the box if you want to proceed.', 'By submitting my Email address I confirm that I have read and accepted the Terms of Use and Declaration of Consent.', 'Definition', 'retrieval-augmented generation ', 'Share this item with your network:', 'By', 'Alexander S. Gillis,', 'Technical Writer and Editor', 'What is retrieval-augmented generation?', 'Retrieval-augmented generation (RAG) is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.', ""LLMs are AI models that power chatbots such as OpenAI's ChatGPT and Google Bard. LLMs can understand, summarize, generate and predict new content. However, they can still be inconsistent and fail at some knowledge-intensive tasks -- especially tasks that are outside their initial training data or those that require up-to-date information and transparency about how they make their decisions. When this happens, the LLM can return false information, also known as an AI hallucination."", ""By retrieving information from external sources when the LLM's trained data isn't enough, the quality of LLM responses improves. Retrieving information from an online source, for example, enables the LLM to access current information that it wasn't initially trained on."", 'What does RAG do?', ""LLMs are commonly trained offline, making the model uncertain of any data that's created after the model was trained. RAG is used to retrieve data from outside the LLM, which then augments the user's prompts by adding relevant retrieved data in its response."", 'This article is part of', 'What is generative AI? Everything you need to know', 'Which also includes:', '15 of the best large language models', 'Will AI replace jobs? 9 job types that might be affected', 'Pros and cons of AI-generated content', 'This process helps reduce any apparent knowledge gaps and AI hallucinations. This can be important in fields that require as much up-to-date and accurate information as possible, such as healthcare.', 'How to use RAG with LLMs', 'RAG combines information retrieval with a text generator model. External knowledge can be retrieved from data sources, online sources, application programming interfaces, databases or document repositories. ', 'Using the example of a chatbot, once a user inputs a prompt, RAG summarizes that prompt using keywords or semantic data. The converted data is then sent to a search platform to retrieve the requested data, which is then sorted through based on relevancy.', 'The LLM then synthesizes the retrieved data with the augmented prompt and its internal training data to create a generated response that can be passed to the chatbot with sourced links for the user.', 'An LLM using RAG can pull from both internal and external data to return a response for users, ensuring it provides relevant information.', 'What are the benefits of RAG?', 'Benefits of a RAG model include the following:', 'Provides current information. RAG pulls information from relevant, reliable and up-to-date sources.', ""Increases user trust. Users can access the model's sources, which promotes transparency and trust in the content and lets users verify its accuracy."", 'Reduces AI hallucinations. Because LLMs are grounded to external data, the model has less of a chance to make up or return incorrect information.', ""Reduces computational and financial costs. Organizations don't have to spend time and resources to continuously train the model on new data."", 'Synthesizes information. RAG synthesizes data by combining relevant information from retrieval and generative models to produce a response.', 'Easier to train. Because RAG uses retrieved knowledge sources, the need to train the LLM on a massive amount of training data is reduced.', 'Can be used for multiple tasks. Aside from chatbots, RAG can be fine-tuned for a variety of specific use cases, such as text summarization and dialogue systems.', 'Learn more about generative AI models, such as VAEs, GANs, diffusion, transformers and NeRFs.', '\t\t\t\t\tThis was last updated in October 2023', '\t\t\tContinue Reading About retrieval-augmented generation', '7 generative AI challenges that businesses should consider', 'Generative AI ethics: 8 biggest concerns', 'Assessing different types of generative AI applications', 'Pros and cons of AI-generated content', 'Cohesity Turing aims AI tools at backup and ransomware', '\t\t\t\tRelated Terms', 'language modeling', 'Language modeling, or LM, is the use of various statistical and probabilistic techniques to determine the probability of a given ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'OpenAI', 'OpenAI is a private research laboratory that aims to develop and direct artificial intelligence (AI) in ways that benefit ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Salesforce Einstein', 'Salesforce Einstein refers to an integrated set of artificial intelligence (AI) technologies developed for the Salesforce ... ', '\t\t\t\t\t\t\tSee\xa0complete\xa0definition', 'Dig Deeper on AI technologies', 'Databricks improves support for generative AI models', 'By: Eric\xa0Avidon', '15 of the best large language models', 'By: Ben\xa0Lutkevich', 'LangChain', 'By: Cameron\xa0Hashemi-Pour', 'large language models (LLMs)', 'By: Sean\xa0Kerner', 'Sponsored News', '4 Things You Need to Know Now About Edge Computing', '–HPE', 'Supporting a more diverse management team', '–AWS', 'See More', 'Vendor Resources', ""What generative AI's rise means for the cybersecurity industry"", '–TechTarget ComputerWeekly.com', 'Fast Track Generative AI with Dell™ Poweredge™ XE9680', '–Dell Technologies & Intel®', 'Latest TechTarget resources', '\t\t\t\t\t\t\tBusiness Analytics', '\t\t\t\t\t\t\tCIO', '\t\t\t\t\t\t\tData Management', '\t\t\t\t\t\t\tERP', 'Business Analytics', 'MicroStrategy launches new suite of generative AI tools', ""The longtime analytics vendor's suite incorporates LLM technology to make users more efficient by enabling them to use natural ..."", 'Sisense unveils composable toolkit for app development', 'Compose SDK for Fusion is a composable set of APIs that enable developers to build customized advanced analytics applications to ...', 'SAS unveils plans to add generative AI to analytics suite', 'After holding off on integrating with LLMs until it could ensure data security and accurate outcomes, the vendor is making ...', 'CIO', '7 challenges with blockchain adoption and how to avoid them', 'Organizations tend to face the same hurdles when they try to implement blockchain. Knowing what they are could be the first big ...', 'U.S. antitrust law enforcers defend actions, lawsuits', 'The FTC and DOJ, which enforce U.S. antitrust law, are focused on reining in big tech through antitrust lawsuits and revising ...', 'Is quantum computing overhyped?', ""Quantum computing may be coming to the enterprise. Here's what to understand about the benefits it promises, the risks it poses ..."", 'Data Management', 'Amazon launches DataZone, a new data management service', ""The tech giant's new service provides data governance, collaboration and catalog capabilities that enable organizations to find ..."", 'Databricks improves support for generative AI models', 'A new service enables users to easily deploy privately built language models and uses a GPU-based architecture to optimize and ...', 'New Boomi AI tool enables natural language data integration', ""The iPaaS vendor's new capabilities are aimed at increasing efficiency by enabling customers to build pipelines and manage data ..."", 'ERP', 'Infor Enterprise Automation looks to ease RPA for ERP', 'Infor unveiled Enterprise Automation, which is designed to help customers bring RPA to Infor cloud ERP applications and Developer...', 'When integrating generative AI and ERP, focus on use cases', 'ERP vendors are rapidly introducing new AI functionality, but experts caution against the hype and advise using the ...', 'PLM and PDM software: Learn the differences', 'PLM and PDM software may seem similar, but they fulfill different needs for manufacturers. Learn the differences and which is ...', 'About Us', 'Editorial Ethics Policy', 'Meet The Editors', 'Contact Us', 'Advertisers', 'Partner with Us', 'Media Kit', 'Corporate Site', 'Contributors', 'Reprints', 'Answers', 'Definitions', 'E-Products', 'Events', 'Features', 'Guides', 'Opinions', 'Photo Stories', 'Quizzes', 'Tips', 'Tutorials', 'Videos', 'All Rights Reserved, ', 'Copyright 2018 - 2023, TechTarget', 'Privacy Policy', 'Cookie Preferences ', 'Cookie Preferences ', 'Do Not Sell or Share My Personal Information', 'Close']",
39,39,Retrieval-Augmented Generation (RAG) in AI,"Sep 29, 2023 — ",https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/,"['Retrieval-Augmented Generation (RAG) in AI', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'Home', 'search', 'New', 'Community', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'DSAT', 'Glossary', 'Archive', ' Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Machine Learning', 'Deep Learning', 'NLP', ' All Courses', 'Certified AI & ML BlackBelt Plus', 'Blogathon', 'MasterSeries', 'Write an Article', 'Creators Club', 'Login/Signup', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'D', 'H', 'M', 'S', '×', 'What is Retrieval-Augmented Generation (RAG) in AI?', 'Facebook', 'Twitter', 'Linkedin', 'Soumyadarshan Dash —', 'Updated On September 29th, 2023 ', 'Advanced', 'Artificial Intelligence', 'Excel', 'Generative AI', 'Healthcare', 'LLMs', 'NLP', 'Introduction', 'The rapid advancements in Large Language Models (LLMs) have transformed the landscape of AI, offering unparalleled capabilities in natural language understanding and generation. LLMs have ushered in a new language understanding and generation era, with OpenAI’s GPT models at the forefront. These remarkable models honed on extensive online data, have broadened our horizons, enabling us to interact with AI-powered systems like never before. However, like any technological marvel, they come with their own set of limitations. One glaring issue is their occasional tendency to provide information that is either inaccurate or outdated. Moreover, these LLMs do not furnish the sources of their responses, making it challenging to verify the reliability of their output. This limitation becomes especially critical in contexts where accuracy and traceability are paramount. Retrieval Augmented Generation (RAG) in AI is a transformative paradigm that promises to revolutionize the capabilities of LLMs.', 'Rapid advancements in LLMs have propelled them to the forefront of AI, yet they still grapple with constraints like information capacity and occasional inaccuracies. RAG bridges these gaps by seamlessly integrating retrieval-based and generative components, endowing LLMs to tap into external knowledge sources. This article explores RAG’s profound impact, unraveling its architecture, benefits, challenges, and the diverse approaches that empower it. In doing so, we unveil the potential of RAG to redefine the landscape of Large Language Models and pave the way for more accurate, context-aware, and reliable AI-driven communication.', 'Learning Objectives', 'Learn about language models and how RAG enhances their capabilities.', 'Discover methods to integrate external data into RAG systems effectively.', 'Explore ethical issues in RAG, including bias and privacy.', 'Gain hands-on experience with RAG using LangChain for real-world applications.', 'This article was published as a part of the\xa0Data Science Blogathon.', 'Table of contentsIntroductionUnderstanding Retrieval Augmented Generation (RAG)The Power of External DataBenefits of Retrieval Augmented Generation (RAG)Diverse Approaches in RAGEthical Considerations in RAGApplications of Retrieval Augmented Generation (RAG)The Future of RAGs and LLMsUtilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)OutputConclusionFrequently Asked Questions', 'Understanding Retrieval Augmented Generation (RAG)', 'Retrieval Augmented Generation, or RAG, represents a cutting-edge approach to artificial intelligence (AI) and natural language processing (NLP). At its core, RAG is an innovative framework that combines the strengths of retrieval-based and generative models, revolutionizing how AI systems understand and generate human-like text.', 'Do you want to know more about RAG? Read more here. ', 'What is the Need for RAG?', 'The development of RAG is a direct response to the limitations of Large Language Models (LLMs) like GPT. While LLMs have shown impressive text generation capabilities, they often struggle to provide contextually relevant responses, hindering their utility in practical applications. RAG aims to bridge this gap by offering a solution that excels in understanding user intent and delivering meaningful and context-aware replies.', 'The Fusion of Retrieval-Based and Generative Models', 'RAG is fundamentally a hybrid model that seamlessly integrates two critical components. Retrieval-based methods involve accessing and extracting information from external knowledge sources such as databases, articles, or websites. On the other hand, generative models excel in generating coherent and contextually relevant text. What distinguishes RAG is its ability to harmonize these two components, creating a symbiotic relationship that allows it to comprehend user queries deeply and produce responses that are not just accurate but also contextually rich.', 'Deconstructing RAG’s Mechanics', 'To grasp the essence of RAG, it’s essential to deconstruct its operational mechanics. RAG operates through a series of well-defined steps. ', 'Begin by receiving and processing user input.', 'Analyze the user input to understand its meaning and intent.', 'Utilize retrieval-based methods to access external knowledge sources. This enriches the understanding of the user’s query.', 'Use the retrieved external knowledge to enhance comprehension.', 'Employ generative capabilities to craft responses. Ensure responses are factually accurate, contextually relevant, and coherent.', 'Combine all the information gathered to produce responses that are meaningful and human-like.', 'Ensure that the transformation of user queries into responses is done effectively.', 'The Role of Language Models and User Input', 'Central to understanding RAG is appreciating the role of Large Language Models (LLMs) in AI systems. LLMs like GPT are the backbone of many NLP applications, including chatbots and virtual assistants. They excel in processing user input and generating text, but their accuracy and contextual awareness are paramount for successful interactions. RAG strives to enhance these essential aspects through its integration of retrieval and generation.', 'Incorporating External Knowledge Sources', 'RAG’s distinguishing feature is its ability to integrate external knowledge sources seamlessly. By drawing from vast information repositories, RAG augments its understanding, enabling it to provide well-informed and contextually nuanced responses. Incorporating external knowledge elevates the quality of interactions and ensures that users receive relevant and accurate information.', 'Generating Contextual Responses', 'Ultimately, the hallmark of RAG is its ability to generate contextual responses. It considers the broader context of user queries, leverages external knowledge, and produces responses demonstrating a deep understanding of the user’s needs. These context-aware responses are a significant advancement, as they facilitate more natural and human-like interactions, making AI systems powered by RAG highly effective in various domains.', 'Retrieval Augmented Generation (RAG) is a transformative concept in AI and NLP. By harmonizing retrieval and generation components, RAG addresses the limitations of existing language models and paves the way for more intelligent and context-aware AI interactions. Its ability to seamlessly integrate external knowledge sources and generate responses that align with user intent positions RAG as a game-changer in developing AI systems that can truly understand and communicate with users in a human-like manner.', 'The Power of External Data', 'In this section, we delve into the pivotal role of external data sources within the Retrieval Augmented Generation (RAG) framework. We explore the diverse range of data sources that can be harnessed to empower RAG-driven models.', 'APIs and Real-time Databases', 'APIs (Application Programming Interfaces) and real-time databases are dynamic sources that provide up-to-the-minute information to RAG-driven models. They allow models to access the latest data as it becomes available.', 'Document Repositories', 'Document repositories serve as valuable knowledge stores, offering structured and unstructured information. They are fundamental in expanding the knowledge base that RAG models can draw upon.', 'Webpages and Scraping', 'Web scraping is a method for extracting information from web pages. It enables RAG models to access dynamic web content, making it a crucial source for real-time data retrieval.', 'Databases and Structured Information', 'Databases provide structured data that can be queried and extracted. RAG models can use databases to retrieve specific information, enhancing the accuracy of their responses.', 'Benefits of Retrieval Augmented Generation (RAG)', 'Enhanced LLM Memory', 'RAG addresses the information capacity limitation of traditional Language Models (LLMs). Traditional LLMs have a limited memory called “Parametric memory.” RAG introduces a “Non-Parametric memory” by tapping into external knowledge sources. This significantly expands the knowledge base of LLMs, enabling them to provide more comprehensive and accurate responses.', 'Improved Contextualization', 'RAG enhances the contextual understanding of LLMs by retrieving and integrating relevant contextual documents. This empowers the model to generate responses that align seamlessly with the specific context of the user’s input, resulting in accurate and contextually appropriate outputs.', 'Updatable Memory', 'A standout advantage of RAG is its ability to accommodate real-time updates and fresh sources without extensive model retraining. This keeps the external knowledge base current and ensures that LLM-generated responses are always based on the latest and most relevant information.', 'Source Citations', 'RAG-equipped models can provide sources for their responses, enhancing transparency and credibility. Users can access the sources that inform the LLM’s responses, promoting transparency and trust in AI-generated content.', 'Reduced Hallucinations', 'Studies have shown that RAG models exhibit fewer hallucinations and higher response accuracy. They are also less likely to leak sensitive information. Reduced hallucinations and increased accuracy make RAG models more reliable in generating content.', 'These benefits collectively make Retrieval Augmented Generation (RAG) a transformative framework in Natural Language Processing, overcoming the limitations of traditional language models and enhancing the capabilities of AI-powered applications.', 'Diverse Approaches in RAG', 'RAG offers a spectrum of approaches for the retrieval mechanism, catering to various needs and scenarios:', 'Simple: Retrieve relevant documents and seamlessly incorporate them into the generation process, ensuring comprehensive responses.', 'Map Reduce: Combine responses generated individually for each document to craft the final response, synthesizing insights from multiple sources.', 'Map Refine: Iteratively refine responses using initial and subsequent documents, enhancing response quality through continuous improvement.', 'Map Rerank: Rank responses and select the highest-ranked response as the final answer, prioritizing accuracy and relevance.', 'Filtering: Apply advanced models to filter documents, utilizing the refined set as context for generating more focused and contextually relevant responses.', 'Contextual Compression: Extract pertinent snippets from documents, generating concise and informative responses and minimizing information overload.', 'Summary-Based Index: Leverage document summaries, index document snippets, and generate responses using relevant summaries and snippets, ensuring concise yet informative answers.', 'Forward-Looking Active Retrieval Augmented Generation (FLARE): Predict forthcoming sentences by initially retrieving relevant documents and iteratively refining responses. Flare ensures a dynamic and contextually aligned generation process.', 'These diverse approaches empower RAG to adapt to various use cases and retrieval scenarios, allowing for tailored solutions that maximize AI-generated responses’ relevance, accuracy, and efficiency.', 'Ethical Considerations in RAG', 'RAG introduces ethical considerations that demand careful attention:', 'Ensuring Fair and Responsible Use: Ethical deployment of RAG involves using the technology responsibly and refraining from any misuse or harmful applications. Developers and users must adhere to ethical guidelines to maintain the integrity of AI-generated content.', 'Addressing Privacy Concerns: RAG’s reliance on external data sources may involve accessing user data or sensitive information. Establishing robust privacy safeguards to protect individuals’ data and ensure compliance with privacy regulations is imperative.', 'Mitigating Biases in External Data Sources: External data sources can inherit biases in their content or collection methods. Developers must implement mechanisms to identify and rectify biases, ensuring AI-generated responses remain unbiased and fair. This involves constant monitoring and refinement of data sources and training processes.', 'Applications of Retrieval Augmented Generation (RAG)', 'RAG finds versatile applications across various domains, enhancing AI capabilities in different contexts:', 'Chatbots and AI Assistants: RAG-powered systems excel in question-answering scenarios, providing context-aware and detailed answers from extensive knowledge bases. These systems enable more informative and engaging interactions with users.', 'Education Tools: RAG can significantly improve educational tools by offering students access to answers, explanations, and additional context based on textbooks and reference materials. This facilitates more effective learning and comprehension.', 'Legal Research and Document Review: Legal professionals can leverage RAG models to streamline document review processes and conduct efficient legal research. RAG assists in summarizing statutes, case law, and other legal documents, saving time and improving accuracy.', 'Medical Diagnosis and Healthcare: In the healthcare domain, RAG models serve as valuable tools for doctors and medical professionals. They provide access to the latest medical literature and clinical guidelines, aiding in accurate diagnosis and treatment recommendations.', 'Language Translation with Context: RAG enhances language translation tasks by considering the context in knowledge bases. This approach results in more accurate translations, accounting for specific terminology and domain knowledge, particularly valuable in technical or specialized fields.', 'These applications highlight how RAG’s integration of external knowledge sources empowers AI systems to excel in various domains, providing context-aware, accurate, and valuable insights and responses.', 'The Future of RAGs and LLMs', 'The evolution of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) is poised for exciting developments:', 'Advancements in Retrieval Mechanisms: The future of RAG will witness refinements in retrieval mechanisms. These enhancements will focus on improving the precision and efficiency of document retrieval, ensuring that LLMs access the most relevant information quickly. Advanced algorithms and AI techniques will play a pivotal role in this evolution.', 'Integration with Multimodal AI: The synergy between RAG and multimodal AI, which combines text with other data types like images and videos, holds immense promise. Future RAG models will seamlessly incorporate multimodal data to provide richer and more contextually aware responses. This will open doors to innovative applications like content generation, recommendation systems, and virtual assistants.', 'RAG in Industry-Specific Applications: As RAG matures, it will find its way into industry-specific applications. Healthcare, law, finance, and education sectors will harness RAG-powered LLMs for specialized tasks. For example, in healthcare, RAG models will aid in diagnosing medical conditions by instantly retrieving the latest clinical guidelines and research papers, ensuring doctors have access to the most current information.', 'Ongoing Research and Innovation in RAG: The future of RAG is marked by relentless research and innovation. AI researchers will continue to push the boundaries of what RAG can achieve, exploring novel architectures, training methodologies, and applications. This ongoing pursuit of excellence will result in more accurate, efficient, and versatile RAG models.', 'LLMs with Enhanced Retrieval Capabilities: LLMs will evolve to possess enhanced retrieval capabilities as a core feature. They will seamlessly integrate retrieval and generation components, making them more efficient at accessing external knowledge sources. This integration will lead to LLMs that are proficient in understanding context and excel in providing context-aware responses.', 'Utilizing LangChain for Enhanced Retrieval-Augmented Generation (RAG)', 'Installation of LangChain and OpenAI Libraries', 'This line of code installs the LangChain and OpenAI libraries. LangChain is critical for handling text data and embedding, while OpenAI provides access to state-of-the-art Large Language Models (LLMs). This installation step is essential for setting up the required tools for RAG.', '!pip install langchain openai', '!pip install -q -U faiss-cpu tiktoken', 'import os', 'import getpass', 'os.environ[""OPENAI_API_KEY""] = getpass.getpass(""Open AI API Key:"")', 'Web Data Loading for the RAG Knowledge Base', 'The code utilizes LangChain’s “WebBaseLoader.”', 'Three web pages are specified for data retrieval: YOLO-NAS object detection, DeciCoder’s code generation efficiency, and a Deep Learning Daily newsletter.', 'This step is essential for building the knowledge base used in RAG, enabling contextually relevant and accurate information retrieval and integration into language model responses.', 'from langchain.document_loaders import WebBaseLoader', 'yolo_nas_loader = WebBaseLoader(""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"").load()', 'decicoder_loader = WebBaseLoader(""https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder\'s%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency."").load()', 'yolo_newsletter_loader = WebBaseLoader(""https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas"").load()', 'Embedding and Vector Store Setup', 'The code sets up embeddings for the RAG process.', 'It uses “OpenAIEmbeddings” to create an embedding model.', 'A “CacheBackedEmbeddings” object is initialized, allowing embeddings to be stored and retrieved efficiently using a local file store.', 'A “FAISS” vector store is created from the preprocessed chunks of web data (yolo_nas_chunks, decicoder_chunks, and yolo_newsletter_chunks), enabling fast and accurate similarity-based retrieval.', 'Finally, a retriever is instantiated from the vector store, facilitating efficient document retrieval during the RAG process.', 'from langchain.embeddings.openai import OpenAIEmbeddings', 'from langchain.embeddings import CacheBackedEmbeddings', 'from langchain.vectorstores import FAISS', 'from langchain.storage import LocalFileStore', 'store = LocalFileStore(""./cachce/"")', '# create an embedder', 'core_embeddings_model = OpenAIEmbeddings()', 'embedder = CacheBackedEmbeddings.from_bytes_store(', '    core_embeddings_model,', '    store,', '    namespace = core_embeddings_model.model', ')', '# store embeddings in vector store', 'vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)', 'vectorstore.add_documents(decicoder_chunks)', 'vectorstore.add_documents(yolo_newsletter_chunks)', '# instantiate a retriever', 'retriever = vectorstore.as_retriever()', 'Establishing the Retrieval System', 'The code configures the retrieval system for Retrieval Augmented Generation (RAG).', 'It uses “OpenAIChat” from the LangChain library to set up a chat-based Large Language Model (LLM).', 'A callback handler named “StdOutCallbackHandler” is defined to manage interactions with the retrieval system.', 'The “RetrievalQA” chain is created, incorporating the LLM, retriever (previously initialized), and callback handler.', 'This chain is designed to perform retrieval-based question-answering tasks, and it is configured to return source documents for added context during the RAG process.', 'from langchain.llms.openai import OpenAIChat', 'from langchain.chains import RetrievalQA', 'from langchain.callbacks import StdOutCallbackHandler', 'llm = OpenAIChat()', 'handler =  StdOutCallbackHandler()', '# This is the entire retrieval system', 'qa_with_sources_chain = RetrievalQA.from_chain_type(', '    llm=llm,', '    retriever=retriever,', '    callbacks=[handler],', '    return_source_documents=True', ')', 'Initializes the RAG System', 'The code sets up a RetrievalQA chain, a critical part of the RAG system, by combining an OpenAIChat language model (LLM) with a retriever and callback handler.', 'Issue Queries to the RAG System', 'It sends various user queries to the RAG system, prompting it to retrieve contextually relevant information.', 'Retrieves Responses', 'After processing the queries, the RAG system generates and returns contextually rich and accurate responses. The responses are printed on the console.', '# This is the entire augment system!', 'response = qa_with_sources_chain({""query"":""What does Neural Architecture Search have to do with how Deci creates its models?""})', 'response', ""print(response['result'])"", ""print(response['source_documents'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""What is DeciCoder""})', ""print(response['result'])"", 'response = qa_with_sources_chain({""query"":""Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder""})', ""print(response['result'])"", 'This code exemplifies how RAG and LangChain can enhance information retrieval and generation in AI applications.', 'Output', 'Conclusion', 'Retrieval Augmented Generation (RAG) represents a transformative leap in artificial intelligence. It seamlessly integrates Large Language Models (LLMs) with external knowledge sources, addressing the limitations of LLMs’ parametric memory.', 'RAG’s ability to access real-time data, coupled with improved contextualization, enhances the relevance and accuracy of AI-generated responses. Its updatable memory ensures responses are current without extensive model retraining. RAG also offers source citations, bolstering transparency and reducing data leakage. In summary, RAG empowers AI to provide more accurate, context-aware, and reliable information, promising a brighter future for AI applications across industries.', 'Key Takeaways', 'Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.', 'RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.', 'With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.', 'RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.', 'This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.', 'Frequently Asked Questions', 'Q1. What is RAG? How does it differ from traditional AI models? A. RAG, or Retrieval Augmented Generation, is an innovative AI framework combining retrieval-based and generative models’ strengths. Unlike traditional AI models, which generate responses solely based on their pre-trained knowledge, RAG integrates external knowledge sources, allowing it to provide more accurate, up-to-date, and contextually relevant responses.  Q2. How does RAG ensure the accuracy of the retrieved information? A. RAG employs a retrieval system that fetches information from external sources. It ensures accuracy through techniques like vector similarity search and real-time updates to external datasets. Additionally, RAG allows users to access source citations, enhancing transparency and credibility.  Q3. Can RAG be used in specific industries or applications? A. Yes, RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.  Q4. Does implementing RAG require extensive technical expertise? A. While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.  Q5. What are the potential ethical concerns with RAG, such as misinformation or data privacy? A. RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns.  ', 'The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\xa0', 'Related', 'AIApplicationsblogathonData Sourcesdocumentslanguage modelsmemoryModels ', 'Recommended For You', 'Become a full stack data scientist', 'Large Language Models Demystified: A Beginner’s Roadmap', 'Training Your Own LLM Without Coding', 'How to Build LLMs for Code?\xa0', 'LLMs in Conversational AI: Building Smarter Chatbots & Assistants', 'From GPT-3 to Future Generations of Language Models', 'What are Large Language Models (LLMs)?', 'About the Author', 'Soumyadarshan Dash', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'How to Explore Text Generation with GPT-2? ', 'Next Post', 'How does Generative AI in Recipe Generation and Culinary Arts Work? ', 'Top Resources', '10 Best AI Image Generator Tools to Use in 2023', 'avcontentteam - ', 'Aug 17, 2023', 'Everything you need to Know about Linear Regression!', 'KAVITA MALI - ', 'Oct 04, 2021', 'Skewness and Kurtosis: Quick Guide (Updated 2023)', 'Suvarna Gawali - ', 'May 02, 2021', 'Guide on Support Vector Machine (SVM) Algorithm', 'Anshul Saini - ', 'Oct 12, 2021', '×', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Join the Community', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2023 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'Loading...', 'To continue reading please login', 'google-icon', 'Continue with Google', 'linkedin', 'Continue with Linkedin', 'email', 'Continue with Email', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Welcome Back :)', 'email 2', 'key-', 'Forgot Password?', 'Log In', ""Don't have an account yet?Register here"", 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', '×', 'back', 'Start your journey here!', 'user', 'user', 'email 2', 'key-', 'Sign up', 'Already have an accountLogin here', 'I accept the Terms and Conditions', ""Please accept the TnC's to continue"", 'Receive Updates on Whatsapp', ' A verification link has been sent to your email id ', ' If you have not recieved the link please goto', 'Sign Up  page again', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your registered email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter the OTP that is sent to your email id', 'email 2', 'Next', 'Loading...', '×', 'back', 'Please enter your registered email id', 'email 2', 'Next', 'This email id is not registered with us. Please enter your registered email id.', ""Don't have an account yet?Register here"", 'Loading...', '×', 'back', 'Please enter the OTP that is sent your registered email id', 'email 2', 'Next', 'Loading...', '×', 'Please create the new password here', 'key-', 'key-', 'Submit', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview ', 'This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.', 'Necessary ', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. ', 'Non-necessary ', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. ', 'SAVE & ACCEPT', '×']","**Summarize the article in 10 bullet points**
- Retrieval Augmented Generation (RAG) is a groundbreaking framework that enhances Large Language Models (LLMs) by integrating external knowledge sources.
- RAG overcomes the limitations of LLMs’ parametric memory, enabling them to access real-time data, improving contextualization, and providing up-to-date responses.
- With RAG, AI-generated content becomes more accurate, context-aware, and transparent, as it can cite sources and reduce data leakage.
- RAG’s updatable memory eliminates frequent model retraining, making it a cost-effective solution for various applications.
- This technology promises to revolutionize AI across industries, providing users with more reliable and relevant information.
- RAG is versatile and can be applied across various domains. It’s particularly useful in fields where accurate and current information is crucial, such as healthcare, finance, legal, and customer support.
- While RAG involves some technical components, user-friendly tools, and libraries are available to simplify the process. Many organizations are also developing user-friendly RAG platforms, making it accessible to a broader audience.
- RAG does raise critical ethical considerations. Ensuring the quality and reliability of external data sources, preventing misinformation, and safeguarding user data are ongoing challenges. Ethical guidelines and responsible AI practices are crucial in addressing these concerns."
